diff --git a/.gitignore b/.gitignore
index 3f69011..2089c7a 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,8 +1,15 @@
 data/*
 README_local_only.md
 outputs/*
-
-
+outputs_V0/*
+outputs_igarss/*
+figures/
+*.out
+*.log
+wandb/*
+geo/*.sh
+run_logs/*
+outputs_V0/*
 
 # Byte-compiled / optimized / DLL files
 __pycache__/
@@ -108,4 +115,4 @@ venv.bak/
 /site
 
 # mypy
-.mypy_cache/
\ No newline at end of file
+.mypy_cache/
diff --git a/.vscode/launch.json b/.vscode/launch.json
index 17e15f2..b069de5 100644
--- a/.vscode/launch.json
+++ b/.vscode/launch.json
@@ -9,7 +9,11 @@
             "type": "python",
             "request": "launch",
             "program": "${file}",
-            "console": "integratedTerminal"
+            "console": "integratedTerminal",
+            "env": {
+                "PYTHONPATH": "/geoinfo_vol1/home/p/u/puzhao/miniforge3/envs/pytorch/bin/python",
+                "CUDA_VISIBLE_DEVICES": "7",
+            },
         }
     ]
 }
\ No newline at end of file
diff --git a/.vscode/settings.json b/.vscode/settings.json
new file mode 100644
index 0000000..1b29ff0
--- /dev/null
+++ b/.vscode/settings.json
@@ -0,0 +1,3 @@
+{
+    "python.pythonPath": "/cephyr/users/puzhao/Alvis/PyTorch_v1.7.0-py3.sif"
+}
diff --git a/README.md b/README.md
index 40bcc3c..c50d240 100644
--- a/README.md
+++ b/README.md
@@ -1,2 +1,65 @@
 
-# smp-seg-pytorch project
\ No newline at end of file
+# smp-seg-pytorch project
+This project aims to train U-Net segementation models for large-scale burned area mapping.
+
+``` python
+main_s1s2_unet.py
+main_s1s2_fuse_unet.py
+main_mtbs.py
+```
+
+
+scp -r puzhao@alvis1.c3se.chalmers.se:/cephyr/NOBACKUP/groups/snic2021-7-104/puzhao-snic-500G/smp-seg-pytorch/outputs/errMap D:\PyProjects\IGARSS-2022-S1S2\
+
+scp -r D:\wildfire-s1s2-dataset-ca-2019-median-tiles-V1\test_images\* puzhao@alvis1.c3se.chalmers.se:/cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ca-tiles/test_images
+
+# SNIC to geoinfo-gpu
+scp -r puzhao@alvis1.c3se.chalmers.se:/cephyr/NOBACKUP/groups/snic2021-7-104/puzhao-snic-500G/smp-seg-pytorch/main_s1s2_fuse_unet_V1.py /home/p/u/puzhao/smp-seg-pytorch/main_s1s2_fuse_unet_V1.py
+
+scp -r /home/p/u/puzhao/smp-seg-pytorch/fcnn4cd/paddle_unet.py puzhao@alvis1.c3se.chalmers.se:/cephyr/NOBACKUP/groups/snic2021-7-104/puzhao-snic-500G/smp-seg-pytorch/fcnn4cd
+
+
+
+# Paddle Installation
+https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/install/conda/linux-conda.html
+
+# change permission
+<!-- https://linuxize.com/post/chmod-command-in-linux/ -->
+chmod [OPTIONS] [ugoa…][-+=]perms…[,…] FILE...
+chmod u=rwx,g=r,o= filename
+
+u - The file owner.
+g - The users who are members of the group.
+o - All other users.
+a - All users, identical to ugo.
+
+-rw-r--r-- 12 linuxize users 12.0K Apr  8 20:51 filename.txt
+|[-][-][-]-   [------] [---]
+| |  |  | |      |       |
+| |  |  | |      |       +-----------> 7. Group
+| |  |  | |      +-------------------> 6. Owner
+| |  |  | +--------------------------> 5. Alternate Access Method
+| |  |  +----------------------------> 4. Others Permissions
+| |  +-------------------------------> 3. Group Permissions
+| +----------------------------------> 2. Owner Permissions
++------------------------------------> 1. File Type
+
+chmod 744 /home/p/u/puzhao/wildfire-s1s2-dataset-ca-tiles
+-rw------- (600) -- Only the user has read and write permissions.
+-rw-r--r-- (644) -- Only user has read and write permissions; the group and others can read only.
+-rwx------ (700) -- Only the user has read, write and execute permissions.
+-rwxr-xr-x (755) -- The user has read, write and execute permissions; the group and others can only read and execute.
+-rwx--x--x (711) -- The user has read, write and execute permissions; the group and others can only execute.
+-rw-rw-rw- (666) -- Everyone can read and write to the file. Bad idea.
+-rwxrwxrwx (777) -- Everyone can read, write and execute. Another bad idea.
+
+r (read) = 4
+w (write) = 2
+x (execute) = 1
+no permissions = 0
+
+Owner: rwx=4+2+1=7
+Group: r-x=4+0+1=5
+Others: r-x=4+0+0=4
+
+chmod a+rw 
\ No newline at end of file
diff --git a/config/distill_unet.yaml b/config/distill_unet.yaml
new file mode 100644
index 0000000..7c51ce0
--- /dev/null
+++ b/config/distill_unet.yaml
@@ -0,0 +1,83 @@
+
+# defaults:
+#     - s1s2_unet: s1
+
+PROJECT:
+    NAME: IGARSS-2022
+    ENTITY: wildfire
+
+RAND: # Rrproduce Results
+    SEED: 42
+    DETERMIN: True
+
+DATA:
+    NAME: s1s2
+    DIR: /home/p/u/puzhao/wildfire-s1s2-dataset-ca-tiles
+    # DIR: /cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ak-tiles
+    # DIR: /cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ca-tiles
+
+    SATELLITES: ['S1']
+    PREPOST: ['pre','post']
+    STACKING: True # stack bi-temporal data
+
+    ALL_BANDS:
+        ALOS: ['ND', 'VH', 'VV']
+        S1: ['ND', 'VH', 'VV']
+        S2: ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']
+
+    INPUT_BANDS:
+        ALOS: ['ND', 'VH', 'VV']
+        S1: ['ND', 'VH', 'VV']
+        S2: ['B2', 'B3', 'B4', 'B8', 'B11', 'B12'] #['B4', 'B8', 'B12']
+        # S2: ['B3', 'B8', 'B12'] #['B4', 'B8', 'B12']
+
+    REF_MASK: poly
+    TRAIN_RATIO: 0.9
+
+MODEL:
+    DEBUG: True
+
+    DISTILL: True
+    PRETRAINED: /home/p/u/puzhao/smp-seg-pytorch/outputs/run_s1s2_UNet_['S2']_Jan13_20220113T231535/model.pth
+    LOSS_COEF: [1,0,0] # a * dice_loss + b * loss_do + c * loss_df
+    L2_NORM: False
+
+    ARCH: distill_unet #FuseUNet, UNet
+    USE_DECONV: True
+    TOPO: [16, 32, 64, 128]
+    LOSS_TYPE: DiceLoss # DiceLoss
+    NUM_CLASSES: 1
+    ACTIVATION: sigmoid #sigmoid
+
+    ENCODER: resnet18 # 'mobilenet_v2'
+    ENCODER_DEPTH: 4
+    ENCODER_WEIGHTS: imagenet
+
+    MAX_EPOCH: 100
+    BATCH_SIZE: 16
+
+    LEARNING_RATE: 1e-4
+    WEIGHT_DECAY: 1e-2
+    LR_SCHEDULER:  poly # ['cosine', 'poly']
+    POLY_SCHEDULER:
+        END_LR: 1e-5
+        POWER: 0.9
+    COSINE_SCHEDULER:
+        WARMUP: 2
+    
+    MAX_SCORE: 0.1 # If IoU > max_score, save model
+    SAVE_INTERVAL: 5 # save model frequency
+    STEP_WISE_LOG: False # log metrics every step/update
+    VERBOSE: True
+
+EVAL:
+    PATCHSIZE: 512
+
+EXP:
+    NOTE: debug
+    NAME: ${DATA.NAME}_${MODEL.ARCH}_${DATA.SATELLITES}_${EXP.NOTE}
+    OUTPUT: ./outputs/run_${EXP.NAME}_${now:%Y%m%dT%H%M%S} #${defaults.0.data}
+
+hydra:
+    run:
+        dir: ${EXP.OUTPUT}
diff --git a/config/mtbs_cfg.yaml b/config/mtbs_cfg.yaml
deleted file mode 100644
index abb222b..0000000
--- a/config/mtbs_cfg.yaml
+++ /dev/null
@@ -1,38 +0,0 @@
-project:
-    name: MTBS
-
-data:
-    name: mtbs
-    useDataWoCAug: False
-    CLASSES: ['unburn', 'low', 'moderate', 'high', 'greener', 'cloud']
-    SEED: 42
-    random_state: 42
-
-model:
-    ARCH: UNet
-    ACTIVATION: sigmoid
-    ENCODER: resnet18 # 'mobilenet_v2'
-    ENCODER_WEIGHTS: imagenet
-    INPUT_CHANNELS: 12
-
-    max_epoch: 10
-    batch_size: 16
-    learning_rate: 1e-4
-    weight_decay: 1e-4
-    
-
-    use_lr_scheduler: False
-    warmup_coef: 2
-    
-    max_score: 0.1 # If IoU > max_score, start to save model
-
-    verbose: True
-
-experiment:
-    note: dft
-    name: ${data.name}_${model.ARCH}_${model.ENCODER}_${experiment.note}
-    output: ./outputs/run_${experiment.name}_${now:%Y%m%dT%H%M%S} #${defaults.0.data}
-
-hydra:
-    run:
-        dir: ${experiment.output}
\ No newline at end of file
diff --git a/config/s1s2_cfg.yaml b/config/s1s2_cfg.yaml
deleted file mode 100644
index 0fc0430..0000000
--- a/config/s1s2_cfg.yaml
+++ /dev/null
@@ -1,53 +0,0 @@
-project:
-    name: S1S2-snic
-
-data:
-    name: s1s2
-    satellites: ['S1', 'S2']
-    prepost: ['pre', 'post']
-    stacking: True
-    # dir: /cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ak-tiles
-    dir: D:\wildfire-s1s2-dataset-ak-tiles
-
-    useDataWoCAug: False
-    SEED: 42
-    random_state: 42
-
-    CLASSES: ['unburn', 'burned']
-    S1_INPUT_BANDS: ['ND', 'VH', 'VV']
-    ALOS_INPUT_BANDS: ['ND', 'VH', 'VV']
-    S2_INPUT_BANDS: ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']
-    REF_MASK: poly
-
-    USE_PRE_IMG: True
-
-model:
-    ARCH: FuseUNet #FuseUNet, UNet
-    ACTIVATION: sigmoid
-    ENCODER: resnet18 # 'mobilenet_v2'
-    ENCODER_WEIGHTS: imagenet
-
-    max_epoch: 2
-    batch_size: 12
-    learning_rate: 1e-4
-    weight_decay: 1e-4
-    cross_domain_coef: 0.1
-
-    use_lr_scheduler: True
-    warmup_coef: 2
-    
-    max_score: 0.1 # If IoU > max_score, save model
-    verbose: True
-
-eval:
-    mode: single_sensor_prepost #single_sensor_prepost #multi_sensor_post
-    patchsize: 512
-
-experiment:
-    note: ${data.satellites}
-    name: ${data.name}_${model.ARCH}_${model.ENCODER}_${experiment.note}
-    output: ./outputs/run_${experiment.name}_${now:%Y%m%dT%H%M%S} #${defaults.0.data}
-
-hydra:
-    run:
-        dir: ${experiment.output}
\ No newline at end of file
diff --git a/config/s1s2_cfg_prg.yaml b/config/s1s2_cfg_prg.yaml
new file mode 100644
index 0000000..cdf445e
--- /dev/null
+++ b/config/s1s2_cfg_prg.yaml
@@ -0,0 +1,82 @@
+
+# defaults:
+#     - s1s2_unet: s1
+
+PROJECT:
+    NAME: IGARSS-2022
+    ENTITY: wildfire
+
+RAND: # Rrproduce Results
+    SEED: 42
+    DETERMIN: True
+
+DATA:
+    NAME: s1s2
+    # DIR: /home/p/u/puzhao/wildfire-s1s2-dataset-ca-tiles
+    # DIR: /cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ak-tiles
+    # DIR: /cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ca-tiles
+    DIR: /home/p/u/puzhao/wildfire-progression-dataset/CA_2021_Kamloops
+
+    SATELLITES: ['S1']
+    PREPOST: ['pre','post']
+    STACKING: True # stack bi-temporal data
+
+    ALL_BANDS:
+        ALOS: ['ND', 'VH', 'VV']
+        S1: ['ND', 'VH', 'VV']
+        S2: ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']
+
+    INPUT_BANDS:
+        ALOS: ['ND', 'VH', 'VV']
+        S1: ['ND', 'VH', 'VV']
+        S2: ['B8', 'B11', 'B12'] #['B4', 'B8', 'B12']
+        # S2: ['B3', 'B8', 'B12'] #['B4', 'B8', 'B12']
+
+    # REF_MASK: poly
+    TRAIN_MASK: poly
+    TEST_MASK: poly
+    TRAIN_RATIO: 0.9
+    AUGMENT: False
+
+MODEL:
+    DEBUG: True
+
+    ARCH: UNet #UNet_resnet18 #FuseUNet, UNet
+    USE_DECONV: True
+    TOPO: [16, 32, 64, 128]
+    LOSS_TYPE: DiceLoss # DiceLoss
+    NUM_CLASSES: 1
+    ACTIVATION: sigmoid #sigmoid
+
+    ENCODER: resnet18 # 'mobilenet_v2'
+    ENCODER_DEPTH: 4
+    ENCODER_WEIGHTS: imagenet
+
+    MAX_EPOCH: 100
+    BATCH_SIZE: 16
+
+    LEARNING_RATE: 1e-4
+    WEIGHT_DECAY: 1e-2
+    LR_SCHEDULER:  poly # ['cosine', 'poly']
+    POLY_SCHEDULER:
+        END_LR: 1e-5
+        POWER: 0.9
+    COSINE_SCHEDULER:
+        WARMUP: 10
+    
+    MAX_SCORE: 0.1 # If IoU > max_score, save model
+    SAVE_INTERVAL: 5 # save model frequency
+    STEP_WISE_LOG: False # log metrics every step/update
+    VERBOSE: True
+
+EVAL:
+    PATCHSIZE: 512
+
+EXP:
+    NOTE: prg-debug
+    NAME: ${DATA.NAME}_${MODEL.ARCH}_${DATA.SATELLITES}_${EXP.NOTE}
+    OUTPUT: ./outputs/run_${EXP.NAME}_${now:%Y%m%dT%H%M%S} #${defaults.0.data}
+
+hydra:
+    run:
+        dir: ${EXP.OUTPUT}
\ No newline at end of file
diff --git a/config/s1s2_fusion.yaml b/config/s1s2_fusion.yaml
deleted file mode 100644
index 02a2157..0000000
--- a/config/s1s2_fusion.yaml
+++ /dev/null
@@ -1,46 +0,0 @@
-project:
-    name: S1S2
-
-data:
-    name: s1s2
-    satellites: ['S1', 'S2']
-    dir: D:\wildfire-s1s2-dataset-ak-tiles
-    useDataWoCAug: False
-    SEED: 42
-    CLASSES: ['unburn', 'burned']
-
-    random_state: 42
-
-model:
-    ARCH: FuseUNet
-    ACTIVATION: sigmoid
-    ENCODER: resnet18 # 'mobilenet_v2'
-    ENCODER_WEIGHTS: imagenet
-
-    S1_INPUT_BANDS: ['ND', 'VH', 'VV']
-    ALOS_INPUT_BANDS: ['ND', 'VH', 'VV']
-    S2_INPUT_BANDS: ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']
-    REF_MASK: poly
-
-    max_epoch: 20
-    batch_size: 12
-    learning_rate: 1e-4
-    weight_decay: 1e-4
-    cross_domain_coef: 0.1
-    
-
-    use_lr_scheduler: False
-    warmup_coef: 2
-    
-    max_score: 0.1 # If IoU > max_score, start to save model
-
-    verbose: True
-
-experiment:
-    note: fuse-mse
-    name: ${data.name}_${model.ARCH}_${model.ENCODER}_${experiment.note}
-    output: ./outputs/run_${experiment.name}_${now:%Y%m%dT%H%M%S} #${defaults.0.data}
-
-hydra:
-    run:
-        dir: ${experiment.output}
\ No newline at end of file
diff --git a/config/segformer.yaml b/config/segformer.yaml
new file mode 100644
index 0000000..bbc84be
--- /dev/null
+++ b/config/segformer.yaml
@@ -0,0 +1,73 @@
+
+# defaults:
+#     - s1s2_unet: s1
+
+project:
+    name: IGARSS-2022
+    entity: wildfire
+
+data:
+    # dir: /cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ak-tiles
+    # dir: /cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ca-tiles
+    # dir: D:\wildfire-s1s2-dataset-ak-tiles
+    dir: /home/p/u/puzhao/wildfire-s1s2-dataset-ca-tiles
+
+    name: s1s2
+    satellites: ['S1']
+    prepost: ['pre', 'post']
+    stacking: True # stack bi-temporal data
+
+    ALL_BANDS:
+        S1: ['ND', 'VH', 'VV']
+        ALOS: ['ND', 'VH', 'VV']
+        S2: ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']
+
+    INPUT_BANDS:
+        S1: ['ND', 'VH', 'VV']
+        ALOS: ['ND', 'VH', 'VV']
+        S2: ['B2', 'B3', 'B4', 'B8', 'B11', 'B12'] #['B4', 'B8', 'B12']
+        # S2: ['B3', 'B8', 'B12'] #['B4', 'B8', 'B12']
+
+    useDataWoCAug: False
+    SEED: 42
+    random_state: 42
+
+    # CLASSES: ['unburn', 'burned']
+    REF_MASK: poly
+    train_ratio: 0.9
+
+model:
+    DEBUG: True
+    ARCH: SegFormer_B2 #FuseUNet, UNet
+    # TOPO: [16, 32, 64, 128]
+    LOSS_TYPE: BCEWithLogitsLoss # smpDiceLoss
+    NUM_CLASSES: 2
+
+    ACTIVATION: sigmoid
+    # ENCODER: resnet18 # 'mobilenet_v2'
+    # ENCODER_WEIGHTS: imagenet
+
+    max_epoch: 100
+    batch_size: 64
+    learning_rate: 1e-4
+    weight_decay: 1e-4
+    cross_domain_coef: 0
+
+    use_lr_scheduler: True
+    warmup_coef: 2
+    
+    max_score: 0.1 # If IoU > max_score, save model
+    save_interval: 5 # save model frequency
+    verbose: True
+
+eval:
+    patchsize: 512
+
+experiment:
+    note: 
+    name: ${data.name}_${model.ARCH}_${data.satellites}_${experiment.note}
+    output: ./outputs/run_${experiment.name}_${now:%Y%m%dT%H%M%S} #${defaults.0.data}
+
+hydra:
+    run:
+        dir: ${experiment.output}
\ No newline at end of file
diff --git a/config/siam_unet.yaml b/config/siam_unet.yaml
new file mode 100644
index 0000000..96085d6
--- /dev/null
+++ b/config/siam_unet.yaml
@@ -0,0 +1,82 @@
+
+# defaults:
+#     - s1s2_unet: s1
+
+PROJECT:
+    NAME: IGARSS-2022
+    ENTITY: wildfire
+
+RAND: # Rrproduce Results
+    SEED: 42
+    DETERMIN: True
+
+DATA:
+    NAME: s1s2
+    DIR: /home/p/u/puzhao/wildfire-s1s2-dataset-ca-tiles
+    # DIR: /cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ak-tiles
+    # DIR: /cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ca-tiles
+
+    SATELLITES: ['S1','S2']
+    PREPOST: ['pre','post']
+    STACKING: True # stack bi-temporal data
+
+    ALL_BANDS:
+        ALOS: ['ND', 'VH', 'VV']
+        S1: ['ND', 'VH', 'VV']
+        S2: ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']
+
+    INPUT_BANDS:
+        ALOS: ['ND', 'VH', 'VV']
+        S1: ['ND', 'VH', 'VV']
+        S2: ['B3','B8', 'B12'] #['B4', 'B8', 'B12']
+        # S2: ['B3', 'B8', 'B12'] #['B4', 'B8', 'B12']
+
+    TRAIN_MASK: poly
+    TEST_MASK: poly
+    TRAIN_RATIO: 0.9
+    AUGMENT: False
+
+MODEL:
+    DEBUG: True
+
+    ARCH: DualUnet_LF #Paddle_unet #FuseUNet #FuseUNet, UNet
+    SHARE_ENCODER: True
+
+    TOPO: [16, 32, 64, 128]
+    USE_DECONV: True
+    LOSS_TYPE: DiceLoss # DiceLoss
+    NUM_CLASSES: 1
+    ACTIVATION: sigmoid #sigmoid
+
+    ENCODER: resnet18 # 'mobilenet_v2'
+    ENCODER_DEPTH: 4
+    ENCODER_WEIGHTS: null
+
+    MAX_EPOCH: 100
+    BATCH_SIZE: 16
+
+    LEARNING_RATE: 1e-4
+    WEIGHT_DECAY: 1e-2
+    LR_SCHEDULER:  cosine # ['cosine', 'poly']
+    POLY_SCHEDULER:
+        END_LR: 1e-5
+        POWER: 0.9
+    COSINE_SCHEDULER:
+        WARMUP: 2
+    
+    MAX_SCORE: 0.1 # If IoU > max_score, save model
+    SAVE_INTERVAL: 5 # save model frequency
+    STEP_WISE_LOG: False # log metrics every step/update
+    VERBOSE: True
+
+EVAL:
+    PATCHSIZE: 512
+
+EXP:
+    NOTE: debug
+    NAME: ${DATA.NAME}_${MODEL.ARCH}_${DATA.SATELLITES}_${EXP.NOTE}
+    OUTPUT: ./outputs/run_${EXP.NAME}_${now:%Y%m%dT%H%M%S} #${defaults.0.data}
+
+hydra:
+    run:
+        dir: ${EXP.OUTPUT}
\ No newline at end of file
diff --git a/config/siam_unet_bitemoral.yaml b/config/siam_unet_bitemoral.yaml
new file mode 100644
index 0000000..292a824
--- /dev/null
+++ b/config/siam_unet_bitemoral.yaml
@@ -0,0 +1,80 @@
+
+# defaults:
+#     - s1s2_unet: s1
+
+PROJECT:
+    NAME: IGARSS-2022
+    ENTITY: wildfire
+
+RAND: # Rrproduce Results
+    SEED: 42
+    DETERMIN: True
+
+DATA:
+    NAME: s1s2
+    DIR: /home/p/u/puzhao/wildfire-s1s2-dataset-ca-tiles
+    # DIR: /cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ak-tiles
+    # DIR: /cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ca-tiles
+
+    SATELLITES: ['S1']
+    PREPOST: ['pre','post']
+    STACKING: True # stack bi-temporal data
+
+    ALL_BANDS:
+        ALOS: ['ND', 'VH', 'VV']
+        S1: ['ND', 'VH', 'VV']
+        S2: ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']
+
+    INPUT_BANDS:
+        ALOS: ['ND', 'VH', 'VV']
+        S1: ['ND', 'VH', 'VV']
+        S2: ['B2', 'B3', 'B4', 'B8', 'B11', 'B12'] #['B4', 'B8', 'B12']
+        # S2: ['B3', 'B8', 'B12'] #['B4', 'B8', 'B12']
+
+    REF_MASK: poly
+    TRAIN_RATIO: 0.9
+
+MODEL:
+    DEBUG: True
+
+    AARCH: SiamUnet_conc #Paddle_unet #FuseUNet #FuseUNet, UNet
+    SHARE_ENCODER: True
+
+    TOPO: [16, 32, 64, 128]
+    USE_DECONV: True
+    LOSS_TYPE: DiceLoss # DiceLoss
+    NUM_CLASSES: 1
+    ACTIVATION: sigmoid #sigmoid
+
+    ENCODER: resnet18 # 'mobilenet_v2'
+    ENCODER_DEPTH: 4
+    ENCODER_WEIGHTS: null
+
+    MAX_EPOCH: 100
+    BATCH_SIZE: 16
+
+    LEARNING_RATE: 1e-4
+    WEIGHT_DECAY: 1e-3
+    LR_SCHEDULER:  poly # ['cosine', 'poly']
+    POLY_SCHEDULER:
+        END_LR: 1e-5
+        POWER: 0.9
+    COSINE_SCHEDULER:
+        WARMUP: 2
+    
+    MAX_SCORE: 0.1 # If IoU > max_score, save model
+    SAVE_INTERVAL: 5 # save model frequency
+    STEP_WISE_LOG: False # log metrics every step/update
+    VERBOSE: True
+
+EVAL:
+    PATCHSIZE: 512
+
+EXP:
+    NOTE: 
+    NAME: ${DATA.NAME}_${MODEL.ARCH}_${DATA.SATELLITES}_${EXP.NOTE}
+    OUTPUT: ./outputs/run_${EXP.NAME}_${now:%Y%m%dT%H%M%S} #${defaults.0.data}
+
+hydra:
+    run:
+        dir: ${EXP.OUTPUT}
\ No newline at end of file
diff --git a/config/siam_unet_multisensor.yaml b/config/siam_unet_multisensor.yaml
new file mode 100644
index 0000000..dc7d9d0
--- /dev/null
+++ b/config/siam_unet_multisensor.yaml
@@ -0,0 +1,80 @@
+
+# defaults:
+#     - s1s2_unet: s1
+
+PROJECT:
+    NAME: IGARSS-2022
+    ENTITY: wildfire
+
+RAND: # Rrproduce Results
+    SEED: 0
+    DETERMIN: True
+
+DATA:
+    NAME: s1s2
+    DIR: /home/p/u/puzhao/wildfire-s1s2-dataset-ca-tiles
+    # DIR: /cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ak-tiles
+    # DIR: /cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ca-tiles
+
+    SATELLITES: ['S1', 'S2']
+    PREPOST: ['pre','post']
+    STACKING: True # stack bi-temporal data
+
+    ALL_BANDS:
+        ALOS: ['ND', 'VH', 'VV']
+        S1: ['ND', 'VH', 'VV']
+        S2: ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']
+
+    INPUT_BANDS:
+        ALOS: ['ND', 'VH', 'VV']
+        S1: ['ND', 'VH', 'VV']
+        S2: ['B2', 'B3', 'B4', 'B8', 'B11', 'B12'] #['B4', 'B8', 'B12']
+        # S2: ['B3', 'B8', 'B12'] #['B4', 'B8', 'B12']
+
+    REF_MASK: poly
+    TRAIN_RATIO: 0.9
+
+MODEL:
+    DEBUG: True
+
+    AARCH: SiamUnet_conc #Paddle_unet #FuseUNet #FuseUNet, UNet
+    SHARE_ENCODER: True
+
+    TOPO: [16, 32, 64, 128]
+    USE_DECONV: True
+    LOSS_TYPE: DiceLoss # DiceLoss
+    NUM_CLASSES: 1
+    ACTIVATION: sigmoid #sigmoid
+
+    ENCODER: resnet18 # 'mobilenet_v2'
+    ENCODER_DEPTH: 4
+    ENCODER_WEIGHTS: null
+
+    MAX_EPOCH: 100
+    BATCH_SIZE: 16
+
+    LEARNING_RATE: 1e-4
+    WEIGHT_DECAY: 1e-3
+    LR_SCHEDULER:  poly # ['cosine', 'poly']
+    POLY_SCHEDULER:
+        END_LR: 1e-5
+        POWER: 0.9
+    COSINE_SCHEDULER:
+        WARMUP: 2
+    
+    MAX_SCORE: 0.1 # If IoU > max_score, save model
+    SAVE_INTERVAL: 5 # save model frequency
+    STEP_WISE_LOG: False # log metrics every step/update
+    VERBOSE: True
+
+EVAL:
+    PATCHSIZE: 512
+
+EXP:
+    NOTE: 
+    NAME: ${DATA.NAME}_${MODEL.ARCH}_${DATA.SATELLITES}_${EXP.NOTE}
+    OUTPUT: ./outputs/run_${EXP.NAME}_${now:%Y%m%dT%H%M%S} #${defaults.0.data}
+
+hydra:
+    run:
+        dir: ${EXP.OUTPUT}
\ No newline at end of file
diff --git a/config/unet.yaml b/config/unet.yaml
new file mode 100644
index 0000000..2d9c737
--- /dev/null
+++ b/config/unet.yaml
@@ -0,0 +1,89 @@
+
+# defaults:
+#     - s1s2_unet: s1
+
+PROJECT:
+    NAME: IGARSS-2022
+    ENTITY: wildfire
+
+RAND: # Rrproduce Results
+    SEED: 42
+    DETERMIN: True
+
+DATA:
+    NAME: s1s2
+    DIR: /home/p/u/puzhao/wildfire-s1s2-dataset-ca-tiles
+    # DIR: /cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ak-tiles
+    # DIR: /cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ca-tiles
+
+    SATELLITES: ['S2']
+    PREPOST: ['pre', 'post']
+    STACKING: True # stack bi-temporal data
+
+    ALL_BANDS:
+        ALOS: ['ND', 'VH', 'VV']
+        S1: ['ND', 'VH', 'VV']
+        S2: ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']
+
+    INPUT_BANDS:
+        ALOS: ['ND', 'VH', 'VV']
+        S1: ['ND', 'VH', 'VV']
+        S2: ['B4', 'B8', 'B12'] #['B4', 'B8', 'B12']
+        # S2: ['B3', 'B8', 'B12'] #['B4', 'B8', 'B12']
+
+    # REF_MASK: poly
+    TRAIN_MASK: poly
+    TEST_MASK: poly
+    TRAIN_RATIO: 0.9
+    AUGMENT: False
+
+MODEL:
+    DEBUG: True
+
+    ARCH: UNet #UNet_resnet18 #FuseUNet, UNet
+    USE_DECONV: True
+    TOPO: [16, 32, 64, 128]
+
+    # LOSS_TYPE: DiceLoss # DiceLoss
+    # NUM_CLASSES: 1
+    # ACTIVATION: sigmoid #sigmoid
+
+    LOSS_TYPE: CrossEntropyLoss # DiceLoss
+    NUM_CLASSES: 2
+    CLASS_NAMES: ['unburn', 'burned']
+    CLASS_WEIGHTS: [0.5, 0.5]
+    ACTIVATION: softmax #sigmoid
+
+    ENCODER: resnet18 # 'mobilenet_v2'
+    ENCODER_DEPTH: 4
+    ENCODER_WEIGHTS: imagenet
+
+    MAX_EPOCH: 100
+    BATCH_SIZE: 16
+
+    LEARNING_RATE: 1e-4
+    WEIGHT_DECAY: 1e-2
+    LR_SCHEDULER:  poly # ['cosine', 'poly']
+    POLY_SCHEDULER:
+        END_LR: 1e-5
+        POWER: 0.9
+    COSINE_SCHEDULER:
+        WARMUP: 10
+    
+    MAX_SCORE: 0.1 # If IoU > max_score, save model
+    SAVE_INTERVAL: 5 # save model frequency
+    STEP_WISE_LOG: False # log metrics every step/update
+    VERBOSE: True
+
+EVAL:
+    PATCHSIZE: 512
+
+EXP:
+    FOLDER: outputs
+    NOTE: debug
+    NAME: ${DATA.TRAIN_MASK}_${MODEL.ARCH}_${DATA.SATELLITES}_${EXP.NOTE}
+    OUTPUT: ./${EXP.FOLDER}/run_${EXP.NAME}_${now:%Y%m%dT%H%M%S} #${defaults.0.data}
+
+hydra:
+    run:
+        dir: ${EXP.OUTPUT}
\ No newline at end of file
diff --git a/config/unet_V0.yaml b/config/unet_V0.yaml
new file mode 100644
index 0000000..26e4ab3
--- /dev/null
+++ b/config/unet_V0.yaml
@@ -0,0 +1,78 @@
+
+# defaults:
+#     - s1s2_unet: s1
+
+project:
+    name: IGARSS-2022
+    entity: wildfire
+
+data:
+    # dir: /cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ak-tiles
+    # dir: /cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ca-tiles
+    # dir: D:\wildfire-s1s2-dataset-ak-tiles
+    dir: /home/p/u/puzhao/wildfire-s1s2-dataset-ca-tiles
+
+    name: s1s2
+    satellites: ['S1','S2']
+    prepost: ['pre', 'post']
+    stacking: True # stack bi-temporal data
+
+    ALL_BANDS:
+        S1: ['ND', 'VH', 'VV']
+        ALOS: ['ND', 'VH', 'VV']
+        S2: ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']
+
+    INPUT_BANDS:
+        S1: ['ND', 'VH', 'VV']
+        ALOS: ['ND', 'VH', 'VV']
+        S2: ['B2', 'B3', 'B4', 'B8', 'B11', 'B12'] #['B4', 'B8', 'B12']
+        # S2: ['B3', 'B8', 'B12'] #['B4', 'B8', 'B12']
+
+    useDataWoCAug: False
+    SEED: 42
+    random_state: 42
+
+    # CLASSES: ['unburn', 'burned']
+    # CLASSES: ['burned']
+    REF_MASK: poly
+    train_ratio: 0.9
+
+MODEL:
+    DEBUG: False
+    ARCH: UNet #FuseUNet, UNet
+    TOPO: [16, 32, 64, 128]
+    LOSS_TYPE: BCEWithLogitsLoss # smpDiceLoss
+    NUM_CLASSES: 2
+
+    ACTIVATION: argmax2d #sigmoid
+    ENCODER: resnet18 # 'mobilenet_v2'
+    ENCODER_WEIGHTS: imagenet
+
+    max_epoch: 100
+    batch_size: 64
+
+    learning_rate: 1e-4
+    weight_decay: 1e-4
+    LR_SCHEDULER:  poly # ['cosine', 'poly']
+    POLY_SCHEDULER:
+        END_LR: 1e-5
+        POWER: 0.9
+    COSINE_SCHEDULER:
+        WARMUP: 2
+    
+    max_score: 0.1 # If IoU > max_score, save model
+    save_interval: 5 # save model frequency
+    STEP_WISE_LOG: False
+    verbose: True
+
+eval:
+    patchsize: 512
+
+experiment:
+    note: 
+    name: ${data.name}_${model.ARCH}_${data.satellites}_${experiment.note}
+    output: ./outputs/run_${experiment.name}_${now:%Y%m%dT%H%M%S} #${defaults.0.data}
+
+hydra:
+    run:
+        dir: ${experiment.output}
\ No newline at end of file
diff --git a/dataset/augument.py b/dataset/augument.py
index 0d5e0f8..3513aaa 100644
--- a/dataset/augument.py
+++ b/dataset/augument.py
@@ -1,43 +1,45 @@
+from operator import index
 import albumentations as albu
+from matplotlib import transforms
 
 def get_training_augmentation():
     train_transform = [
 
-        albu.HorizontalFlip(p=0.5),
+        albu.HorizontalFlip(p=1),
 
-        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),
+        # albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),
 
-        albu.PadIfNeeded(min_height=256, min_width=256, always_apply=True, border_mode=0),
-        albu.RandomCrop(height=256, width=256, always_apply=True),
+        # albu.PadIfNeeded(min_height=256, min_width=256, always_apply=True, border_mode=0),
+        # albu.RandomCrop(height=256, width=256, always_apply=True),
 
-        albu.IAAAdditiveGaussianNoise(p=0.2),
-        albu.IAAPerspective(p=0.5),
+        # albu.IAAAdditiveGaussianNoise(p=0.2),
+        # albu.IAAPerspective(p=0.5),
 
         # albu.OneOf(
         #     [
-        #         albu.CLAHE(p=1),
-        #         albu.RandomBrightness(p=1),
-        #         albu.RandomGamma(p=1),
+        #         # albu.channel_shuffle(),
+        #         albu.ChannelDropout(channel_drop_range=(1,1), fill_value=0, p=1),
+        #         albu.InvertImg(p=1),
         #     ],
         #     p=0.9,
         # ),
 
-        albu.OneOf(
-            [
-                albu.IAASharpen(p=1),
-                albu.Blur(blur_limit=3, p=1),
-                albu.MotionBlur(blur_limit=3, p=1),
-            ],
-            p=0.9,
-        ),
-
-        albu.OneOf(
-            [
-                albu.RandomContrast(p=1),
-                albu.HueSaturationValue(p=1),
-            ],
-            p=0.9,
-        ),
+        # albu.OneOf(
+        #     [
+        #         albu.IAASharpen(p=1),
+        #         albu.Blur(blur_limit=3, p=1),
+        #         # albu.MotionBlur(blur_limit=3, p=1),
+        #     ],
+        #     p=0.9,
+        # ),
+
+        # albu.OneOf(
+        #     [
+        #         albu.RandomContrast(p=1),
+        #         albu.HueSaturationValue(p=1),
+        #     ],
+        #     p=0.9,
+        # ),
     ]
     return albu.Compose(train_transform)
 
@@ -45,7 +47,7 @@ def get_training_augmentation():
 def get_validation_augmentation():
     """Add paddings to make image shape divisible by 32"""
     test_transform = [
-        albu.PadIfNeeded(384, 480)
+        albu.PadIfNeeded(256, 256)
     ]
     return albu.Compose(test_transform)
 
@@ -69,4 +71,95 @@ def get_preprocessing(preprocessing_fn):
         albu.Lambda(image=preprocessing_fn),
         albu.Lambda(image=to_tensor, mask=to_tensor),
     ]
-    return albu.Compose(_transform)
\ No newline at end of file
+    return albu.Compose(_transform)
+
+
+
+
+import torchvision.transforms as T
+import matplotlib.pyplot as plt
+
+def augment_data(imgs):
+    '''
+    imgs: list of array
+    '''
+    inputs = torch.cat(imgs, dim=0)
+    channels_list = [im.shape[0] for im in imgs]
+    idxs = [np.sum(np.array(channels_list[:i+1])) for i in range(0,len(channels_list))]
+    idxs = [0] + idxs
+
+    _transforms = T.Compose([
+        T.RandomVerticalFlip(p=0.5),
+        T.RandomHorizontalFlip(p=0.5),
+        T.RandomRotation(degrees=270),
+        T.RandomResizedCrop(size=(256,256), scale=(0.2,1), interpolation=T.InterpolationMode.NEAREST),
+        # T.ToTensor()
+    ])
+
+    input_aug = _transforms(inputs)
+    outputs = [input_aug[idxs[i]:idxs[i+1]] for i in range(len(imgs))]
+    return outputs
+
+
+if __name__ == "__main__":
+
+    from pathlib import Path
+    import os
+    import tifffile as tiff
+    import torch
+    import numpy as np
+
+    train_dir = Path('/home/p/u/puzhao/wildfire-s1s2-dataset-ca-tiles') / "test"
+    fileList = os.listdir(train_dir / "mask" / "poly")
+    
+    filename = fileList[400]
+
+    im1 = tiff.imread(train_dir / "S2" / "pre" / f"{filename}")[[5,3,2],] 
+    im2 = tiff.imread(train_dir / "S2" / "post" / f"{filename}")[[5,3,2],] 
+    mask = tiff.imread(train_dir / "mask" / "poly" / f"{filename}")
+
+    # mean=[0.485, 0.456, 0.406],
+    # std=[0.229, 0.224, 0.225]
+
+    im1 = np.nan_to_num(im1, 0)
+    im2 = np.nan_to_num(im2, 0)
+
+    min, max = im2.min(), im2.max()
+    im1 = (im1 - min) / (max - min)
+    im2 = (im2 - min) / (max - min)
+
+
+    # plt.imsave("aug_org.png", im1.transpose(1,2,0))
+
+    im1 = torch.from_numpy(im1)
+    im2 = torch.from_numpy(im2)
+    mask = torch.from_numpy(mask[np.newaxis,])
+
+    imgs = [im1, im2, mask]
+
+    # imgs = [(im*255).type(torch.uint8) for im in imgs]
+
+    outputs = augment_data(imgs)
+    print(type(outputs[0]))
+    print(len(fileList))
+    # print(np.sum((im1_ - im2_).type(torch.float16)))
+
+    # print(im1_.shape, mask_.shape)
+    # print(im1_ - mask_)
+
+    cnt = 0
+    fig, axs = plt.subplots(nrows=len(imgs), ncols=2, constrained_layout=True)
+    for i in range(len(imgs)):
+        img = imgs[i]
+        img_ = outputs[i]
+
+        if len(img.shape) >= 3:
+            axs[i,0].imshow(img.numpy().transpose(1,2,0))
+            axs[i,1].imshow(img_.numpy().transpose(1,2,0))
+        else:
+            axs[i,0].imshow(img.numpy())
+            axs[i,1].imshow(img_.numpy())
+
+    plt.savefig("aug.png")
+
+    print(np.unique(im1))
\ No newline at end of file
diff --git a/dataset/wildfire.py b/dataset/wildfire.py
index 0daeed7..ed76eea 100644
--- a/dataset/wildfire.py
+++ b/dataset/wildfire.py
@@ -7,8 +7,9 @@ import tifffile as tiff
 from torch.utils.data import DataLoader
 from torch.utils.data import Dataset as BaseDataset
 
-class MTBS(BaseDataset):
-    """ MTBS Dataset. Read images, apply augmentation and preprocessing transformations.
+
+class S1S2(BaseDataset):
+    """ S1S2 Dataset. Read images, apply augmentation and preprocessing transformations.
     
     Args:
         images_dir (str): path to images folder
@@ -21,28 +22,45 @@ class MTBS(BaseDataset):
     
     """
     
-    CLASSES = ['bgd', 'unburn', 'low', 'moderate', 'high', 'greener', 'cloud']
+    CLASSES = ['unburn', 'burned']
     
     def __init__(
             self, 
-            images_dir, 
-            masks_dir, 
+            data_dir, 
+            cfg, 
             classes=None, 
             augmentation=None, 
             preprocessing=None,
     ):
+        self.cfg = cfg
+        self.band_index_dict = self.get_band_index_dict()
+        print(self.band_index_dict)
+
         # images_dir
-        images_dir = Path(images_dir)
+        data_dir = Path(data_dir)
 
-        # image and mask dir
-        pre_dir = images_dir / "pre"
-        post_dir = images_dir / "post"
-        masks_dir = images_dir / "mask"
+        self.phase = os.path.split(data_dir)[-1]
+        self.REF_MASK = cfg.DATA.TEST_MASK if self.phase == "test" else cfg.DATA.TRAIN_MASK
+        masks_dir = data_dir / "mask" / self.REF_MASK
 
-        # fps
-        self.ids = os.listdir(pre_dir)
-        self.pre_fps = [os.path.join(pre_dir, image_id) for image_id in self.ids]
-        self.post_fps = [os.path.join(post_dir, image_id) for image_id in self.ids]
+        print("data_dir: ", data_dir)
+
+        def get_fps(sat):
+            # image and mask dir
+            pre_dir = data_dir / sat / "pre"
+            post_dir = data_dir / sat / "post"
+            
+            # fps
+            ids = sorted(os.listdir(post_dir)) # modified on Jan-09
+            pre_fps = [os.path.join(pre_dir, image_id) for image_id in ids]
+            post_fps = [os.path.join(post_dir, image_id) for image_id in ids]
+            return pre_fps, post_fps, ids
+
+        self.fps_dict = {}
+        for sat in self.cfg.DATA.SATELLITES:
+            self.fps_dict[sat] = get_fps(sat)
+        # self.ids = self.fps_dict[sat][-1]
+        self.ids = sorted(self.fps_dict[self.cfg.DATA.SATELLITES[0]][-1])
         self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]
         
         # convert str names to class values on masks
@@ -54,41 +72,109 @@ class MTBS(BaseDataset):
     
     def __getitem__(self, i):
         
-        # read data
-        image_pre = tiff.imread(self.pre_fps[i])
-        image_post = tiff.imread(self.post_fps[i])
+        ''' read data '''
+        image_list = []
+        # for sat in sorted(self.fps_dict.keys()):
+        for sat in self.cfg.DATA.SATELLITES: # modified on Jan-9
+            post_fps = self.fps_dict[sat][1]
+            image_post = tiff.imread(post_fps[i]) # C*H*W
+            image_post = np.nan_to_num(image_post, 0)
+            if sat in ['S1','ALOS']: image_post = self.normalize_sar(image_post)
+            image_post = image_post[self.band_index_dict[sat],] # select bands
 
-        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
-        image = np.concatenate((image_pre, image_post), axis=0)
-        # image = image.transpose(1,2,0)
+            if 'pre' in self.cfg.DATA.PREPOST:
+                pre_fps = self.fps_dict[sat][0]
+                image_pre = tiff.imread(pre_fps[i])
+                image_pre = np.nan_to_num(image_pre, 0)
+                if sat in ['S1','ALOS']: image_pre = self.normalize_sar(image_pre)
+                image_pre = image_pre[self.band_index_dict[sat],] # select bands
+                
+                if self.cfg.DATA.STACKING: # if stacking bi-temporal data
+                    stacked = np.concatenate((image_pre, image_post), axis=0) 
+                    image_list.append(stacked) #[x1, x2]
+                else:
+                    image_list += [image_pre, image_post] #[t1, t2]
+            else:
+                image_list.append(image_post) #[x1_t2, x2_t2]
+
+        ''' read mask '''
         mask = tiff.imread(self.masks_fps[i])
-        mask[mask==0] = 1 # set background as unburned 1, added by Puzhao on June 2
-        mask = mask.astype(float) - 1
-                        
-        # # extract certain classes from mask (e.g. cars)
-        # masks = [(mask == v) for v in self.class_values] # 1~6
-        # # mask = np.stack(masks, axis=-1).astype('float')
-        # mask = np.stack(masks, axis=0).astype('float') # modified by puzhao on June 1st, 2021
-        
-        # apply augmentations
-        if self.augmentation:
-            sample = self.augmentation(image=image, mask=mask)
-            image, mask = sample['image'], sample['mask']
-        
-        # apply preprocessing
-        if self.preprocessing:
-            sample = self.preprocessing(image=image, mask=mask)
-            image, mask = sample['image'], sample['mask']
+
+        if self.REF_MASK in ['modis', 'firecci']:
+            mask = (mask > 0).astype('float32')
+        
+        # if 'poly' == self.cfg.DATA.REF_MASK:
+        masks = [(mask == v) for v in self.class_values] # 1~6
+        mask = np.stack(masks, axis=0).astype('float32')
+        image_list.append(mask)
+        imgs = [torch.from_numpy(img) for img in image_list]
+
+        if 'train' == self.phase:
+            if self.cfg.DATA.AUGMENT:
+                imgs_aug = augment_data(imgs)
+                imgs = [torch.cat((x, x_aug), dim=0) for x, x_aug in zip(imgs, imgs_aug)] # # [x,x_aug], [y,y_aug]
             
-        # return image, mask
-        return image_pre, image_post, mask # edited on Setp. 6
+        return (tuple(imgs[:-1]), imgs[-1]) # x, y
+        
+        
+        # # apply preprocessing
+        # if self.preprocessing:
+        #     # sample = self.preprocessing(image=mask, mask=mask)
+        #     # image, mask = sample['image'], sample['mask']
+        #     image_list = [self.preprocessing(image=image.transpose(1,2,0))['image'].transpose(2,0,1) for image in image_list]
+
+        # return tuple(image_list)
+        # return (tuple(image_list[:-1]), image_list[-1])
+        
         
     def __len__(self):
         return len(self.ids)
 
+    def get_band_index_dict(self):
+        ALL_BANDS = self.cfg.DATA.ALL_BANDS
+        INPUT_BANDS = self.cfg.DATA.INPUT_BANDS
 
+        def get_band_index(sat):
+            all_bands = list(ALL_BANDS[sat])
+            input_bands = list(INPUT_BANDS[sat])
+            return [all_bands.index(band) for band in input_bands]
 
-class S1S2(BaseDataset):
+        band_index_dict = {}
+        for sat in ['ALOS', 'S1', 'S2']:
+            band_index_dict[sat] = get_band_index(sat)
+        
+        return band_index_dict
+
+    def normalize_sar(self, img):
+        return (np.clip(img, -30, 0) + 30) / 30
+
+
+''' Data Augments '''
+import torch
+import torchvision.transforms as T
+
+def augment_data(imgs):
+    '''
+    imgs: list of array
+    '''
+    inputs = torch.cat(imgs, dim=0)
+    channels_list = [im.shape[0] for im in imgs]
+    idxs = [np.sum(np.array(channels_list[:i+1])) for i in range(0,len(channels_list))]
+    idxs = [0] + idxs
+
+    _transforms = T.Compose([
+        T.RandomVerticalFlip(p=0.5),
+        T.RandomHorizontalFlip(p=0.5),
+        T.RandomRotation(degrees=270),
+        T.RandomResizedCrop(size=(256,256), scale=(0.2,1), interpolation=T.InterpolationMode.NEAREST),
+        # T.ToTensor()
+    ])
+
+    input_aug = _transforms(inputs)
+    outputs = [input_aug[idxs[i]:idxs[i+1]] for i in range(len(imgs))]
+    return outputs
+
+class MTBS(BaseDataset):
     """ MTBS Dataset. Read images, apply augmentation and preprocessing transformations.
     
     Args:
@@ -102,7 +188,7 @@ class S1S2(BaseDataset):
     
     """
     
-    CLASSES = ['unburn', 'burned']
+    CLASSES = ['unburn', 'low', 'moderate', 'high'] # 'greener', 'cloud'
     
     def __init__(
             self, 
@@ -113,10 +199,16 @@ class S1S2(BaseDataset):
             preprocessing=None,
     ):
         self.cfg = cfg
+        self.band_index_dict = self.get_band_index_dict()
+        print(self.band_index_dict)
 
         # images_dir
         data_dir = Path(data_dir)
-        masks_dir = data_dir / "mask" / "poly"
+
+        self.phase = os.path.split(data_dir)[-1]
+        self.REF_MASK = cfg.DATA.TEST_MASK if self.phase == "test" else cfg.DATA.TRAIN_MASK
+        masks_dir = data_dir / "mask" / self.REF_MASK
+
         print("data_dir: ", data_dir)
 
         def get_fps(sat):
@@ -125,16 +217,16 @@ class S1S2(BaseDataset):
             post_dir = data_dir / sat / "post"
             
             # fps
-            ids = os.listdir(post_dir)
+            ids = sorted(os.listdir(post_dir)) # modified on Jan-09
             pre_fps = [os.path.join(pre_dir, image_id) for image_id in ids]
             post_fps = [os.path.join(post_dir, image_id) for image_id in ids]
             return pre_fps, post_fps, ids
 
         self.fps_dict = {}
-        for sat in cfg.data.satellites:
+        for sat in self.cfg.DATA.SATELLITES:
             self.fps_dict[sat] = get_fps(sat)
-        self.ids = self.fps_dict[sat][-1]
-        
+        # self.ids = self.fps_dict[sat][-1]
+        self.ids = sorted(self.fps_dict[self.cfg.DATA.SATELLITES[0]][-1])
         self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]
         
         # convert str names to class values on masks
@@ -148,46 +240,84 @@ class S1S2(BaseDataset):
         
         ''' read data '''
         image_list = []
-        for sat in self.fps_dict.keys():
-
+        # for sat in sorted(self.fps_dict.keys()):
+        for sat in self.cfg.DATA.SATELLITES: # modified on Jan-9
             post_fps = self.fps_dict[sat][1]
             image_post = tiff.imread(post_fps[i]) # C*H*W
-            if sat in ['S1', 'ALOS']: image_post = (np.clip(image_post, -30, 0) + 30) / 30
+            image_post = np.nan_to_num(image_post, 0)
+            if sat in ['S1','ALOS']: image_post = self.normalize_sar(image_post)
+            image_post = image_post[self.band_index_dict[sat],] # select bands
 
-            if 'pre' in self.cfg.data.prepost:
+            if 'pre' in self.cfg.DATA.PREPOST:
                 pre_fps = self.fps_dict[sat][0]
                 image_pre = tiff.imread(pre_fps[i])
-                if sat in ['S1', 'ALOS']: image_pre = (np.clip(image_pre, -30, 0) + 30) / 30
+                image_pre = np.nan_to_num(image_pre, 0)
+                if sat in ['S1','ALOS']: image_pre = self.normalize_sar(image_pre)
+                image_pre = image_pre[self.band_index_dict[sat],] # select bands
                 
-                if self.cfg.data.stacking: # if stacking bi-temporal data
+                if self.cfg.DATA.STACKING: # if stacking bi-temporal data
                     stacked = np.concatenate((image_pre, image_post), axis=0) 
-                    image_list.append(stacked)
+                    image_list.append(stacked) #[x1, x2]
                 else:
-                    image_list += [image_pre, image_post]
+                    image_list += [image_pre, image_post] #[t1, t2]
             else:
-                image_list.append(image_post)
+                image_list.append(image_post) #[x1_t2, x2_t2]
 
         ''' read mask '''
         mask = tiff.imread(self.masks_fps[i])
-        
-        if 'poly' == self.cfg.data.REF_MASK:
+
+        if self.REF_MASK in ['modis', 'firecci']:
+            mask = (mask > 0).astype('float32')
+
+        if 'mtbs' == self.REF_MASK:
+            mask[mask==0] = 1 # both 0 and 1 are unburned
+            mask[mask>=5] = 0 # ignore 'greener' (5), 'cloud' (6)
+            mask = mask - 1 # 4 classes in total: 0, 1, 2, 3
+        else:
             masks = [(mask == v) for v in self.class_values] # 1~6
             mask = np.stack(masks, axis=0).astype('float32')
-            image_list.append(mask)
 
-        # apply augmentations
-        if self.augmentation:
-            # sample = self.augmentation(image=image, mask=mask)
-            # image, mask = sample['image'], sample['mask']
-            image_list = [self.augmentation(image=image.transpose(1,2,0))['image'].transpose(2,0,1) for image in image_list]
+        image_list.append(mask[np.newaxis,])
+        imgs = [torch.from_numpy(img) for img in image_list]
+
+        if 'train' == self.phase:
+            if self.cfg.DATA.AUGMENT:
+                imgs_aug = augment_data(imgs)
+                imgs = [torch.cat((x, x_aug), dim=0) for x, x_aug in zip(imgs, imgs_aug)] # # [x,x_aug], [y,y_aug]
+            
+        return (tuple(imgs[:-1]), imgs[-1]) # x, y
         
-        # apply preprocessing
-        if self.preprocessing:
-            # sample = self.preprocessing(image=mask, mask=mask)
-            # image, mask = sample['image'], sample['mask']
-            image_list = [self.preprocessing(image=image.transpose(1,2,0))['image'].transpose(2,0,1) for image in image_list]
+        
+        # # apply preprocessing
+        # if self.preprocessing:
+        #     # sample = self.preprocessing(image=mask, mask=mask)
+        #     # image, mask = sample['image'], sample['mask']
+        #     image_list = [self.preprocessing(image=image.transpose(1,2,0))['image'].transpose(2,0,1) for image in image_list]
 
-        return tuple(image_list)
+        # return tuple(image_list)
+        # return (tuple(image_list[:-1]), image_list[-1])
+        
         
     def __len__(self):
-        return len(self.ids)
\ No newline at end of file
+        return len(self.ids)
+
+    def get_band_index_dict(self):
+        ALL_BANDS = self.cfg.DATA.ALL_BANDS
+        INPUT_BANDS = self.cfg.DATA.INPUT_BANDS
+
+        def get_band_index(sat):
+            all_bands = list(ALL_BANDS[sat])
+            input_bands = list(INPUT_BANDS[sat])
+            return [all_bands.index(band) for band in input_bands]
+
+        band_index_dict = {}
+        for sat in ['ALOS', 'S1', 'S2']:
+            band_index_dict[sat] = get_band_index(sat)
+        
+        return band_index_dict
+
+    def normalize_sar(self, img):
+        return (np.clip(img, -30, 0) + 30) / 30
+
+
+
Submodule fcnn4cd contains untracked content
Submodule fcnn4cd contains modified content
Submodule fcnn4cd 0000000...4dd8323 (new submodule)
diff --git a/fcnn4cd/README.md b/fcnn4cd/README.md
new file mode 100644
index 0000000..c3159d1
--- /dev/null
+++ b/fcnn4cd/README.md
@@ -0,0 +1,7 @@
+# fully_convolutional_change_detection
+
+Fully convolutional network architectures for change detection using remote sensing images.
+
+[Rodrigo Caye Daudt, Bertrand Le Saux, Alexandre Boulch. (2018, October). Fully convolutional siamese networks for change detection. In 2018 25th IEEE International Conference on Image Processing (ICIP) (pp. 4063-4067). IEEE.](https://ieeexplore.ieee.org/abstract/document/8451652)
+
+[arXiv](https://arxiv.org/abs/1810.08462)
\ No newline at end of file
diff --git a/fcnn4cd/fresunet.py b/fcnn4cd/fresunet.py
new file mode 100644
index 0000000..a84b396
--- /dev/null
+++ b/fcnn4cd/fresunet.py
@@ -0,0 +1,203 @@
+# Rodrigo Caye Daudt
+# https://rcdaudt.github.io/
+# Daudt, R.C., Le Saux, B., Boulch, A. and Gousseau, Y., 2019. Multitask learning for large-scale semantic change detection. Computer Vision and Image Understanding, 187, p.102783.
+
+
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.nn.modules.padding import ReplicationPad2d
+
+def conv3x3(in_planes, out_planes, stride=1):
+    "3x3 convolution with padding"
+    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1)
+
+
+class BasicBlock_ss(nn.Module):
+
+    def __init__(self, inplanes, planes = None, subsamp=1):
+        super(BasicBlock_ss, self).__init__()
+        if planes == None:
+            planes = inplanes * subsamp
+        self.conv1 = conv3x3(inplanes, planes)
+        self.bn1 = nn.BatchNorm2d(planes)
+        self.relu = nn.ReLU(inplace=True)
+        self.conv2 = conv3x3(planes, planes)
+        self.bn2 = nn.BatchNorm2d(planes)
+        self.subsamp = subsamp
+        self.doit = planes != inplanes
+        if self.doit:
+            self.couple = nn.Conv2d(inplanes, planes, kernel_size=1)
+            self.bnc = nn.BatchNorm2d(planes)
+
+    def forward(self, x):
+        if self.doit:
+            residual = self.couple(x)
+            residual = self.bnc(residual)
+        else:
+            residual = x
+
+        out = self.conv1(x)
+        out = self.bn1(out)
+        out = self.relu(out)
+        
+        if self.subsamp > 1:
+            out = F.max_pool2d(out, kernel_size=self.subsamp, stride=self.subsamp)
+            residual = F.max_pool2d(residual, kernel_size=self.subsamp, stride=self.subsamp)
+
+        out = self.conv2(out)
+        out = self.bn2(out)
+        
+        out += residual
+        out = self.relu(out)
+
+        return out
+    
+
+    
+class BasicBlock_us(nn.Module):
+
+    def __init__(self, inplanes, upsamp=1):
+        super(BasicBlock_us, self).__init__()
+        planes = int(inplanes / upsamp) # assumes integer result, fix later
+        self.conv1 = nn.ConvTranspose2d(inplanes, planes, kernel_size=3, padding=1, stride=upsamp, output_padding=1)
+        self.bn1 = nn.BatchNorm2d(planes)
+        self.relu = nn.ReLU(inplace=True)
+        self.conv2 = conv3x3(planes, planes)
+        self.bn2 = nn.BatchNorm2d(planes)
+        self.upsamp = upsamp
+        self.couple = nn.ConvTranspose2d(inplanes, planes, kernel_size=3, padding=1, stride=upsamp, output_padding=1) 
+        self.bnc = nn.BatchNorm2d(planes)
+
+    def forward(self, x):
+        residual = self.couple(x)
+        residual = self.bnc(residual)
+
+        out = self.conv1(x)
+        out = self.bn1(out)
+        out = self.relu(out)
+        
+
+        out = self.conv2(out)
+        out = self.bn2(out)
+
+        out += residual
+        out = self.relu(out)
+
+        return out
+    
+    
+class FresUNet(nn.Module):
+    """FresUNet segmentation network."""
+
+    def __init__(self, input_nbr, label_nbr):
+        """Init FresUNet fields."""
+        super(FresUNet, self).__init__()
+
+        self.input_nbr = input_nbr
+        
+        cur_depth = input_nbr
+        
+        base_depth = 8
+        
+        # Encoding stage 1
+        self.encres1_1 = BasicBlock_ss(cur_depth, planes = base_depth)
+        cur_depth = base_depth
+        d1 = base_depth
+        self.encres1_2 = BasicBlock_ss(cur_depth, subsamp=2)
+        cur_depth *= 2
+        
+        # Encoding stage 2
+        self.encres2_1 = BasicBlock_ss(cur_depth)
+        d2 = cur_depth
+        self.encres2_2 = BasicBlock_ss(cur_depth, subsamp=2)
+        cur_depth *= 2
+        
+        # Encoding stage 3
+        self.encres3_1 = BasicBlock_ss(cur_depth)
+        d3 = cur_depth
+        self.encres3_2 = BasicBlock_ss(cur_depth, subsamp=2)
+        cur_depth *= 2
+        
+        # Encoding stage 4
+        self.encres4_1 = BasicBlock_ss(cur_depth)
+        d4 = cur_depth
+        self.encres4_2 = BasicBlock_ss(cur_depth, subsamp=2)
+        cur_depth *= 2
+        
+        # Decoding stage 4
+        self.decres4_1 = BasicBlock_ss(cur_depth)
+        self.decres4_2 = BasicBlock_us(cur_depth, upsamp=2)
+        cur_depth = int(cur_depth/2)
+        
+        # Decoding stage 3
+        self.decres3_1 = BasicBlock_ss(cur_depth + d4, planes = cur_depth)
+        self.decres3_2 = BasicBlock_us(cur_depth, upsamp=2)
+        cur_depth = int(cur_depth/2)
+        
+        # Decoding stage 2
+        self.decres2_1 = BasicBlock_ss(cur_depth + d3, planes = cur_depth)
+        self.decres2_2 = BasicBlock_us(cur_depth, upsamp=2)
+        cur_depth = int(cur_depth/2)
+        
+        # Decoding stage 1
+        self.decres1_1 = BasicBlock_ss(cur_depth + d2, planes = cur_depth)
+        self.decres1_2 = BasicBlock_us(cur_depth, upsamp=2)
+        cur_depth = int(cur_depth/2)
+        
+        # Output
+        self.coupling = nn.Conv2d(cur_depth + d1, label_nbr, kernel_size=1)
+        self.sm = nn.LogSoftmax(dim=1)
+        
+    def forward(self, x1, x2):
+
+        x = torch.cat((x1, x2), 1)
+        
+#         pad5 = ReplicationPad2d((0, x53.size(3) - x5d.size(3), 0, x53.size(2) - x5d.size(2)))
+        
+        s1_1 = x.size()
+        x1 = self.encres1_1(x)
+        x = self.encres1_2(x1)
+        
+        s2_1 = x.size()
+        x2 = self.encres2_1(x)
+        x = self.encres2_2(x2)
+        
+        s3_1 = x.size()
+        x3 = self.encres3_1(x)
+        x = self.encres3_2(x3)
+        
+        s4_1 = x.size()
+        x4 = self.encres4_1(x)
+        x = self.encres4_2(x4)
+        
+        x = self.decres4_1(x)
+        x = self.decres4_2(x)
+        s4_2 = x.size()
+        pad4 = ReplicationPad2d((0, s4_1[3] - s4_2[3], 0, s4_1[2] - s4_2[2]))
+        x = pad4(x)
+        
+        # x = self.decres3_1(x)
+        x = self.decres3_1(torch.cat((x, x4), 1))
+        x = self.decres3_2(x)
+        s3_2 = x.size()
+        pad3 = ReplicationPad2d((0, s3_1[3] - s3_2[3], 0, s3_1[2] - s3_2[2]))
+        x = pad3(x)
+        
+        x = self.decres2_1(torch.cat((x, x3), 1))
+        x = self.decres2_2(x)
+        s2_2 = x.size()
+        pad2 = ReplicationPad2d((0, s2_1[3] - s2_2[3], 0, s2_1[2] - s2_2[2]))
+        x = pad2(x)
+        
+        x = self.decres1_1(torch.cat((x, x2), 1))
+        x = self.decres1_2(x)
+        s1_2 = x.size()
+        pad1 = ReplicationPad2d((0, s1_1[3] - s1_2[3], 0, s1_1[2] - s1_2[2]))
+        x = pad1(x)
+        
+        x = self.coupling(torch.cat((x, x1), 1))
+        x = self.sm(x)
+        
+        return x
\ No newline at end of file
diff --git a/fcnn4cd/fully-convolutional-change-detection.ipynb b/fcnn4cd/fully-convolutional-change-detection.ipynb
new file mode 100644
index 0000000..de89932
--- /dev/null
+++ b/fcnn4cd/fully-convolutional-change-detection.ipynb
@@ -0,0 +1,1019 @@
+{
+ "cells": [
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# Fully Convolutional Networks for Change Detection\n",
+    "\n",
+    "Example code for training the network presented in the paper:\n",
+    "\n",
+    "```\n",
+    "Daudt, R.C., Le Saux, B. and Boulch, A., 2018, October. Fully convolutional siamese networks for change detection. In 2018 25th IEEE International Conference on Image Processing (ICIP) (pp. 4063-4067). IEEE.\n",
+    "```\n",
+    "\n",
+    "Code uses the OSCD dataset:\n",
+    "\n",
+    "```\n",
+    "Daudt, R.C., Le Saux, B., Boulch, A. and Gousseau, Y., 2018, July. Urban change detection for multispectral earth observation using convolutional neural networks. In IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium (pp. 2115-2118). IEEE.\n",
+    "```\n",
+    "\n",
+    "\n",
+    "FresUNet architecture from paper:\n",
+    "\n",
+    "```\n",
+    "Daudt, R.C., Le Saux, B., Boulch, A. and Gousseau, Y., 2019. Multitask learning for large-scale semantic change detection. Computer Vision and Image Understanding, 187, p.102783.\n",
+    "```\n",
+    "\n",
+    "Please consider all relevant papers if you use this code."
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Rodrigo Daudt\n",
+    "# rcdaudt.github.io\n",
+    "# rodrigo.daudt@onera.fr"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Imports\n",
+    "\n",
+    "# PyTorch\n",
+    "import torch\n",
+    "import torch.nn as nn\n",
+    "from torch.utils.data import Dataset, DataLoader\n",
+    "from torch.autograd import Variable\n",
+    "import torchvision.transforms as tr\n",
+    "\n",
+    "# Models\n",
+    "from unet import Unet\n",
+    "from siamunet_conc import SiamUnet_conc\n",
+    "from siamunet_diff import SiamUnet_diff\n",
+    "from fresunet import FresUNet\n",
+    "\n",
+    "# Other\n",
+    "import os\n",
+    "import numpy as np\n",
+    "import random\n",
+    "from skimage import io\n",
+    "from scipy.ndimage import zoom\n",
+    "import matplotlib.pyplot as plt\n",
+    "%matplotlib inline\n",
+    "from tqdm import tqdm as tqdm\n",
+    "from pandas import read_csv\n",
+    "from math import floor, ceil, sqrt, exp\n",
+    "from IPython import display\n",
+    "import time\n",
+    "from itertools import chain\n",
+    "import time\n",
+    "import warnings\n",
+    "from pprint import pprint\n",
+    "\n",
+    "\n",
+    "\n",
+    "print('IMPORTS OK')\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Global Variables' Definitions\n",
+    "\n",
+    "PATH_TO_DATASET = './OSCD/'\n",
+    "IS_PROTOTYPE = False\n",
+    "\n",
+    "FP_MODIFIER = 10 # Tuning parameter, use 1 if unsure\n",
+    "\n",
+    "BATCH_SIZE = 32\n",
+    "PATCH_SIDE = 96\n",
+    "N_EPOCHS = 50\n",
+    "\n",
+    "NORMALISE_IMGS = True\n",
+    "\n",
+    "TRAIN_STRIDE = int(PATCH_SIDE/2) - 1\n",
+    "\n",
+    "TYPE = 3 # 0-RGB | 1-RGBIr | 2-All bands s.t. resulution <= 20m | 3-All bands\n",
+    "\n",
+    "LOAD_TRAINED = False\n",
+    "\n",
+    "DATA_AUG = True\n",
+    "\n",
+    "\n",
+    "print('DEFINITIONS OK')"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Functions\n",
+    "\n",
+    "def adjust_shape(I, s):\n",
+    "    \"\"\"Adjust shape of grayscale image I to s.\"\"\"\n",
+    "    \n",
+    "    # crop if necesary\n",
+    "    I = I[:s[0],:s[1]]\n",
+    "    si = I.shape\n",
+    "    \n",
+    "    # pad if necessary \n",
+    "    p0 = max(0,s[0] - si[0])\n",
+    "    p1 = max(0,s[1] - si[1])\n",
+    "    \n",
+    "    return np.pad(I,((0,p0),(0,p1)),'edge')\n",
+    "    \n",
+    "\n",
+    "def read_sentinel_img(path):\n",
+    "    \"\"\"Read cropped Sentinel-2 image: RGB bands.\"\"\"\n",
+    "    im_name = os.listdir(path)[0][:-7]\n",
+    "    r = io.imread(path + im_name + \"B04.tif\")\n",
+    "    g = io.imread(path + im_name + \"B03.tif\")\n",
+    "    b = io.imread(path + im_name + \"B02.tif\")\n",
+    "    \n",
+    "    I = np.stack((r,g,b),axis=2).astype('float')\n",
+    "    \n",
+    "    if NORMALISE_IMGS:\n",
+    "        I = (I - I.mean()) / I.std()\n",
+    "\n",
+    "    return I\n",
+    "\n",
+    "def read_sentinel_img_4(path):\n",
+    "    \"\"\"Read cropped Sentinel-2 image: RGB and NIR bands.\"\"\"\n",
+    "    im_name = os.listdir(path)[0][:-7]\n",
+    "    r = io.imread(path + im_name + \"B04.tif\")\n",
+    "    g = io.imread(path + im_name + \"B03.tif\")\n",
+    "    b = io.imread(path + im_name + \"B02.tif\")\n",
+    "    nir = io.imread(path + im_name + \"B08.tif\")\n",
+    "    \n",
+    "    I = np.stack((r,g,b,nir),axis=2).astype('float')\n",
+    "    \n",
+    "    if NORMALISE_IMGS:\n",
+    "        I = (I - I.mean()) / I.std()\n",
+    "\n",
+    "    return I\n",
+    "\n",
+    "def read_sentinel_img_leq20(path):\n",
+    "    \"\"\"Read cropped Sentinel-2 image: bands with resolution less than or equals to 20m.\"\"\"\n",
+    "    im_name = os.listdir(path)[0][:-7]\n",
+    "    \n",
+    "    r = io.imread(path + im_name + \"B04.tif\")\n",
+    "    s = r.shape\n",
+    "    g = io.imread(path + im_name + \"B03.tif\")\n",
+    "    b = io.imread(path + im_name + \"B02.tif\")\n",
+    "    nir = io.imread(path + im_name + \"B08.tif\")\n",
+    "    \n",
+    "    ir1 = adjust_shape(zoom(io.imread(path + im_name + \"B05.tif\"),2),s)\n",
+    "    ir2 = adjust_shape(zoom(io.imread(path + im_name + \"B06.tif\"),2),s)\n",
+    "    ir3 = adjust_shape(zoom(io.imread(path + im_name + \"B07.tif\"),2),s)\n",
+    "    nir2 = adjust_shape(zoom(io.imread(path + im_name + \"B8A.tif\"),2),s)\n",
+    "    swir2 = adjust_shape(zoom(io.imread(path + im_name + \"B11.tif\"),2),s)\n",
+    "    swir3 = adjust_shape(zoom(io.imread(path + im_name + \"B12.tif\"),2),s)\n",
+    "    \n",
+    "    I = np.stack((r,g,b,nir,ir1,ir2,ir3,nir2,swir2,swir3),axis=2).astype('float')\n",
+    "    \n",
+    "    if NORMALISE_IMGS:\n",
+    "        I = (I - I.mean()) / I.std()\n",
+    "\n",
+    "    return I\n",
+    "\n",
+    "def read_sentinel_img_leq60(path):\n",
+    "    \"\"\"Read cropped Sentinel-2 image: all bands.\"\"\"\n",
+    "    im_name = os.listdir(path)[0][:-7]\n",
+    "    \n",
+    "    r = io.imread(path + im_name + \"B04.tif\")\n",
+    "    s = r.shape\n",
+    "    g = io.imread(path + im_name + \"B03.tif\")\n",
+    "    b = io.imread(path + im_name + \"B02.tif\")\n",
+    "    nir = io.imread(path + im_name + \"B08.tif\")\n",
+    "    \n",
+    "    ir1 = adjust_shape(zoom(io.imread(path + im_name + \"B05.tif\"),2),s)\n",
+    "    ir2 = adjust_shape(zoom(io.imread(path + im_name + \"B06.tif\"),2),s)\n",
+    "    ir3 = adjust_shape(zoom(io.imread(path + im_name + \"B07.tif\"),2),s)\n",
+    "    nir2 = adjust_shape(zoom(io.imread(path + im_name + \"B8A.tif\"),2),s)\n",
+    "    swir2 = adjust_shape(zoom(io.imread(path + im_name + \"B11.tif\"),2),s)\n",
+    "    swir3 = adjust_shape(zoom(io.imread(path + im_name + \"B12.tif\"),2),s)\n",
+    "    \n",
+    "    uv = adjust_shape(zoom(io.imread(path + im_name + \"B01.tif\"),6),s)\n",
+    "    wv = adjust_shape(zoom(io.imread(path + im_name + \"B09.tif\"),6),s)\n",
+    "    swirc = adjust_shape(zoom(io.imread(path + im_name + \"B10.tif\"),6),s)\n",
+    "    \n",
+    "    I = np.stack((r,g,b,nir,ir1,ir2,ir3,nir2,swir2,swir3,uv,wv,swirc),axis=2).astype('float')\n",
+    "    \n",
+    "    if NORMALISE_IMGS:\n",
+    "        I = (I - I.mean()) / I.std()\n",
+    "\n",
+    "    return I\n",
+    "\n",
+    "def read_sentinel_img_trio(path):\n",
+    "    \"\"\"Read cropped Sentinel-2 image pair and change map.\"\"\"\n",
+    "#     read images\n",
+    "    if TYPE == 0:\n",
+    "        I1 = read_sentinel_img(path + '/imgs_1/')\n",
+    "        I2 = read_sentinel_img(path + '/imgs_2/')\n",
+    "    elif TYPE == 1:\n",
+    "        I1 = read_sentinel_img_4(path + '/imgs_1/')\n",
+    "        I2 = read_sentinel_img_4(path + '/imgs_2/')\n",
+    "    elif TYPE == 2:\n",
+    "        I1 = read_sentinel_img_leq20(path + '/imgs_1/')\n",
+    "        I2 = read_sentinel_img_leq20(path + '/imgs_2/')\n",
+    "    elif TYPE == 3:\n",
+    "        I1 = read_sentinel_img_leq60(path + '/imgs_1/')\n",
+    "        I2 = read_sentinel_img_leq60(path + '/imgs_2/')\n",
+    "        \n",
+    "    cm = io.imread(path + '/cm/cm.png', as_gray=True) != 0\n",
+    "    \n",
+    "    # crop if necessary\n",
+    "    s1 = I1.shape\n",
+    "    s2 = I2.shape\n",
+    "    I2 = np.pad(I2,((0, s1[0] - s2[0]), (0, s1[1] - s2[1]), (0,0)),'edge')\n",
+    "    \n",
+    "    \n",
+    "    return I1, I2, cm\n",
+    "\n",
+    "\n",
+    "\n",
+    "def reshape_for_torch(I):\n",
+    "    \"\"\"Transpose image for PyTorch coordinates.\"\"\"\n",
+    "#     out = np.swapaxes(I,1,2)\n",
+    "#     out = np.swapaxes(out,0,1)\n",
+    "#     out = out[np.newaxis,:]\n",
+    "    out = I.transpose((2, 0, 1))\n",
+    "    return torch.from_numpy(out)\n",
+    "\n",
+    "\n",
+    "\n",
+    "class ChangeDetectionDataset(Dataset):\n",
+    "    \"\"\"Change Detection dataset class, used for both training and test data.\"\"\"\n",
+    "\n",
+    "    def __init__(self, path, train = True, patch_side = 96, stride = None, use_all_bands = False, transform=None):\n",
+    "        \"\"\"\n",
+    "        Args:\n",
+    "            csv_file (string): Path to the csv file with annotations.\n",
+    "            root_dir (string): Directory with all the images.\n",
+    "            transform (callable, optional): Optional transform to be applied\n",
+    "                on a sample.\n",
+    "        \"\"\"\n",
+    "        \n",
+    "        # basics\n",
+    "        self.transform = transform\n",
+    "        self.path = path\n",
+    "        self.patch_side = patch_side\n",
+    "        if not stride:\n",
+    "            self.stride = 1\n",
+    "        else:\n",
+    "            self.stride = stride\n",
+    "        \n",
+    "        if train:\n",
+    "            fname = 'train.txt'\n",
+    "        else:\n",
+    "            fname = 'test.txt'\n",
+    "        \n",
+    "#         print(path + fname)\n",
+    "        self.names = read_csv(path + fname).columns\n",
+    "        self.n_imgs = self.names.shape[0]\n",
+    "        \n",
+    "        n_pix = 0\n",
+    "        true_pix = 0\n",
+    "        \n",
+    "        \n",
+    "        # load images\n",
+    "        self.imgs_1 = {}\n",
+    "        self.imgs_2 = {}\n",
+    "        self.change_maps = {}\n",
+    "        self.n_patches_per_image = {}\n",
+    "        self.n_patches = 0\n",
+    "        self.patch_coords = []\n",
+    "        for im_name in tqdm(self.names):\n",
+    "            # load and store each image\n",
+    "            I1, I2, cm = read_sentinel_img_trio(self.path + im_name)\n",
+    "            self.imgs_1[im_name] = reshape_for_torch(I1)\n",
+    "            self.imgs_2[im_name] = reshape_for_torch(I2)\n",
+    "            self.change_maps[im_name] = cm\n",
+    "            \n",
+    "            s = cm.shape\n",
+    "            n_pix += np.prod(s)\n",
+    "            true_pix += cm.sum()\n",
+    "            \n",
+    "            # calculate the number of patches\n",
+    "            s = self.imgs_1[im_name].shape\n",
+    "            n1 = ceil((s[1] - self.patch_side + 1) / self.stride)\n",
+    "            n2 = ceil((s[2] - self.patch_side + 1) / self.stride)\n",
+    "            n_patches_i = n1 * n2\n",
+    "            self.n_patches_per_image[im_name] = n_patches_i\n",
+    "            self.n_patches += n_patches_i\n",
+    "            \n",
+    "            # generate path coordinates\n",
+    "            for i in range(n1):\n",
+    "                for j in range(n2):\n",
+    "                    # coordinates in (x1, x2, y1, y2)\n",
+    "                    current_patch_coords = (im_name, \n",
+    "                                    [self.stride*i, self.stride*i + self.patch_side, self.stride*j, self.stride*j + self.patch_side],\n",
+    "                                    [self.stride*(i + 1), self.stride*(j + 1)])\n",
+    "                    self.patch_coords.append(current_patch_coords)\n",
+    "                    \n",
+    "        self.weights = [ FP_MODIFIER * 2 * true_pix / n_pix, 2 * (n_pix - true_pix) / n_pix]\n",
+    "        \n",
+    "        \n",
+    "\n",
+    "    def get_img(self, im_name):\n",
+    "        return self.imgs_1[im_name], self.imgs_2[im_name], self.change_maps[im_name]\n",
+    "\n",
+    "    def __len__(self):\n",
+    "        return self.n_patches\n",
+    "\n",
+    "    def __getitem__(self, idx):\n",
+    "        current_patch_coords = self.patch_coords[idx]\n",
+    "        im_name = current_patch_coords[0]\n",
+    "        limits = current_patch_coords[1]\n",
+    "        centre = current_patch_coords[2]\n",
+    "        \n",
+    "        I1 = self.imgs_1[im_name][:, limits[0]:limits[1], limits[2]:limits[3]]\n",
+    "        I2 = self.imgs_2[im_name][:, limits[0]:limits[1], limits[2]:limits[3]]\n",
+    "        \n",
+    "        label = self.change_maps[im_name][limits[0]:limits[1], limits[2]:limits[3]]\n",
+    "        label = torch.from_numpy(1*np.array(label)).float()\n",
+    "        \n",
+    "        sample = {'I1': I1, 'I2': I2, 'label': label}\n",
+    "        \n",
+    "        if self.transform:\n",
+    "            sample = self.transform(sample)\n",
+    "\n",
+    "        return sample\n",
+    "\n",
+    "\n",
+    "\n",
+    "class RandomFlip(object):\n",
+    "    \"\"\"Flip randomly the images in a sample.\"\"\"\n",
+    "\n",
+    "#     def __init__(self):\n",
+    "#         return\n",
+    "\n",
+    "    def __call__(self, sample):\n",
+    "        I1, I2, label = sample['I1'], sample['I2'], sample['label']\n",
+    "        \n",
+    "        if random.random() > 0.5:\n",
+    "            I1 =  I1.numpy()[:,:,::-1].copy()\n",
+    "            I1 = torch.from_numpy(I1)\n",
+    "            I2 =  I2.numpy()[:,:,::-1].copy()\n",
+    "            I2 = torch.from_numpy(I2)\n",
+    "            label =  label.numpy()[:,::-1].copy()\n",
+    "            label = torch.from_numpy(label)\n",
+    "\n",
+    "        return {'I1': I1, 'I2': I2, 'label': label}\n",
+    "\n",
+    "\n",
+    "\n",
+    "class RandomRot(object):\n",
+    "    \"\"\"Rotate randomly the images in a sample.\"\"\"\n",
+    "\n",
+    "#     def __init__(self):\n",
+    "#         return\n",
+    "\n",
+    "    def __call__(self, sample):\n",
+    "        I1, I2, label = sample['I1'], sample['I2'], sample['label']\n",
+    "        \n",
+    "        n = random.randint(0, 3)\n",
+    "        if n:\n",
+    "            I1 =  sample['I1'].numpy()\n",
+    "            I1 = np.rot90(I1, n, axes=(1, 2)).copy()\n",
+    "            I1 = torch.from_numpy(I1)\n",
+    "            I2 =  sample['I2'].numpy()\n",
+    "            I2 = np.rot90(I2, n, axes=(1, 2)).copy()\n",
+    "            I2 = torch.from_numpy(I2)\n",
+    "            label =  sample['label'].numpy()\n",
+    "            label = np.rot90(label, n, axes=(0, 1)).copy()\n",
+    "            label = torch.from_numpy(label)\n",
+    "\n",
+    "        return {'I1': I1, 'I2': I2, 'label': label}\n",
+    "\n",
+    "\n",
+    "\n",
+    "\n",
+    "\n",
+    "print('UTILS OK')\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Dataset\n",
+    "\n",
+    "\n",
+    "if DATA_AUG:\n",
+    "    data_transform = tr.Compose([RandomFlip(), RandomRot()])\n",
+    "else:\n",
+    "    data_transform = None\n",
+    "\n",
+    "\n",
+    "        \n",
+    "\n",
+    "train_dataset = ChangeDetectionDataset(PATH_TO_DATASET, train = True, stride = TRAIN_STRIDE, transform=data_transform)\n",
+    "weights = torch.FloatTensor(train_dataset.weights).cuda()\n",
+    "print(weights)\n",
+    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4)\n",
+    "\n",
+    "test_dataset = ChangeDetectionDataset(PATH_TO_DATASET, train = False, stride = TRAIN_STRIDE)\n",
+    "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4)\n",
+    "\n",
+    "\n",
+    "print('DATASETS OK')"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# print(weights)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# 0-RGB | 1-RGBIr | 2-All bands s.t. resulution <= 20m | 3-All bands\n",
+    "\n",
+    "if TYPE == 0:\n",
+    "#     net, net_name = Unet(2*3, 2), 'FC-EF'\n",
+    "#     net, net_name = SiamUnet_conc(3, 2), 'FC-Siam-conc'\n",
+    "#     net, net_name = SiamUnet_diff(3, 2), 'FC-Siam-diff'\n",
+    "    net, net_name = FresUNet(2*3, 2), 'FresUNet'\n",
+    "elif TYPE == 1:\n",
+    "#     net, net_name = Unet(2*4, 2), 'FC-EF'\n",
+    "#     net, net_name = SiamUnet_conc(4, 2), 'FC-Siam-conc'\n",
+    "#     net, net_name = SiamUnet_diff(4, 2), 'FC-Siam-diff'\n",
+    "    net, net_name = FresUNet(2*4, 2), 'FresUNet'\n",
+    "elif TYPE == 2:\n",
+    "#     net, net_name = Unet(2*10, 2), 'FC-EF'\n",
+    "#     net, net_name = SiamUnet_conc(10, 2), 'FC-Siam-conc'\n",
+    "#     net, net_name = SiamUnet_diff(10, 2), 'FC-Siam-diff'\n",
+    "    net, net_name = FresUNet(2*10, 2), 'FresUNet'\n",
+    "elif TYPE == 3:\n",
+    "#     net, net_name = Unet(2*13, 2), 'FC-EF'\n",
+    "#     net, net_name = SiamUnet_conc(13, 2), 'FC-Siam-conc'\n",
+    "#     net, net_name = SiamUnet_diff(13, 2), 'FC-Siam-diff'\n",
+    "    net, net_name = FresUNet(2*13, 2), 'FresUNet'\n",
+    "\n",
+    "\n",
+    "net.cuda()\n",
+    "\n",
+    "criterion = nn.NLLLoss(weight=weights) # to be used with logsoftmax output\n",
+    "\n",
+    "print('NETWORK OK')"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "def count_parameters(model):\n",
+    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
+    "\n",
+    "print('Number of trainable parameters:', count_parameters(net))"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# net.load_state_dict(torch.load('net-best_epoch-1_fm-0.7394933126157746.pth.tar'))\n",
+    "\n",
+    "def train(n_epochs = N_EPOCHS, save = True):\n",
+    "    t = np.linspace(1, n_epochs, n_epochs)\n",
+    "    \n",
+    "    epoch_train_loss = 0 * t\n",
+    "    epoch_train_accuracy = 0 * t\n",
+    "    epoch_train_change_accuracy = 0 * t\n",
+    "    epoch_train_nochange_accuracy = 0 * t\n",
+    "    epoch_train_precision = 0 * t\n",
+    "    epoch_train_recall = 0 * t\n",
+    "    epoch_train_Fmeasure = 0 * t\n",
+    "    epoch_test_loss = 0 * t\n",
+    "    epoch_test_accuracy = 0 * t\n",
+    "    epoch_test_change_accuracy = 0 * t\n",
+    "    epoch_test_nochange_accuracy = 0 * t\n",
+    "    epoch_test_precision = 0 * t\n",
+    "    epoch_test_recall = 0 * t\n",
+    "    epoch_test_Fmeasure = 0 * t\n",
+    "    \n",
+    "#     mean_acc = 0\n",
+    "#     best_mean_acc = 0\n",
+    "    fm = 0\n",
+    "    best_fm = 0\n",
+    "    \n",
+    "    lss = 1000\n",
+    "    best_lss = 1000\n",
+    "    \n",
+    "    plt.figure(num=1)\n",
+    "    plt.figure(num=2)\n",
+    "    plt.figure(num=3)\n",
+    "    \n",
+    "    \n",
+    "    optimizer = torch.optim.Adam(net.parameters(), weight_decay=1e-4)\n",
+    "#     optimizer = torch.optim.Adam(net.parameters(), lr=0.0005)\n",
+    "        \n",
+    "    \n",
+    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.95)\n",
+    "    \n",
+    "    \n",
+    "    for epoch_index in tqdm(range(n_epochs)):\n",
+    "        net.train()\n",
+    "        print('Epoch: ' + str(epoch_index + 1) + ' of ' + str(N_EPOCHS))\n",
+    "\n",
+    "        tot_count = 0\n",
+    "        tot_loss = 0\n",
+    "        tot_accurate = 0\n",
+    "        class_correct = list(0. for i in range(2))\n",
+    "        class_total = list(0. for i in range(2))\n",
+    "#         for batch_index, batch in enumerate(tqdm(data_loader)):\n",
+    "        for batch in train_loader:\n",
+    "            I1 = Variable(batch['I1'].float().cuda())\n",
+    "            I2 = Variable(batch['I2'].float().cuda())\n",
+    "            label = torch.squeeze(Variable(batch['label'].cuda()))\n",
+    "\n",
+    "            optimizer.zero_grad()\n",
+    "            output = net(I1, I2)\n",
+    "            loss = criterion(output, label.long())\n",
+    "            loss.backward()\n",
+    "            optimizer.step()\n",
+    "            \n",
+    "        scheduler.step()\n",
+    "\n",
+    "\n",
+    "        epoch_train_loss[epoch_index], epoch_train_accuracy[epoch_index], cl_acc, pr_rec = test(train_dataset)\n",
+    "        epoch_train_nochange_accuracy[epoch_index] = cl_acc[0]\n",
+    "        epoch_train_change_accuracy[epoch_index] = cl_acc[1]\n",
+    "        epoch_train_precision[epoch_index] = pr_rec[0]\n",
+    "        epoch_train_recall[epoch_index] = pr_rec[1]\n",
+    "        epoch_train_Fmeasure[epoch_index] = pr_rec[2]\n",
+    "        \n",
+    "#         epoch_test_loss[epoch_index], epoch_test_accuracy[epoch_index], cl_acc, pr_rec = test(test_dataset)\n",
+    "        epoch_test_loss[epoch_index], epoch_test_accuracy[epoch_index], cl_acc, pr_rec = test(test_dataset)\n",
+    "        epoch_test_nochange_accuracy[epoch_index] = cl_acc[0]\n",
+    "        epoch_test_change_accuracy[epoch_index] = cl_acc[1]\n",
+    "        epoch_test_precision[epoch_index] = pr_rec[0]\n",
+    "        epoch_test_recall[epoch_index] = pr_rec[1]\n",
+    "        epoch_test_Fmeasure[epoch_index] = pr_rec[2]\n",
+    "\n",
+    "        plt.figure(num=1)\n",
+    "        plt.clf()\n",
+    "        l1_1, = plt.plot(t[:epoch_index + 1], epoch_train_loss[:epoch_index + 1], label='Train loss')\n",
+    "        l1_2, = plt.plot(t[:epoch_index + 1], epoch_test_loss[:epoch_index + 1], label='Test loss')\n",
+    "        plt.legend(handles=[l1_1, l1_2])\n",
+    "        plt.grid()\n",
+    "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
+    "        plt.gcf().gca().set_xlim(left = 0)\n",
+    "        plt.title('Loss')\n",
+    "        display.clear_output(wait=True)\n",
+    "        display.display(plt.gcf())\n",
+    "\n",
+    "        plt.figure(num=2)\n",
+    "        plt.clf()\n",
+    "        l2_1, = plt.plot(t[:epoch_index + 1], epoch_train_accuracy[:epoch_index + 1], label='Train accuracy')\n",
+    "        l2_2, = plt.plot(t[:epoch_index + 1], epoch_test_accuracy[:epoch_index + 1], label='Test accuracy')\n",
+    "        plt.legend(handles=[l2_1, l2_2])\n",
+    "        plt.grid()\n",
+    "        plt.gcf().gca().set_ylim(0, 100)\n",
+    "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
+    "#         plt.gcf().gca().set_xlim(left = 0)\n",
+    "        plt.title('Accuracy')\n",
+    "        display.clear_output(wait=True)\n",
+    "        display.display(plt.gcf())\n",
+    "\n",
+    "        plt.figure(num=3)\n",
+    "        plt.clf()\n",
+    "        l3_1, = plt.plot(t[:epoch_index + 1], epoch_train_nochange_accuracy[:epoch_index + 1], label='Train accuracy: no change')\n",
+    "        l3_2, = plt.plot(t[:epoch_index + 1], epoch_train_change_accuracy[:epoch_index + 1], label='Train accuracy: change')\n",
+    "        l3_3, = plt.plot(t[:epoch_index + 1], epoch_test_nochange_accuracy[:epoch_index + 1], label='Test accuracy: no change')\n",
+    "        l3_4, = plt.plot(t[:epoch_index + 1], epoch_test_change_accuracy[:epoch_index + 1], label='Test accuracy: change')\n",
+    "        plt.legend(handles=[l3_1, l3_2, l3_3, l3_4])\n",
+    "        plt.grid()\n",
+    "        plt.gcf().gca().set_ylim(0, 100)\n",
+    "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
+    "#         plt.gcf().gca().set_xlim(left = 0)\n",
+    "        plt.title('Accuracy per class')\n",
+    "        display.clear_output(wait=True)\n",
+    "        display.display(plt.gcf())\n",
+    "\n",
+    "        plt.figure(num=4)\n",
+    "        plt.clf()\n",
+    "        l4_1, = plt.plot(t[:epoch_index + 1], epoch_train_precision[:epoch_index + 1], label='Train precision')\n",
+    "        l4_2, = plt.plot(t[:epoch_index + 1], epoch_train_recall[:epoch_index + 1], label='Train recall')\n",
+    "        l4_3, = plt.plot(t[:epoch_index + 1], epoch_train_Fmeasure[:epoch_index + 1], label='Train Dice/F1')\n",
+    "        l4_4, = plt.plot(t[:epoch_index + 1], epoch_test_precision[:epoch_index + 1], label='Test precision')\n",
+    "        l4_5, = plt.plot(t[:epoch_index + 1], epoch_test_recall[:epoch_index + 1], label='Test recall')\n",
+    "        l4_6, = plt.plot(t[:epoch_index + 1], epoch_test_Fmeasure[:epoch_index + 1], label='Test Dice/F1')\n",
+    "        plt.legend(handles=[l4_1, l4_2, l4_3, l4_4, l4_5, l4_6])\n",
+    "        plt.grid()\n",
+    "        plt.gcf().gca().set_ylim(0, 1)\n",
+    "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
+    "#         plt.gcf().gca().set_xlim(left = 0)\n",
+    "        plt.title('Precision, Recall and F-measure')\n",
+    "        display.clear_output(wait=True)\n",
+    "        display.display(plt.gcf())\n",
+    "        \n",
+    "        \n",
+    "#         mean_acc = (epoch_test_nochange_accuracy[epoch_index] + epoch_test_change_accuracy[epoch_index])/2\n",
+    "#         if mean_acc > best_mean_acc:\n",
+    "#             best_mean_acc = mean_acc\n",
+    "#             save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_acc-' + str(mean_acc) + '.pth.tar'\n",
+    "#             torch.save(net.state_dict(), save_str)\n",
+    "        \n",
+    "        \n",
+    "#         fm = pr_rec[2]\n",
+    "        fm = epoch_train_Fmeasure[epoch_index]\n",
+    "        if fm > best_fm:\n",
+    "            best_fm = fm\n",
+    "            save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_fm-' + str(fm) + '.pth.tar'\n",
+    "            torch.save(net.state_dict(), save_str)\n",
+    "        \n",
+    "        lss = epoch_train_loss[epoch_index]\n",
+    "        if lss < best_lss:\n",
+    "            best_lss = lss\n",
+    "            save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_loss-' + str(lss) + '.pth.tar'\n",
+    "            torch.save(net.state_dict(), save_str)\n",
+    "            \n",
+    "            \n",
+    "#         print('Epoch loss: ' + str(tot_loss/tot_count))\n",
+    "        if save:\n",
+    "            im_format = 'png'\n",
+    "    #         im_format = 'eps'\n",
+    "\n",
+    "            plt.figure(num=1)\n",
+    "            plt.savefig(net_name + '-01-loss.' + im_format)\n",
+    "\n",
+    "            plt.figure(num=2)\n",
+    "            plt.savefig(net_name + '-02-accuracy.' + im_format)\n",
+    "\n",
+    "            plt.figure(num=3)\n",
+    "            plt.savefig(net_name + '-03-accuracy-per-class.' + im_format)\n",
+    "\n",
+    "            plt.figure(num=4)\n",
+    "            plt.savefig(net_name + '-04-prec-rec-fmeas.' + im_format)\n",
+    "        \n",
+    "    out = {'train_loss': epoch_train_loss[-1],\n",
+    "           'train_accuracy': epoch_train_accuracy[-1],\n",
+    "           'train_nochange_accuracy': epoch_train_nochange_accuracy[-1],\n",
+    "           'train_change_accuracy': epoch_train_change_accuracy[-1],\n",
+    "           'test_loss': epoch_test_loss[-1],\n",
+    "           'test_accuracy': epoch_test_accuracy[-1],\n",
+    "           'test_nochange_accuracy': epoch_test_nochange_accuracy[-1],\n",
+    "           'test_change_accuracy': epoch_test_change_accuracy[-1]}\n",
+    "    \n",
+    "    print('pr_c, rec_c, f_meas, pr_nc, rec_nc')\n",
+    "    print(pr_rec)\n",
+    "    \n",
+    "    return out\n",
+    "\n",
+    "L = 1024\n",
+    "N = 2\n",
+    "\n",
+    "def test(dset):\n",
+    "    net.eval()\n",
+    "    tot_loss = 0\n",
+    "    tot_count = 0\n",
+    "    tot_accurate = 0\n",
+    "    \n",
+    "    n = 2\n",
+    "    class_correct = list(0. for i in range(n))\n",
+    "    class_total = list(0. for i in range(n))\n",
+    "    class_accuracy = list(0. for i in range(n))\n",
+    "    \n",
+    "    tp = 0\n",
+    "    tn = 0\n",
+    "    fp = 0\n",
+    "    fn = 0\n",
+    "    \n",
+    "    for img_index in dset.names:\n",
+    "        I1_full, I2_full, cm_full = dset.get_img(img_index)\n",
+    "        \n",
+    "        s = cm_full.shape\n",
+    "        \n",
+    "\n",
+    "        steps0 = np.arange(0,s[0],ceil(s[0]/N))\n",
+    "        steps1 = np.arange(0,s[1],ceil(s[1]/N))\n",
+    "        for ii in range(N):\n",
+    "            for jj in range(N):\n",
+    "                xmin = steps0[ii]\n",
+    "                if ii == N-1:\n",
+    "                    xmax = s[0]\n",
+    "                else:\n",
+    "                    xmax = steps0[ii+1]\n",
+    "                ymin = jj\n",
+    "                if jj == N-1:\n",
+    "                    ymax = s[1]\n",
+    "                else:\n",
+    "                    ymax = steps1[jj+1]\n",
+    "                I1 = I1_full[:, xmin:xmax, ymin:ymax]\n",
+    "                I2 = I2_full[:, xmin:xmax, ymin:ymax]\n",
+    "                cm = cm_full[xmin:xmax, ymin:ymax]\n",
+    "\n",
+    "                I1 = Variable(torch.unsqueeze(I1, 0).float()).cuda()\n",
+    "                I2 = Variable(torch.unsqueeze(I2, 0).float()).cuda()\n",
+    "                cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm),0).float()).cuda()\n",
+    "\n",
+    "\n",
+    "                output = net(I1, I2)\n",
+    "                loss = criterion(output, cm.long())\n",
+    "        #         print(loss)\n",
+    "                tot_loss += loss.data * np.prod(cm.size())\n",
+    "                tot_count += np.prod(cm.size())\n",
+    "\n",
+    "                _, predicted = torch.max(output.data, 1)\n",
+    "\n",
+    "                c = (predicted.int() == cm.data.int())\n",
+    "                for i in range(c.size(1)):\n",
+    "                    for j in range(c.size(2)):\n",
+    "                        l = int(cm.data[0, i, j])\n",
+    "                        class_correct[l] += c[0, i, j]\n",
+    "                        class_total[l] += 1\n",
+    "                        \n",
+    "                pr = (predicted.int() > 0).cpu().numpy()\n",
+    "                gt = (cm.data.int() > 0).cpu().numpy()\n",
+    "                \n",
+    "                tp += np.logical_and(pr, gt).sum()\n",
+    "                tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
+    "                fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
+    "                fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
+    "        \n",
+    "    net_loss = tot_loss/tot_count\n",
+    "    net_accuracy = 100 * (tp + tn)/tot_count\n",
+    "    \n",
+    "    for i in range(n):\n",
+    "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
+    "\n",
+    "    prec = tp / (tp + fp)\n",
+    "    rec = tp / (tp + fn)\n",
+    "    f_meas = 2 * prec * rec / (prec + rec)\n",
+    "    prec_nc = tn / (tn + fn)\n",
+    "    rec_nc = tn / (tn + fp)\n",
+    "    \n",
+    "    pr_rec = [prec, rec, f_meas, prec_nc, rec_nc]\n",
+    "        \n",
+    "    return net_loss, net_accuracy, class_accuracy, pr_rec\n",
+    "    \n",
+    "    \n",
+    "\n",
+    "\n",
+    "\n",
+    "    \n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "if LOAD_TRAINED:\n",
+    "    net.load_state_dict(torch.load('net_final.pth.tar'))\n",
+    "    print('LOAD OK')\n",
+    "else:\n",
+    "    t_start = time.time()\n",
+    "    out_dic = train()\n",
+    "    t_end = time.time()\n",
+    "    print(out_dic)\n",
+    "    print('Elapsed time:')\n",
+    "    print(t_end - t_start)\n",
+    "\n",
+    "\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "if not LOAD_TRAINED:\n",
+    "    torch.save(net.state_dict(), 'net_final.pth.tar')\n",
+    "    print('SAVE OK')"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "\n",
+    "\n",
+    "def save_test_results(dset):\n",
+    "    for name in tqdm(dset.names):\n",
+    "        with warnings.catch_warnings():\n",
+    "            I1, I2, cm = dset.get_img(name)\n",
+    "            I1 = Variable(torch.unsqueeze(I1, 0).float()).cuda()\n",
+    "            I2 = Variable(torch.unsqueeze(I2, 0).float()).cuda()\n",
+    "            out = net(I1, I2)\n",
+    "            _, predicted = torch.max(out.data, 1)\n",
+    "            I = np.stack((255*cm,255*np.squeeze(predicted.cpu().numpy()),255*cm),2)\n",
+    "            io.imsave(f'{net_name}-{name}.png',I)\n",
+    "\n",
+    "\n",
+    "\n",
+    "t_start = time.time()\n",
+    "# save_test_results(train_dataset)\n",
+    "save_test_results(test_dataset)\n",
+    "t_end = time.time()\n",
+    "print('Elapsed time: {}'.format(t_end - t_start))\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "L = 1024\n",
+    "\n",
+    "def kappa(tp, tn, fp, fn):\n",
+    "    N = tp + tn + fp + fn\n",
+    "    p0 = (tp + tn) / N\n",
+    "    pe = ((tp+fp)*(tp+fn) + (tn+fp)*(tn+fn)) / (N * N)\n",
+    "    \n",
+    "    return (p0 - pe) / (1 - pe)\n",
+    "\n",
+    "def test(dset):\n",
+    "    net.eval()\n",
+    "    tot_loss = 0\n",
+    "    tot_count = 0\n",
+    "    tot_accurate = 0\n",
+    "    \n",
+    "    n = 2\n",
+    "    class_correct = list(0. for i in range(n))\n",
+    "    class_total = list(0. for i in range(n))\n",
+    "    class_accuracy = list(0. for i in range(n))\n",
+    "    \n",
+    "    tp = 0\n",
+    "    tn = 0\n",
+    "    fp = 0\n",
+    "    fn = 0\n",
+    "    \n",
+    "    for img_index in tqdm(dset.names):\n",
+    "        I1_full, I2_full, cm_full = dset.get_img(img_index)\n",
+    "        \n",
+    "        s = cm_full.shape\n",
+    "        \n",
+    "        for ii in range(ceil(s[0]/L)):\n",
+    "            for jj in range(ceil(s[1]/L)):\n",
+    "                xmin = L*ii\n",
+    "                xmax = min(L*(ii+1),s[1])\n",
+    "                ymin = L*jj\n",
+    "                ymax = min(L*(jj+1),s[1])\n",
+    "                I1 = I1_full[:, xmin:xmax, ymin:ymax]\n",
+    "                I2 = I2_full[:, xmin:xmax, ymin:ymax]\n",
+    "                cm = cm_full[xmin:xmax, ymin:ymax]\n",
+    "\n",
+    "                I1 = Variable(torch.unsqueeze(I1, 0).float()).cuda()\n",
+    "                I2 = Variable(torch.unsqueeze(I2, 0).float()).cuda()\n",
+    "                cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm),0).float()).cuda()\n",
+    "\n",
+    "                output = net(I1, I2)\n",
+    "                    \n",
+    "                loss = criterion(output, cm.long())\n",
+    "                tot_loss += loss.data * np.prod(cm.size())\n",
+    "                tot_count += np.prod(cm.size())\n",
+    "\n",
+    "                _, predicted = torch.max(output.data, 1)\n",
+    "\n",
+    "                c = (predicted.int() == cm.data.int())\n",
+    "                for i in range(c.size(1)):\n",
+    "                    for j in range(c.size(2)):\n",
+    "                        l = int(cm.data[0, i, j])\n",
+    "                        class_correct[l] += c[0, i, j]\n",
+    "                        class_total[l] += 1\n",
+    "                        \n",
+    "                pr = (predicted.int() > 0).cpu().numpy()\n",
+    "                gt = (cm.data.int() > 0).cpu().numpy()\n",
+    "                \n",
+    "                tp += np.logical_and(pr, gt).sum()\n",
+    "                tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
+    "                fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
+    "                fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
+    "        \n",
+    "    net_loss = tot_loss/tot_count        \n",
+    "    net_loss = float(net_loss.cpu().numpy())\n",
+    "    \n",
+    "    net_accuracy = 100 * (tp + tn)/tot_count\n",
+    "    \n",
+    "    for i in range(n):\n",
+    "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
+    "        class_accuracy[i] =  float(class_accuracy[i].cpu().numpy())\n",
+    "\n",
+    "    prec = tp / (tp + fp)\n",
+    "    rec = tp / (tp + fn)\n",
+    "    dice = 2 * prec * rec / (prec + rec)\n",
+    "    prec_nc = tn / (tn + fn)\n",
+    "    rec_nc = tn / (tn + fp)\n",
+    "    \n",
+    "    pr_rec = [prec, rec, dice, prec_nc, rec_nc]\n",
+    "    \n",
+    "    k = kappa(tp, tn, fp, fn)\n",
+    "    \n",
+    "    return {'net_loss': net_loss, \n",
+    "            'net_accuracy': net_accuracy, \n",
+    "            'class_accuracy': class_accuracy, \n",
+    "            'precision': prec, \n",
+    "            'recall': rec, \n",
+    "            'dice': dice, \n",
+    "            'kappa': k}\n",
+    "\n",
+    "results = test(test_dataset)\n",
+    "pprint(results)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "Python 3",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.7.6"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 4
+}
diff --git a/fcnn4cd/siamunet_conc.py b/fcnn4cd/siamunet_conc.py
new file mode 100644
index 0000000..a3fbc6e
--- /dev/null
+++ b/fcnn4cd/siamunet_conc.py
@@ -0,0 +1,180 @@
+# Rodrigo Caye Daudt
+# https://rcdaudt.github.io/
+# Daudt, R. C., Le Saux, B., & Boulch, A. "Fully convolutional siamese networks for change detection". In 2018 25th IEEE International Conference on Image Processing (ICIP) (pp. 4063-4067). IEEE.
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.nn.modules.padding import ReplicationPad2d
+
+class SiamUnet_conc(nn.Module):
+    """SiamUnet_conc segmentation network."""
+
+    def __init__(self, input_nbr, label_nbr):
+        super(SiamUnet_conc, self).__init__()
+
+        self.input_nbr = input_nbr
+
+        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)
+        self.bn11 = nn.BatchNorm2d(16)
+        self.do11 = nn.Dropout2d(p=0.2)
+        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)
+        self.bn12 = nn.BatchNorm2d(16)
+        self.do12 = nn.Dropout2d(p=0.2)
+
+        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
+        self.bn21 = nn.BatchNorm2d(32)
+        self.do21 = nn.Dropout2d(p=0.2)
+        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)
+        self.bn22 = nn.BatchNorm2d(32)
+        self.do22 = nn.Dropout2d(p=0.2)
+
+        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
+        self.bn31 = nn.BatchNorm2d(64)
+        self.do31 = nn.Dropout2d(p=0.2)
+        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
+        self.bn32 = nn.BatchNorm2d(64)
+        self.do32 = nn.Dropout2d(p=0.2)
+        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
+        self.bn33 = nn.BatchNorm2d(64)
+        self.do33 = nn.Dropout2d(p=0.2)
+
+        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
+        self.bn41 = nn.BatchNorm2d(128)
+        self.do41 = nn.Dropout2d(p=0.2)
+        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
+        self.bn42 = nn.BatchNorm2d(128)
+        self.do42 = nn.Dropout2d(p=0.2)
+        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
+        self.bn43 = nn.BatchNorm2d(128)
+        self.do43 = nn.Dropout2d(p=0.2)
+
+        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)
+
+        self.conv43d = nn.ConvTranspose2d(384, 128, kernel_size=3, padding=1)
+        self.bn43d = nn.BatchNorm2d(128)
+        self.do43d = nn.Dropout2d(p=0.2)
+        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)
+        self.bn42d = nn.BatchNorm2d(128)
+        self.do42d = nn.Dropout2d(p=0.2)
+        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)
+        self.bn41d = nn.BatchNorm2d(64)
+        self.do41d = nn.Dropout2d(p=0.2)
+
+        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)
+
+        self.conv33d = nn.ConvTranspose2d(192, 64, kernel_size=3, padding=1)
+        self.bn33d = nn.BatchNorm2d(64)
+        self.do33d = nn.Dropout2d(p=0.2)
+        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)
+        self.bn32d = nn.BatchNorm2d(64)
+        self.do32d = nn.Dropout2d(p=0.2)
+        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)
+        self.bn31d = nn.BatchNorm2d(32)
+        self.do31d = nn.Dropout2d(p=0.2)
+
+        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)
+
+        self.conv22d = nn.ConvTranspose2d(96, 32, kernel_size=3, padding=1)
+        self.bn22d = nn.BatchNorm2d(32)
+        self.do22d = nn.Dropout2d(p=0.2)
+        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)
+        self.bn21d = nn.BatchNorm2d(16)
+        self.do21d = nn.Dropout2d(p=0.2)
+
+        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)
+
+        self.conv12d = nn.ConvTranspose2d(48, 16, kernel_size=3, padding=1)
+        self.bn12d = nn.BatchNorm2d(16)
+        self.do12d = nn.Dropout2d(p=0.2)
+        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)
+
+        # self.sm = nn.LogSoftmax(dim=1)
+        self.sm = nn.Sigmoid()
+
+    def forward(self, x):
+        x1, x2 = x
+
+        """Forward method."""
+        # Stage 1
+        x11 = self.do11(F.relu(self.bn11(self.conv11(x1))))
+        x12_1 = self.do12(F.relu(self.bn12(self.conv12(x11))))
+        x1p = F.max_pool2d(x12_1, kernel_size=2, stride=2)
+
+
+        # Stage 2
+        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))
+        x22_1 = self.do22(F.relu(self.bn22(self.conv22(x21))))
+        x2p = F.max_pool2d(x22_1, kernel_size=2, stride=2)
+
+        # Stage 3
+        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))
+        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))
+        x33_1 = self.do33(F.relu(self.bn33(self.conv33(x32))))
+        x3p = F.max_pool2d(x33_1, kernel_size=2, stride=2)
+
+        # Stage 4
+        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))
+        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))
+        x43_1 = self.do43(F.relu(self.bn43(self.conv43(x42))))
+        x4p = F.max_pool2d(x43_1, kernel_size=2, stride=2)
+
+
+        ####################################################
+        # Stage 1
+        x11 = self.do11(F.relu(self.bn11(self.conv11(x2))))
+        x12_2 = self.do12(F.relu(self.bn12(self.conv12(x11))))
+        x1p = F.max_pool2d(x12_2, kernel_size=2, stride=2)
+
+        # Stage 2
+        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))
+        x22_2 = self.do22(F.relu(self.bn22(self.conv22(x21))))
+        x2p = F.max_pool2d(x22_2, kernel_size=2, stride=2)
+
+        # Stage 3
+        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))
+        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))
+        x33_2 = self.do33(F.relu(self.bn33(self.conv33(x32))))
+        x3p = F.max_pool2d(x33_2, kernel_size=2, stride=2)
+
+        # Stage 4
+        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))
+        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))
+        x43_2 = self.do43(F.relu(self.bn43(self.conv43(x42))))
+        x4p = F.max_pool2d(x43_2, kernel_size=2, stride=2)
+
+
+        ####################################################
+        # Stage 4d
+        x4d = self.upconv4(x4p)
+        pad4 = ReplicationPad2d((0, x43_1.size(3) - x4d.size(3), 0, x43_1.size(2) - x4d.size(2)))
+        x4d = torch.cat((pad4(x4d), x43_1, x43_2), 1)
+        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))
+        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))
+        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))
+
+        # Stage 3d
+        x3d = self.upconv3(x41d)
+        pad3 = ReplicationPad2d((0, x33_1.size(3) - x3d.size(3), 0, x33_1.size(2) - x3d.size(2)))
+        x3d = torch.cat((pad3(x3d), x33_1, x33_2), 1)
+        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))
+        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))
+        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))
+
+        # Stage 2d
+        x2d = self.upconv2(x31d)
+        pad2 = ReplicationPad2d((0, x22_1.size(3) - x2d.size(3), 0, x22_1.size(2) - x2d.size(2)))
+        x2d = torch.cat((pad2(x2d), x22_1, x22_2), 1)
+        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))
+        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))
+
+        # Stage 1d
+        x1d = self.upconv1(x21d)
+        pad1 = ReplicationPad2d((0, x12_1.size(3) - x1d.size(3), 0, x12_1.size(2) - x1d.size(2)))
+        x1d = torch.cat((pad1(x1d), x12_1, x12_2), 1)
+        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))
+        x11d = self.conv11d(x12d)
+
+        return self.sm(x11d)
+
+    
diff --git a/fcnn4cd/siamunet_diff.py b/fcnn4cd/siamunet_diff.py
new file mode 100644
index 0000000..8a99162
--- /dev/null
+++ b/fcnn4cd/siamunet_diff.py
@@ -0,0 +1,179 @@
+# Rodrigo Caye Daudt
+# https://rcdaudt.github.io/
+# Daudt, R. C., Le Saux, B., & Boulch, A. "Fully convolutional siamese networks for change detection". In 2018 25th IEEE International Conference on Image Processing (ICIP) (pp. 4063-4067). IEEE.
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.nn.modules.padding import ReplicationPad2d
+
+class SiamUnet_diff(nn.Module):
+    """SiamUnet_diff segmentation network."""
+
+    def __init__(self, input_nbr, label_nbr):
+        super(SiamUnet_diff, self).__init__()
+
+        self.input_nbr = input_nbr
+
+        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)
+        self.bn11 = nn.BatchNorm2d(16)
+        self.do11 = nn.Dropout2d(p=0.2)
+        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)
+        self.bn12 = nn.BatchNorm2d(16)
+        self.do12 = nn.Dropout2d(p=0.2)
+
+        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
+        self.bn21 = nn.BatchNorm2d(32)
+        self.do21 = nn.Dropout2d(p=0.2)
+        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)
+        self.bn22 = nn.BatchNorm2d(32)
+        self.do22 = nn.Dropout2d(p=0.2)
+
+        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
+        self.bn31 = nn.BatchNorm2d(64)
+        self.do31 = nn.Dropout2d(p=0.2)
+        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
+        self.bn32 = nn.BatchNorm2d(64)
+        self.do32 = nn.Dropout2d(p=0.2)
+        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
+        self.bn33 = nn.BatchNorm2d(64)
+        self.do33 = nn.Dropout2d(p=0.2)
+
+        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
+        self.bn41 = nn.BatchNorm2d(128)
+        self.do41 = nn.Dropout2d(p=0.2)
+        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
+        self.bn42 = nn.BatchNorm2d(128)
+        self.do42 = nn.Dropout2d(p=0.2)
+        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
+        self.bn43 = nn.BatchNorm2d(128)
+        self.do43 = nn.Dropout2d(p=0.2)
+
+        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)
+
+        self.conv43d = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)
+        self.bn43d = nn.BatchNorm2d(128)
+        self.do43d = nn.Dropout2d(p=0.2)
+        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)
+        self.bn42d = nn.BatchNorm2d(128)
+        self.do42d = nn.Dropout2d(p=0.2)
+        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)
+        self.bn41d = nn.BatchNorm2d(64)
+        self.do41d = nn.Dropout2d(p=0.2)
+
+        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)
+
+        self.conv33d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)
+        self.bn33d = nn.BatchNorm2d(64)
+        self.do33d = nn.Dropout2d(p=0.2)
+        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)
+        self.bn32d = nn.BatchNorm2d(64)
+        self.do32d = nn.Dropout2d(p=0.2)
+        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)
+        self.bn31d = nn.BatchNorm2d(32)
+        self.do31d = nn.Dropout2d(p=0.2)
+
+        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)
+
+        self.conv22d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)
+        self.bn22d = nn.BatchNorm2d(32)
+        self.do22d = nn.Dropout2d(p=0.2)
+        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)
+        self.bn21d = nn.BatchNorm2d(16)
+        self.do21d = nn.Dropout2d(p=0.2)
+
+        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)
+
+        self.conv12d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)
+        self.bn12d = nn.BatchNorm2d(16)
+        self.do12d = nn.Dropout2d(p=0.2)
+        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)
+
+        # self.sm = nn.LogSoftmax(dim=1)
+        self.sm = nn.Sigmoid()
+
+    def forward(self, x):
+        x1, x2 = x
+        """Forward method."""
+        # Stage 1
+        x11 = self.do11(F.relu(self.bn11(self.conv11(x1))))
+        x12_1 = self.do12(F.relu(self.bn12(self.conv12(x11))))
+        x1p = F.max_pool2d(x12_1, kernel_size=2, stride=2)
+
+
+        # Stage 2
+        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))
+        x22_1 = self.do22(F.relu(self.bn22(self.conv22(x21))))
+        x2p = F.max_pool2d(x22_1, kernel_size=2, stride=2)
+
+        # Stage 3
+        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))
+        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))
+        x33_1 = self.do33(F.relu(self.bn33(self.conv33(x32))))
+        x3p = F.max_pool2d(x33_1, kernel_size=2, stride=2)
+
+        # Stage 4
+        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))
+        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))
+        x43_1 = self.do43(F.relu(self.bn43(self.conv43(x42))))
+        x4p = F.max_pool2d(x43_1, kernel_size=2, stride=2)
+
+        ####################################################
+        # Stage 1
+        x11 = self.do11(F.relu(self.bn11(self.conv11(x2))))
+        x12_2 = self.do12(F.relu(self.bn12(self.conv12(x11))))
+        x1p = F.max_pool2d(x12_2, kernel_size=2, stride=2)
+
+
+        # Stage 2
+        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))
+        x22_2 = self.do22(F.relu(self.bn22(self.conv22(x21))))
+        x2p = F.max_pool2d(x22_2, kernel_size=2, stride=2)
+
+        # Stage 3
+        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))
+        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))
+        x33_2 = self.do33(F.relu(self.bn33(self.conv33(x32))))
+        x3p = F.max_pool2d(x33_2, kernel_size=2, stride=2)
+
+        # Stage 4
+        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))
+        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))
+        x43_2 = self.do43(F.relu(self.bn43(self.conv43(x42))))
+        x4p = F.max_pool2d(x43_2, kernel_size=2, stride=2)
+
+
+
+        # Stage 4d
+        x4d = self.upconv4(x4p)
+        pad4 = ReplicationPad2d((0, x43_1.size(3) - x4d.size(3), 0, x43_1.size(2) - x4d.size(2)))
+        x4d = torch.cat((pad4(x4d), torch.abs(x43_1 - x43_2)), 1)
+        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))
+        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))
+        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))
+
+        # Stage 3d
+        x3d = self.upconv3(x41d)
+        pad3 = ReplicationPad2d((0, x33_1.size(3) - x3d.size(3), 0, x33_1.size(2) - x3d.size(2)))
+        x3d = torch.cat((pad3(x3d), torch.abs(x33_1 - x33_2)), 1)
+        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))
+        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))
+        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))
+
+        # Stage 2d
+        x2d = self.upconv2(x31d)
+        pad2 = ReplicationPad2d((0, x22_1.size(3) - x2d.size(3), 0, x22_1.size(2) - x2d.size(2)))
+        x2d = torch.cat((pad2(x2d), torch.abs(x22_1 - x22_2)), 1)
+        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))
+        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))
+
+        # Stage 1d
+        x1d = self.upconv1(x21d)
+        pad1 = ReplicationPad2d((0, x12_1.size(3) - x1d.size(3), 0, x12_1.size(2) - x1d.size(2)))
+        x1d = torch.cat((pad1(x1d), torch.abs(x12_1 - x12_2)), 1)
+        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))
+        x11d = self.conv11d(x12d)
+
+        return self.sm(x11d)
+
+    
diff --git a/fcnn4cd/unet.py b/fcnn4cd/unet.py
new file mode 100644
index 0000000..b3b98ce
--- /dev/null
+++ b/fcnn4cd/unet.py
@@ -0,0 +1,161 @@
+# Rodrigo Caye Daudt
+# https://rcdaudt.github.io/
+# Daudt, R. C., Le Saux, B., & Boulch, A. "Fully convolutional siamese networks for change detection". In 2018 25th IEEE International Conference on Image Processing (ICIP) (pp. 4063-4067). IEEE.
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.nn.modules.padding import ReplicationPad2d
+
+class Unet(nn.Module):
+    """EF segmentation network."""
+
+    def __init__(self, input_nbr, label_nbr):
+        super(Unet, self).__init__()
+
+        self.input_nbr = input_nbr
+
+        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)
+        self.bn11 = nn.BatchNorm2d(16)
+        self.do11 = nn.Dropout2d(p=0.2)
+        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)
+        self.bn12 = nn.BatchNorm2d(16)
+        self.do12 = nn.Dropout2d(p=0.2)
+
+        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
+        self.bn21 = nn.BatchNorm2d(32)
+        self.do21 = nn.Dropout2d(p=0.2)
+        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)
+        self.bn22 = nn.BatchNorm2d(32)
+        self.do22 = nn.Dropout2d(p=0.2)
+
+        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
+        self.bn31 = nn.BatchNorm2d(64)
+        self.do31 = nn.Dropout2d(p=0.2)
+        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
+        self.bn32 = nn.BatchNorm2d(64)
+        self.do32 = nn.Dropout2d(p=0.2)
+        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
+        self.bn33 = nn.BatchNorm2d(64)
+        self.do33 = nn.Dropout2d(p=0.2)
+
+        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
+        self.bn41 = nn.BatchNorm2d(128)
+        self.do41 = nn.Dropout2d(p=0.2)
+        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
+        self.bn42 = nn.BatchNorm2d(128)
+        self.do42 = nn.Dropout2d(p=0.2)
+        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
+        self.bn43 = nn.BatchNorm2d(128)
+        self.do43 = nn.Dropout2d(p=0.2)
+
+
+        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)
+
+        self.conv43d = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)
+        self.bn43d = nn.BatchNorm2d(128)
+        self.do43d = nn.Dropout2d(p=0.2)
+        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)
+        self.bn42d = nn.BatchNorm2d(128)
+        self.do42d = nn.Dropout2d(p=0.2)
+        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)
+        self.bn41d = nn.BatchNorm2d(64)
+        self.do41d = nn.Dropout2d(p=0.2)
+
+        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)
+
+        self.conv33d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)
+        self.bn33d = nn.BatchNorm2d(64)
+        self.do33d = nn.Dropout2d(p=0.2)
+        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)
+        self.bn32d = nn.BatchNorm2d(64)
+        self.do32d = nn.Dropout2d(p=0.2)
+        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)
+        self.bn31d = nn.BatchNorm2d(32)
+        self.do31d = nn.Dropout2d(p=0.2)
+
+        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)
+
+        self.conv22d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)
+        self.bn22d = nn.BatchNorm2d(32)
+        self.do22d = nn.Dropout2d(p=0.2)
+        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)
+        self.bn21d = nn.BatchNorm2d(16)
+        self.do21d = nn.Dropout2d(p=0.2)
+
+        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)
+
+        self.conv12d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)
+        self.bn12d = nn.BatchNorm2d(16)
+        self.do12d = nn.Dropout2d(p=0.2)
+        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)
+
+        # self.sm = nn.LogSoftmax(dim=1)
+        self.sm = nn.Sigmoid()
+
+    def forward(self, x):
+        x = torch.cat(x, 1)
+
+        """Forward method."""
+        # Stage 1
+        x11 = self.do11(F.relu(self.bn11(self.conv11(x))))
+        x12 = self.do12(F.relu(self.bn12(self.conv12(x11))))
+        x1p = F.max_pool2d(x12, kernel_size=2, stride=2)
+
+        # Stage 2
+        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))
+        x22 = self.do22(F.relu(self.bn22(self.conv22(x21))))
+        x2p = F.max_pool2d(x22, kernel_size=2, stride=2)
+
+        # Stage 3
+        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))
+        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))
+        x33 = self.do33(F.relu(self.bn33(self.conv33(x32))))
+        x3p = F.max_pool2d(x33, kernel_size=2, stride=2)
+
+        # Stage 4
+        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))
+        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))
+        x43 = self.do43(F.relu(self.bn43(self.conv43(x42))))
+        x4p = F.max_pool2d(x43, kernel_size=2, stride=2)
+
+        # Stage 4d
+        x4d = self.upconv4(x4p)
+        pad4 = ReplicationPad2d((0, x43.size(3) - x4d.size(3), 0, x43.size(2) - x4d.size(2)))
+        x4d = torch.cat((pad4(x4d), x43), 1)
+        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))
+        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))
+        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))
+
+        # Stage 3d
+        x3d = self.upconv3(x41d)
+        pad3 = ReplicationPad2d((0, x33.size(3) - x3d.size(3), 0, x33.size(2) - x3d.size(2)))
+        x3d = torch.cat((pad3(x3d), x33), 1)
+        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))
+        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))
+        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))
+
+        # Stage 2d
+        x2d = self.upconv2(x31d)
+        pad2 = ReplicationPad2d((0, x22.size(3) - x2d.size(3), 0, x22.size(2) - x2d.size(2)))
+        x2d = torch.cat((pad2(x2d), x22), 1)
+        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))
+        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))
+
+        # Stage 1d
+        x1d = self.upconv1(x21d)
+        pad1 = ReplicationPad2d((0, x12.size(3) - x1d.size(3), 0, x12.size(2) - x1d.size(2)))
+        x1d = torch.cat((pad1(x1d), x12), 1)
+        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))
+        x11d = self.conv11d(x12d)
+
+        return self.sm(x11d)
+
+    
+if __name__ == "__main__":
+
+    import numpy as np
+    from torchsummary import summary
+
+    myunet = Unet(input_nbr=3, label_nbr=2)
+    # summary(myunet, (3,256,256))
\ No newline at end of file
diff --git a/image2tiles.py b/image2tiles.py
deleted file mode 100644
index dabadf7..0000000
--- a/image2tiles.py
+++ /dev/null
@@ -1,163 +0,0 @@
-""" =================> Imagery to Tiles <============== """
-
-
-import os
-import random
-from re import split
-import numpy as np
-from pathlib import Path
-# import solaris as sol
-import tifffile as tiff
-from imageio import imread, imsave
-from prettyprinter import pprint
-
-from utils.tiff2tiles import geotiff_tiling
-
-
-SEED = 42
-train_ratio = 0.7
-
-random.seed(SEED)
-np.random.seed(SEED)
-
-
-def get_BANDS_and_BANDS_INDEX(sat, REGION, folder):
-    if "S2" == sat:
-        # AK
-        if 'ak' == REGION:
-            BANDS = ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']
-            BANDS_INDEX = [0, 1, 2, 6, 8, 9]
-
-        # CA
-        if 'ca' == REGION:
-            BANDS = ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']
-            BANDS_INDEX = [0, 1, 2, 3, 5, 6]
-
-        # US
-        if 'us' == REGION:
-            BANDS = ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']
-            BANDS_INDEX = [0, 1, 2, 3, 5, 6]
-
-    if "S1" == sat:
-        BANDS = ['ND', 'VH', 'VV']
-        BANDS_INDEX = [0, 1, 2]
-
-    if "ALOS" == sat:
-        BANDS = ['ND', 'VH', 'VV']
-        BANDS_INDEX = [0, 1, 2]
-
-    if "mask" == sat:
-        BANDS = [folder]
-        BANDS_INDEX = [0]
-
-    if "AUZ" == sat:
-        if "DSM" == folder:
-            BANDS = ["elevation", "slope", "aspect", "hillshade"]
-            BANDS_INDEX = [0, 1, 2, 3]
-        else:
-            BANDS = [folder]
-            BANDS_INDEX = [0]
-
-    return BANDS, BANDS_INDEX
-
-def tiling_wildfire_s1s2_dataset(REGION, workPath, savePath, tile_size, do_tilling=True):
-    """ find events having S1, S2 and ALOS data """
-    eventList = [eventName[:-4] for eventName in os.listdir(workPath / "mask" / "poly")]
-    pprint(eventList)
-
-    S2_preDir = workPath / "S2" / "pre"
-    S2_postDir = workPath / "S2" / "post"
-
-    ALOS_preDir = workPath / "ALOS" / "pre"
-    ALOS_postDir = workPath / "ALOS" / "post"
-
-    """ S1 Selection """
-    S1_preDir = workPath / "S1" / "pre"
-    S1_postDir = workPath / "S1" / "post"
-
-    def path2str(path): return str(os.path.split(path)[-1])
-    event_sets = []
-    for event in eventList:
-        S1_preList = map(path2str, list(S1_preDir.glob(f"{event}*.tif")))
-        S1_postList = map(path2str, list(S1_postDir.glob(f"{event}*.tif")))
-
-        S1_intersection = list(set(S1_preList).intersection(set(S1_postList)))
-        if len(S1_intersection) > 0 \
-            and os.path.isfile(S2_preDir / f"{event}.tif") \
-            and os.path.isfile(S2_postDir / f"{event}.tif") \
-                and os.path.isfile(ALOS_preDir / f"{event}.tif") \
-                and os.path.isfile(ALOS_postDir / f"{event}.tif"):
-                    rand = np.random.randint(0, len(S1_intersection), 1)[0]
-                    # print(rand, S1_intersection[rand])
-                    event_sets.append(S1_intersection[rand][:-4])
-
-    pprint(event_sets)
-    print(len(event_sets))
-
-    """ Train & Test Split """
-    split_dict = {
-        'seed': SEED,
-        'train_ratio': train_ratio,
-        'train': {
-                    'NUM': 0,
-                    'ASC': 0,
-                    'DSC': 0,
-                    'sarname': [] 
-                }, 
-        'test': {
-                    'NUM': 0,
-                    'ASC': 0,
-                    'DSC': 0,
-                    'sarname': [], 
-                }
-            }
-
-    # training and validation split
-    train_idx = list(np.random.permutation(len(event_sets)))[:int(len(event_sets)*train_ratio)]
-    print("train: ", len(train_idx))
-    print(train_idx)
-    for idx, sarname in enumerate(event_sets):
-        phase = 'train' if idx in train_idx else "test"
-        # print(idx, phase)
-
-        split_dict[phase]['sarname'].append(sarname)
-        if 'ASC' in sarname: split_dict[phase]['ASC'] += 1 
-        if 'DSC' in sarname: split_dict[phase]['DSC'] += 1
-
-        if do_tilling:
-            for sat in ["S2", "S1", "ALOS", "mask", "AUZ"]:
-                for folder in os.listdir(workPath / sat):
-                    dstFolder = savePath / phase / sat / folder
-                    dstFolder.mkdir(parents=True, exist_ok=True)
-
-                    event = sarname.split("_")[0]
-                    filename = f"{sarname}.tif" if sat == "S1" else f"{event}.tif"
-                    src_url = workPath / sat / folder / filename
-                    # print(src_url)
-
-                    BANDS, BANDS_INDEX = get_BANDS_and_BANDS_INDEX(sat, REGION, folder)
-
-                    geotiff_tiling(src_url, dstFolder, BANDS, BANDS_INDEX, tile_size)
-
-
-    for phase in ['train', 'test']:
-        split_dict[phase]['NUM'] = len(split_dict[phase]['sarname'])
-
-    # write to json
-    import json
-    with open(savePath / 'train_test_.json', 'w') as outfile:
-        json.dump(split_dict,  outfile, indent=4)
-
-
-if __name__ == "__main__":
-
-    REGION = 'us'
-    workPath = Path(f"D://wildfire-s1s2-dataset-{REGION}")
-    savePath = Path(f"{str(workPath)}-tiles")
- 
-    tiling_wildfire_s1s2_dataset(REGION, workPath, savePath, tile_size=256, do_tilling=False)
-    
-
-
-
-           
\ No newline at end of file
diff --git a/image2tiles_specify_testset.py b/image2tiles_specify_testset.py
deleted file mode 100644
index 04ea30d..0000000
--- a/image2tiles_specify_testset.py
+++ /dev/null
@@ -1,168 +0,0 @@
-""" =================> Imagery to Tiles <============== """
-
-
-import os
-import random
-from re import split
-import numpy as np
-from pathlib import Path
-# import solaris as sol
-import tifffile as tiff
-from imageio import imread, imsave
-from prettyprinter import pprint
-
-from utils.tiff2tiles import geotiff_tiling
-
-
-SEED = 42
-train_ratio = 0.7
-
-random.seed(SEED)
-np.random.seed(SEED)
-
-
-def get_BANDS_and_BANDS_INDEX(sat, REGION, folder):
-    if "S2" == sat:
-        # AK
-        if 'ak' == REGION:
-            BANDS = ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']
-            BANDS_INDEX = [0, 1, 2, 6, 8, 9]
-
-        # CA
-        if 'ca' == REGION:
-            BANDS = ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']
-            BANDS_INDEX = [0, 1, 2, 3, 5, 6]
-
-        # US
-        if 'us' == REGION:
-            BANDS = ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']
-            BANDS_INDEX = [0, 1, 2, 3, 5, 6]
-
-    if "S1" == sat:
-        BANDS = ['ND', 'VH', 'VV']
-        BANDS_INDEX = [0, 1, 2]
-
-    if "ALOS" == sat:
-        BANDS = ['ND', 'VH', 'VV']
-        BANDS_INDEX = [0, 1, 2]
-
-    if "mask" == sat:
-        BANDS = [folder]
-        BANDS_INDEX = [0]
-
-    if "AUZ" == sat:
-        if "DSM" == folder:
-            BANDS = ["elevation", "slope", "aspect", "hillshade"]
-            BANDS_INDEX = [0, 1, 2, 3]
-        else:
-            BANDS = [folder]
-            BANDS_INDEX = [0]
-
-    return BANDS, BANDS_INDEX
-
-def tiling_wildfire_s1s2_dataset(REGION, workPath, savePath, tile_size, do_tilling=True):
-    """ find events having S1, S2 and ALOS data """
-    eventList = [eventName[:-4] for eventName in os.listdir(workPath / "mask" / "poly")]
-    pprint(eventList)
-
-    S2_preDir = workPath / "S2" / "pre"
-    S2_postDir = workPath / "S2" / "post"
-
-    ALOS_preDir = workPath / "ALOS" / "pre"
-    ALOS_postDir = workPath / "ALOS" / "post"
-
-    """ S1 Selection """
-    S1_preDir = workPath / "S1" / "pre"
-    S1_postDir = workPath / "S1" / "post"
-
-    def path2str(path): return str(os.path.split(path)[-1])
-    event_sets = []
-    for event in eventList:
-        S1_preList = map(path2str, list(S1_preDir.glob(f"{event}*.tif")))
-        S1_postList = map(path2str, list(S1_postDir.glob(f"{event}*.tif")))
-
-        S1_intersection = list(set(S1_preList).intersection(set(S1_postList)))
-        if len(S1_intersection) > 0 \
-            and os.path.isfile(S2_preDir / f"{event}.tif") \
-            and os.path.isfile(S2_postDir / f"{event}.tif") \
-                and os.path.isfile(ALOS_preDir / f"{event}.tif") \
-                and os.path.isfile(ALOS_postDir / f"{event}.tif"):
-                    rand = np.random.randint(0, len(S1_intersection), 1)[0]
-                    # print(rand, S1_intersection[rand])
-                    event_sets.append(S1_intersection[rand][:-4])
-
-    pprint(event_sets)
-    print(len(event_sets))
-
-    """ Train & Test Split """
-    split_dict = {
-        'seed': SEED,
-        'train_ratio': train_ratio,
-        'train': {
-                    'NUM': 0,
-                    'ASC': 0,
-                    'DSC': 0,
-                    'sarname': [] 
-                }, 
-        'test': {
-                    'NUM': 0,
-                    'ASC': 0,
-                    'DSC': 0,
-                    'sarname': [], 
-                }
-            }
-
-    # training and validation split
-    # train_idx = list(np.random.permutation(len(event_sets)))[:int(len(event_sets)*train_ratio)]
-    # print("train: ", len(train_idx))
-    # print(train_idx)
-
-    test_events = os.listdir("D:/wildfire-s1s2-dataset-ak-check/test_events")
-    test_events = [event[:-4] for event in test_events]
-
-    for idx, sarname in enumerate(event_sets):
-        event = sarname.split("_")[0]
-        phase = 'test' if event in test_events else "train"
-        # print(idx, phase)
-
-        split_dict[phase]['sarname'].append(sarname)
-        if 'ASC' in sarname: split_dict[phase]['ASC'] += 1 
-        if 'DSC' in sarname: split_dict[phase]['DSC'] += 1
-
-        if do_tilling and ('test' == phase):
-            for sat in ["S2", "S1", "ALOS", "mask", "AUZ"]:
-                for folder in os.listdir(workPath / sat):
-                    dstFolder = savePath / phase / sat / folder
-                    dstFolder.mkdir(parents=True, exist_ok=True)
-
-                    event = sarname.split("_")[0]
-                    filename = f"{sarname}.tif" if sat == "S1" else f"{event}.tif"
-                    src_url = workPath / sat / folder / filename
-                    
-                    BANDS, BANDS_INDEX = get_BANDS_and_BANDS_INDEX(sat, REGION, folder)
-                    
-                    geotiff_tiling(phase, src_url, dstFolder, BANDS, BANDS_INDEX, tile_size, False)
-
-
-    for phase in ['train', 'test']:
-        split_dict[phase]['NUM'] = len(split_dict[phase]['sarname'])
-
-    # write to json
-    import json
-    with open(savePath / 'train_test.json', 'w') as outfile:
-        json.dump(split_dict,  outfile, indent=4)
-
-
-if __name__ == "__main__":
-
-    REGION = 'ak'
-    workPath = Path(f"D://wildfire-s1s2-dataset-{REGION}")
-    savePath = Path(f"{str(workPath)}-tiles")
-    savePath.mkdir(exist_ok=True)
- 
-    tiling_wildfire_s1s2_dataset(REGION, workPath, savePath, tile_size=256, do_tilling=True)
-    
-
-
-
-           
\ No newline at end of file
diff --git a/main_s1s2_distill_unet.py b/main_s1s2_distill_unet.py
new file mode 100644
index 0000000..4d9cf5e
--- /dev/null
+++ b/main_s1s2_distill_unet.py
@@ -0,0 +1,442 @@
+# checkpoint: https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L101
+
+import os, json
+import random
+from cv2 import mean, normalize
+from easydict import EasyDict as edict
+from pathlib import Path
+from prettyprinter import pprint
+from imageio import imread, imsave
+
+import hydra
+import wandb
+from omegaconf import DictConfig, OmegaConf
+
+
+###################################################################################
+import os, sys
+import numpy as np
+from pathlib import Path
+import hydra
+from omegaconf import DictConfig, OmegaConf
+
+import copy
+import time
+import torch
+
+import torch.optim as optim
+import torch.nn as nn
+import torch.nn.functional as F
+from easydict import EasyDict as edict
+
+from tqdm import tqdm as tqdm
+
+import logging
+logger = logging.getLogger(__name__)
+
+import smp
+from smp.base.modules import Activation
+from models.model_selection import get_model
+import wandb
+
+# f_score = smp.utils.functional.f_score
+# Dice/F1 score - https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient
+# IoU/Jaccard score - https://en.wikipedia.org/wiki/Jaccard_index
+# diceLoss = smp.utils.losses.DiceLoss(eps=1)
+mse_loss = nn.MSELoss(reduction="mean")
+
+
+from models.loss_ref import soft_dice_loss, soft_dice_loss_balanced, jaccard_like_loss, jaccard_like_balanced_loss
+
+AverageValueMeter =  smp.utils.train.AverageValueMeter
+
+# Augmentations
+from dataset.augument import get_training_augmentation, \
+    get_validation_augmentation, get_preprocessing
+
+from torch.utils.data import DataLoader
+from dataset.wildfire import S1S2 as Dataset # ------------------------------------------------------- Dataset
+
+from models.lr_schedule import get_cosine_schedule_with_warmup, PolynomialLRDecay
+
+
+def format_logs(logs):
+    str_logs = ['{}: {:.4}'.format(k, v) for k, v in logs.items()]
+    s = ', '.join(str_logs)
+    return s
+
+
+def loss_fun(CFG, DEVICE='cuda'):
+    if CFG.MODEL.LOSS_TYPE == 'BCELoss':
+        criterion = nn.BCELoss()
+
+    elif CFG.MODEL.LOSS_TYPE == 'BCEWithLogitsLoss':
+        criterion = nn.BCEWithLogitsLoss() # includes sigmoid activation
+
+    elif CFG.MODEL.LOSS_TYPE == 'DiceLoss':
+        criterion = smp.utils.losses.DiceLoss(eps=1, activation=CFG.MODEL.ACTIVATION)
+
+    elif CFG.MODEL.LOSS_TYPE == 'CrossEntropyLoss':
+        balance_weight = [CFG.MODEL.NEGATIVE_WEIGHT, CFG.MODEL.POSITIVE_WEIGHT]
+        balance_weight = torch.tensor(balance_weight).float().to(DEVICE)
+        criterion = nn.CrossEntropyLoss(weight = balance_weight)
+        
+    elif CFG.MODEL.LOSS_TYPE == 'SoftDiceLoss':
+        criterion = soft_dice_loss 
+    elif CFG.MODEL.LOSS_TYPE == 'SoftDiceBalancedLoss':
+        criterion = soft_dice_loss_balanced
+    elif CFG.MODEL.LOSS_TYPE == 'JaccardLikeLoss':
+        criterion = jaccard_like_loss
+    elif CFG.MODEL.LOSS_TYPE == 'ComboLoss':
+        criterion = lambda pred, gts: F.binary_cross_entropy_with_logits(pred, gts) + soft_dice_loss(pred, gts)
+    elif CFG.MODEL.LOSS_TYPE == 'WeightedComboLoss':
+        criterion = lambda pred, gts: F.binary_cross_entropy_with_logits(pred, gts) + 10 * soft_dice_loss(pred, gts)
+    elif CFG.MODEL.LOSS_TYPE == 'FrankensteinLoss':
+        criterion = lambda pred, gts: F.binary_cross_entropy_with_logits(pred, gts) + jaccard_like_balanced_loss(pred, gts)
+
+    return criterion
+
+class SegModel(object):
+    def __init__(self, cfg) -> None:
+        super().__init__()
+        self.PROJECT_DIR = Path(hydra.utils.get_original_cwd())
+
+        self.cfg = cfg
+        self.DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
+        # self.DEVICE = 'cpu'
+
+        self.model = get_model(cfg)
+        self.activation = Activation(self.cfg.MODEL.ACTIVATION)
+
+        # self.MODEL_URL = str(self.PROJECT_DIR / "outputs" / "best_model.pth")
+        self.RUN_DIR = self.PROJECT_DIR / self.cfg.EXP.OUTPUT
+        self.MODEL_URL = str(self.RUN_DIR / "model.pth")
+
+
+        ''' load S2 pretrained model '''
+        if self.cfg.MODEL.DISTILL:
+            self.model_pretrained = torch.load(self.cfg.MODEL.PRETRAINED, map_location=torch.device('cpu'))
+            self.model_pretrained.to(self.DEVICE)
+            self.model_pretrained.eval()
+
+        # if self.cfg.MODEL.ENCODER is not None:
+        #     self.preprocessing_fn = \
+        #         smp.encoders.get_preprocessing_fn(self.cfg.MODEL.ENCODER, self.cfg.MODEL.ENCODER_WEIGHTS)
+
+        self.metrics = [smp.utils.metrics.IoU(threshold=0.5, activation=None),
+                        smp.utils.metrics.Fscore(activation=None)
+                    ]
+
+        '''--------------> need to improve <-----------------'''
+        # specify data folder
+        self.TRAIN_DIR = Path(self.cfg.DATA.DIR) / 'train'
+        self.VALID_DIR = Path(self.cfg.DATA.DIR) / 'test'
+        '''--------------------------------------------------'''
+    
+    def get_dataloaders(self) -> dict:
+
+        if self.cfg.MODEL.NUM_CLASSES == 1:
+            classes = ['burned']
+        elif self.cfg.MODEL.NUM_CLASSES == 2:
+            classes = ['unburn', 'burned']
+        elif self.cfg.MODEL.NUM_CLASSES > 2:
+            print(" ONLY ALLOW ONE or TWO CLASSES SO FAR !!!")
+            pass
+
+        """ Data Preparation """
+        train_dataset = Dataset(
+            self.TRAIN_DIR, 
+            self.cfg, 
+            # augmentation=get_training_augmentation(), 
+            # preprocessing=get_preprocessing(self.preprocessing_fn),
+            classes=classes,
+        )
+
+        valid_dataset = Dataset(
+            self.VALID_DIR, 
+            self.cfg, 
+            # augmentation=get_validation_augmentation(), 
+            # preprocessing=get_preprocessing(self.preprocessing_fn),
+            classes=classes,
+        )
+
+        generator=torch.Generator().manual_seed(self.cfg.RAND.SEED)
+        train_size = int(len(train_dataset) * self.cfg.DATA.TRAIN_RATIO)
+        valid_size = len(train_dataset) - train_size
+        train_set, val_set = torch.utils.data.random_split(train_dataset, [train_size, valid_size], generator=generator)
+
+        train_loader = DataLoader(train_set, batch_size=self.cfg.MODEL.BATCH_SIZE, shuffle=True, num_workers=4, generator=generator)
+        valid_loader = DataLoader(val_set, batch_size=self.cfg.MODEL.BATCH_SIZE, shuffle=True, num_workers=4, generator=generator)
+        test_loader = DataLoader(valid_dataset, batch_size=self.cfg.MODEL.BATCH_SIZE, shuffle=True, num_workers=4, generator=generator)
+        
+        dataloaders = { 
+                        'train': train_loader, \
+                        'valid': valid_loader, \
+                        'test': test_loader, \
+
+                        'train_size': train_size, \
+                        'valid_size': valid_size, \
+                        'test_size': len(valid_dataset)
+                    }
+
+        return dataloaders
+
+
+    def run(self) -> None:
+        self.model.to(self.DEVICE)
+        self.criterion = loss_fun(self.cfg)
+
+        self.dataloaders = self.get_dataloaders()
+        self.optimizer = torch.optim.Adam([dict(
+                params=self.model.parameters(), 
+                lr=self.cfg.MODEL.LEARNING_RATE, 
+                weight_decay=self.cfg.MODEL.WEIGHT_DECAY)])
+
+        """ ===================== >> learning rate scheduler << ========================= """
+        per_epoch_steps = self.dataloaders['train_size'] // self.cfg.MODEL.BATCH_SIZE
+        total_training_steps = self.cfg.MODEL.MAX_EPOCH * per_epoch_steps
+
+        self.USE_LR_SCHEDULER = True \
+            if self.cfg.MODEL.LR_SCHEDULER in ['cosine_warmup', 'polynomial'] \
+            else True
+
+        if self.cfg.MODEL.LR_SCHEDULER == 'cosine':
+            ''' cosine scheduler '''
+            warmup_steps = self.cfg.MODEL.COSINE_SCHEDULER.WARMUP * per_epoch_steps
+            self.lr_scheduler = get_cosine_schedule_with_warmup(self.optimizer, warmup_steps, total_training_steps)
+
+        elif self.cfg.MODEL.LR_SCHEDULER == 'poly':
+            ''' polynomial '''
+            self.lr_scheduler = PolynomialLRDecay(self.optimizer, 
+                            max_decay_steps=total_training_steps, 
+                            end_learning_rate=self.cfg.MODEL.POLY_SCHEDULER.END_LR, #1e-5, 
+                            power=self.cfg.MODEL.POLY_SCHEDULER.POWER, #0.9
+                        )
+        else:
+            pass
+
+
+        # self.history_logs = edict()
+        # self.history_logs['train'] = []
+        # self.history_logs['valid'] = []
+        # self.history_logs['test'] = []
+
+        # --------------------------------- Train -------------------------------------------
+        max_score = self.cfg.MODEL.MAX_SCORE
+        self.iters = 0
+        for epoch in range(0, self.cfg.MODEL.MAX_EPOCH):
+            epoch = epoch + 1
+            print(f"\n==> train epoch: {epoch}/{self.cfg.MODEL.MAX_EPOCH}")
+            self.train_one_epoch(epoch)
+            valid_logs = self.valid_logs
+            
+            # do something (save model, change lr, etc.)
+            if valid_logs['iou_score'] > max_score:
+                max_score = valid_logs['iou_score']
+
+                if (1 == epoch) or (0 == (epoch % self.cfg.MODEL.SAVE_INTERVAL)):
+                    torch.save(self.model, self.MODEL_URL)
+                    # torch.save(self.model.state_dict(), self.MODEL_URL)
+                    print('Model saved!')
+
+            # if epoch % 50 == 0:
+            #     self.optimizer.param_groups[0]['lr'] = 0.1 * self.optimizer.param_groups[0]['lr']
+                        
+        
+    def train_one_epoch(self, epoch):
+    
+        # wandb.
+        for phase in ['train', 'valid', 'test']:
+            if phase == 'train':
+                self.model.train()
+            else:
+                self.model.eval()
+
+            logs = self.step(phase) 
+            # print(phase, logs)
+
+            currlr = self.optimizer.param_groups[0]['lr'] 
+            wandb.log({phase: logs, 'epoch': epoch, 'lr': currlr})
+
+            # temp = [logs["total_loss"]] + [logs[self.metrics[i].__name__] for i in range(0, len(self.metrics))]
+            # self.history_logs[phase].append(temp)
+
+            if phase == 'valid': self.valid_logs = logs
+
+
+    def step(self, phase) -> dict:
+        logs = {}
+        loss_meter = AverageValueMeter()
+        metrics_meters = {metric.__name__: AverageValueMeter() for metric in self.metrics}
+        metrics_meters.update({
+            'loss_base': AverageValueMeter(),
+            'loss_do': AverageValueMeter(),
+            'loss_df': AverageValueMeter()
+        })
+
+        # if ('Train' in phase) and (self.cfg.useDataWoCAug):
+        #     dataLoader_woCAug = iter(self.dataloaders['Train_woCAug'])
+
+        with tqdm(iter(self.dataloaders[phase]), desc=phase, file=sys.stdout, disable=not self.cfg.MODEL.VERBOSE) as iterator:
+            for i, (x, y) in enumerate(iterator):
+                self.optimizer.zero_grad()
+
+                ''' move data to GPU '''
+                input = []
+                for x_ in x: input.append(x_.to(self.DEVICE))
+                y = y.to(self.DEVICE)
+                # print(len(input))
+
+                ''' do prediction '''
+                if 'UNet_resnet' in self.cfg.MODEL.ARCH: 
+                    input = input[0]
+                    out = self.model.forward(input)
+                elif 'distill_unet' == self.cfg.MODEL.ARCH: 
+                    xc, out = self.model.forward(input[:1])
+                else:
+                    out = self.model.forward(input)[-1]
+                y_pred = self.activation(out) # If use this, set IoU/F1 metrics activation=None
+
+                ''' compute loss '''
+                loss_ = self.criterion(out, y)
+
+                ''' normalize output or features '''
+                if self.cfg.MODEL.L2_NORM:
+                    l2_norm = lambda x: F.normalize(x, dim=1, p=2)  
+                else: 
+                    l2_norm = Activation(self.cfg.MODEL.ACTIVATION)
+
+                if self.cfg.MODEL.DISTILL:
+                    xc2, out2 = self.model_pretrained.forward(input[-1:])
+                    loss_distill_outout = mse_loss(l2_norm(out), l2_norm(out2))
+                    loss_distill_feat = mse_loss(l2_norm(xc), l2_norm(xc2))
+
+                    metrics_meters['loss_base'].add(loss_.cpu().detach().numpy())
+
+                    loss_ = self.cfg.MODEL.LOSS_COEF[0] * loss_ \
+                        + self.cfg.MODEL.LOSS_COEF[1] * loss_distill_outout \
+                        + self.cfg.MODEL.LOSS_COEF[2] * loss_distill_feat
+
+                    metrics_meters['loss_do'].add(loss_distill_outout.cpu().detach().numpy())
+                    metrics_meters['loss_df'].add(loss_distill_feat.cpu().detach().numpy())
+
+                ''' Back Propergation (BP) '''
+                if phase == 'train':
+                    loss_.backward()
+                    self.optimizer.step()
+                    self.iters = self.iters + 1
+
+                    ''' Iteration-Wise log for train stage only '''
+                    if self.cfg.MODEL.STEP_WISE_LOG:
+                        self.iters = self.iters + 1
+                        currlr = self.optimizer.param_groups[0]['lr'] 
+                        # wandb.log({'x0.mean': x[0].mean()})
+                        wandb.log({phase: logs, 'iters': self.iters, 'lr': currlr})
+
+                    if self.USE_LR_SCHEDULER:
+                        self.lr_scheduler.step()
+
+                # if mask is in one-hot: NCWH, C=NUM_CLASSES (C>1), and do nothing if C=1
+                if y.shape[1] >= 2: 
+                    y = self.activation(y)
+
+                ''' update loss and metrics logs '''
+                # update loss logs
+                loss_value = loss_.cpu().detach().numpy()
+                loss_meter.add(loss_value)
+                # loss_logs = {criterion.__name__: loss_meter.mean}
+                loss_logs = {'total_loss': loss_meter.mean}
+                logs.update(loss_logs)
+
+                # update metrics logs
+                for metric_fn in self.metrics:
+                    metric_value = metric_fn(y_pred, y).cpu().detach().numpy()
+                    metrics_meters[metric_fn.__name__].add(metric_value)
+
+                metrics_logs = {k: v.mean for k, v in metrics_meters.items()}
+                logs.update(metrics_logs)
+                # print(self.iters, x[0].mean().item(), loss_.item())
+
+                if self.cfg.MODEL.VERBOSE:
+                    s = format_logs(logs)
+                    iterator.set_postfix_str(s)
+
+        return logs
+##############################################################
+
+
+def set_random_seed(seed, deterministic=False):
+    """Set random seed.
+
+    Args:
+        seed (int): Seed to be used.
+        deterministic (bool): Whether to set the deterministic option for
+            CUDNN backend, i.e., set `torch.backends.cudnn.deterministic`
+            to True and `torch.backends.cudnn.benchmark` to False.
+            Default: False.
+    """
+    random.seed(seed)
+    np.random.seed(seed)
+    torch.manual_seed(seed)
+    torch.cuda.manual_seed_all(seed)
+    if deterministic:
+        torch.backends.cudnn.deterministic = True
+        torch.backends.cudnn.benchmark = False
+
+
+@hydra.main(config_path="./config", config_name="distill_unet")
+def run_app(cfg : DictConfig) -> None:
+
+    ''' set randome seed '''
+    os.environ['HYDRA_FULL_ERROR'] = str(1)
+    os.environ['PYTHONHASHSEED'] = str(cfg.RAND.SEED) #cfg.RAND.SEED
+    if cfg.RAND.DETERMIN:
+        os.environ['CUBLAS_WORKSPACE_CONFIG']=":4096:8" #https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility
+        torch.use_deterministic_algorithms(True)
+    set_random_seed(cfg.RAND.SEED, deterministic=cfg.RAND.DETERMIN)
+
+    # wandb.init(config=cfg, project=cfg.project.name, name=cfg.EXP.name)
+    import pandas as pd
+    from prettyprinter import pprint
+    
+    cfg_dict = OmegaConf.to_container(cfg, resolve=True)
+    cfg_flat = pd.json_normalize(cfg_dict, sep='.').to_dict(orient='records')[0]
+
+    # cfg.MODEL.DEBUG = False
+    # if not cfg.MODEL.DEBUG:
+    wandb.init(config=cfg_flat, project=cfg.PROJECT.NAME, entity=cfg.PROJECT.ENTITY, name=cfg.EXP.NAME)
+    pprint(cfg_flat)
+
+    ''' train '''
+    # from experiments.seg_model import SegModel
+    mySegModel = SegModel(cfg)
+    mySegModel.run()
+
+    ''' inference '''
+    from s1s2_evaluator import evaluate_model
+    evaluate_model(cfg, mySegModel.MODEL_URL, mySegModel.RUN_DIR / "errMap")
+
+    ''' compute IoU and F1 for all events '''
+    from utils.iou4all import compute_IoU_F1
+    compute_IoU_F1(phase="test_images", 
+                    result_dir=mySegModel.RUN_DIR / "errMap", 
+                    dataset_dir=cfg.DATA.DIR)
+    
+    # if not cfg.MODEL.DEBUG:
+    wandb.finish()
+
+
+if __name__ == "__main__":
+
+    run_app()
+
+
+
+
+
+
+
+
+
+
+                
\ No newline at end of file
diff --git a/main_s1s2_segformer.py b/main_s1s2_segformer.py
new file mode 100644
index 0000000..8fc4427
--- /dev/null
+++ b/main_s1s2_segformer.py
@@ -0,0 +1,350 @@
+
+import os, json
+import random
+from easydict import EasyDict as edict
+from pathlib import Path
+from prettyprinter import pprint
+from imageio import imread, imsave
+
+import hydra
+import wandb
+from omegaconf import DictConfig, OmegaConf
+
+
+###################################################################################
+import os, sys
+import numpy as np
+from pathlib import Path
+import hydra
+from omegaconf import DictConfig, OmegaConf
+
+import copy
+import time
+import torch
+import torch.optim as optim
+import torch.nn as nn
+import torch.nn.functional as F
+from easydict import EasyDict as edict
+
+from tqdm import tqdm as tqdm
+
+import logging
+logger = logging.getLogger(__name__)
+
+import smp
+from smp.base.modules import Activation
+from models.model_selection import get_model
+import wandb
+
+f_score = smp.utils.functional.f_score
+# Dice/F1 score - https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient
+# IoU/Jaccard score - https://en.wikipedia.org/wiki/Jaccard_index
+# diceLoss = smp.utils.losses.DiceLoss(eps=1)
+from models.loss_ref import soft_dice_loss, soft_dice_loss_balanced, jaccard_like_loss, jaccard_like_balanced_loss
+
+AverageValueMeter =  smp.utils.train.AverageValueMeter
+
+# Augmentations
+from dataset.augument import get_training_augmentation, \
+    get_validation_augmentation, get_preprocessing
+
+from torch.utils.data import DataLoader
+from dataset.wildfire import S1S2 as Dataset # ------------------------------------------------------- Dataset
+
+from models.lr_schedule import get_cosine_schedule_with_warmup
+
+
+def format_logs(logs):
+    str_logs = ['{}: {:.4}'.format(k, v) for k, v in logs.items()]
+    s = ', '.join(str_logs)
+    return s
+
+# Loss Functions
+def loss_fun(cfg, DEVICE='cuda'):
+    if cfg.model.LOSS_TYPE == 'BCEWithLogitsLoss':
+        criterion = nn.BCEWithLogitsLoss()
+
+    elif cfg.model.LOSS_TYPE == 'smpDiceLoss':
+        criterion = smp.utils.losses.DiceLoss(eps=1)
+
+    elif cfg.model.LOSS_TYPE == 'CrossEntropyLoss':
+        balance_weight = [cfg.MODEL.NEGATIVE_WEIGHT, cfg.MODEL.POSITIVE_WEIGHT]
+        balance_weight = torch.tensor(balance_weight).float().to(DEVICE)
+        criterion = nn.CrossEntropyLoss(weight = balance_weight)
+
+    elif cfg.model.LOSS_TYPE == 'SoftDiceLoss':
+        criterion = soft_dice_loss 
+    elif cfg.model.LOSS_TYPE == 'SoftDiceBalancedLoss':
+        criterion = soft_dice_loss_balanced
+    elif cfg.model.LOSS_TYPE == 'JaccardLikeLoss':
+        criterion = jaccard_like_loss
+    elif cfg.model.LOSS_TYPE == 'ComboLoss':
+        criterion = lambda pred, gts: F.binary_cross_entropy_with_logits(pred, gts) + soft_dice_loss(pred, gts)
+    elif cfg.model.LOSS_TYPE == 'WeightedComboLoss':
+        criterion = lambda pred, gts: F.binary_cross_entropy_with_logits(pred, gts) + 10 * soft_dice_loss(pred, gts)
+    elif cfg.model.LOSS_TYPE == 'FrankensteinLoss':
+        criterion = lambda pred, gts: F.binary_cross_entropy_with_logits(pred, gts) + jaccard_like_balanced_loss(pred, gts)
+
+    return criterion
+
+class SegModel(object):
+    def __init__(self, cfg) -> None:
+        super().__init__()
+        self.project_dir = Path(hydra.utils.get_original_cwd())
+
+        self.cfg = cfg
+        self.DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
+
+        self.model = get_model(cfg)
+        self.activation = Activation(cfg.model.ACTIVATION)
+
+        # self.model_url = str(self.project_dir / "outputs" / "best_model.pth")
+        self.rundir = self.project_dir / self.cfg.experiment.output
+        self.model_url = str( self.rundir / "model.pth")
+
+        # if cfg.model.ENCODER is not None:
+        #     self.preprocessing_fn = \
+        #         smp.encoders.get_preprocessing_fn(cfg.model.ENCODER, cfg.model.ENCODER_WEIGHTS)
+
+        self.metrics = [smp.utils.metrics.IoU(threshold=0.5),
+                        smp.utils.metrics.Fscore()
+                    ]
+
+        '''--------------> need to improve <-----------------'''
+        # specify data folder
+        self.train_dir = Path(self.cfg.data.dir) / 'train'
+        self.valid_dir = Path(self.cfg.data.dir) / 'test'
+        '''--------------------------------------------------'''
+
+    
+    def get_dataloaders(self) -> dict:
+
+        if self.cfg.model.NUM_CLASSES == 1:
+            classes = ['burned']
+        elif self.cfg.model.NUM_CLASSES == 2:
+            classes = ['unburn', 'burned']
+        elif self.cfg.model.NUM_CLASSES > 2:
+            print(" ONLY ALLOW ONE or TWO CLASSES SO FAR !!!")
+            pass
+
+        """ Data Preparation """
+        train_dataset = Dataset(
+            self.train_dir, 
+            self.cfg, 
+            # augmentation=get_training_augmentation(), 
+            # preprocessing=get_preprocessing(self.preprocessing_fn),
+            classes=classes,
+        )
+
+        valid_dataset = Dataset(
+            self.valid_dir, 
+            self.cfg, 
+            # augmentation=get_validation_augmentation(), 
+            # preprocessing=get_preprocessing(self.preprocessing_fn),
+            classes=classes,
+        )
+
+        train_size = int(len(train_dataset) * self.cfg.data.train_ratio)
+        valid_size = len(train_dataset) - train_size
+        train_set, val_set = torch.utils.data.random_split(train_dataset, [train_size, valid_size])
+
+        train_loader = DataLoader(train_set, batch_size=self.cfg.model.batch_size, shuffle=True, num_workers=4)
+        valid_loader = DataLoader(val_set, batch_size=self.cfg.model.batch_size, shuffle=True, num_workers=4)
+        test_loader = DataLoader(valid_dataset, batch_size=self.cfg.model.batch_size, shuffle=False, num_workers=4)
+
+        dataloaders = { 
+                        'train': train_loader, \
+                        'valid': valid_loader, \
+                        'test': test_loader, \
+
+                        'train_size': train_size, \
+                        'valid_size': valid_size, \
+                        'test_size': len(valid_dataset)
+                    }
+
+        return dataloaders
+
+
+    def run(self) -> None:
+        self.dataloaders = self.get_dataloaders()
+        self.optimizer = torch.optim.Adam([dict(
+                params=self.model.parameters(), 
+                lr=self.cfg.model.learning_rate, 
+                weight_decay=self.cfg.model.weight_decay)])
+
+        # lr scheduler
+        per_epoch_steps = self.dataloaders['train_size'] // self.cfg.model.batch_size
+        total_training_steps = self.cfg.model.max_epoch * per_epoch_steps
+        warmup_steps = self.cfg.model.warmup_coef * per_epoch_steps
+        if self.cfg.model.use_lr_scheduler:
+            self.lr_scheduler = get_cosine_schedule_with_warmup(self.optimizer, warmup_steps, total_training_steps)
+
+        self.history_logs = edict()
+        self.history_logs['train'] = []
+        self.history_logs['valid'] = []
+        self.history_logs['test'] = []
+
+        # --------------------------------- Train -------------------------------------------
+        max_score = self.cfg.model.max_score
+        for epoch in range(0, self.cfg.model.max_epoch):
+            epoch = epoch + 1
+            print(f"\n==> train epoch: {epoch}/{self.cfg.model.max_epoch}")
+            self.train_one_epoch(epoch)
+            valid_logs = self.valid_logs
+            
+            # do something (save model, change lr, etc.)
+            if valid_logs['iou_score'] > max_score:
+                max_score = valid_logs['iou_score']
+
+                if (1 == epoch) or (0 == (epoch % self.cfg.model.save_interval)):
+                    torch.save(self.model, self.model_url)
+                    # torch.save(self.model.state_dict(), self.model_url)
+                    print('Model saved!')
+
+            # if epoch % 50 == 0:
+            #     self.optimizer.param_groups[0]['lr'] = 0.1 * self.optimizer.param_groups[0]['lr']
+                        
+        
+    def train_one_epoch(self, epoch):
+        self.model.to(self.DEVICE)
+        
+        # wandb.
+        for phase in ['train', 'valid', 'test']:
+            if phase == 'train':
+                self.model.train()
+            else:
+                self.model.eval()
+
+            logs = self.step(phase) 
+            # print(phase, logs)
+
+            currlr = self.lr_scheduler.get_last_lr()[0] if self.cfg.model.use_lr_scheduler else self.optimizer.param_groups[0]['lr']          
+            wandb.log({phase: logs, 'epoch': epoch, 'lr': currlr})
+
+            temp = [logs["total_loss"]] + [logs[self.metrics[i].__name__] for i in range(0, len(self.metrics))]
+            self.history_logs[phase].append(temp)
+
+            if phase == 'valid': self.valid_logs = logs
+
+
+    def step(self, phase) -> dict:
+        logs = {}
+        loss_meter = AverageValueMeter()
+        metrics_meters = {metric.__name__: AverageValueMeter() for metric in self.metrics}
+
+        # if ('Train' in phase) and (self.cfg.useDataWoCAug):
+        #     dataLoader_woCAug = iter(self.dataloaders['Train_woCAug'])
+
+        with tqdm(iter(self.dataloaders[phase]), desc=phase, file=sys.stdout, disable=not self.cfg.model.verbose) as iterator:
+            for (x, y) in iterator:
+                self.optimizer.zero_grad()
+
+                ''' move data to GPU '''
+                input = []
+                for x_i in x: input.append(x_i.to(self.DEVICE))
+                y = y.to(self.DEVICE)
+                # print(len(input))
+
+                ''' do prediction '''
+                out = self.model.forward(input)[-1]
+                # y_pred = self.activation(out)
+
+                ''' compute loss '''
+                # loss_ = diceLoss(y_pred, y)
+
+                criterion = loss_fun(self.cfg)
+                loss_ = criterion(out, y)
+
+                if phase == 'train':
+                    loss_.backward()
+                    self.optimizer.step()
+
+                    if self.cfg.model.use_lr_scheduler:
+                        self.lr_scheduler.step()
+
+                # update loss logs
+                loss_value = loss_.cpu().detach().numpy()
+                loss_meter.add(loss_value)
+                # loss_logs = {criterion.__name__: loss_meter.mean}
+                loss_logs = {'total_loss': loss_meter.mean}
+                logs.update(loss_logs)
+
+                # added for NUM_CLASSES >= 2
+                if self.cfg.model.NUM_CLASSES >= 2:
+                    y_pred = self.activation(out)
+                    y = self.activation(y)
+
+                # update metrics logs
+                for metric_fn in self.metrics:
+                    metric_value = metric_fn(y_pred, y).cpu().detach().numpy()
+                    metrics_meters[metric_fn.__name__].add(metric_value)
+
+                metrics_logs = {k: v.mean for k, v in metrics_meters.items()}
+                logs.update(metrics_logs)
+                # print(logs)
+
+                if self.cfg.model.verbose:
+                    s = format_logs(logs)
+                    iterator.set_postfix_str(s)
+
+                # if phase == 'train':
+                #     loss_.backward()
+                #     self.optimizer.step()
+
+                #     if self.cfg.model.use_lr_scheduler:
+                #         self.lr_scheduler.step()
+
+            return logs
+##############################################################
+
+
+def set_random_seed(seed, deterministic=False):
+    """Set random seed.
+
+    Args:
+        seed (int): Seed to be used.
+        deterministic (bool): Whether to set the deterministic option for
+            CUDNN backend, i.e., set `torch.backends.cudnn.deterministic`
+            to True and `torch.backends.cudnn.benchmark` to False.
+            Default: False.
+    """
+    random.seed(seed)
+    np.random.seed(seed)
+    torch.manual_seed(seed)
+    torch.cuda.manual_seed_all(seed)
+    if deterministic:
+        torch.backends.cudnn.deterministic = True
+        torch.backends.cudnn.benchmark = False
+
+
+@hydra.main(config_path="./config", config_name="segformer")
+def run_app(cfg : DictConfig) -> None:
+    # wandb.init(config=cfg, project=cfg.project.name, name=cfg.experiment.name)
+    import pandas as pd
+    from prettyprinter import pprint
+    from easydict import EasyDict as edict
+    
+    cfg_dict = OmegaConf.to_container(cfg, resolve=True)
+    cfg_flat = pd.json_normalize(cfg_dict, sep='.').to_dict(orient='records')[0]
+
+    if not cfg.model.DEBUG:
+        wandb.init(config=cfg_flat, project=cfg.project.name, entity=cfg.project.entity, name=cfg.experiment.name)
+    pprint(cfg_flat)
+    
+    # set randome seed
+    set_random_seed(cfg.data.SEED)
+
+    # from experiments.seg_model import SegModel
+    mySegModel = SegModel(cfg)
+    mySegModel.run()
+
+    # evaluation
+    from s1s2_evaluator import evaluate_model
+    evaluate_model(cfg, mySegModel.model_url, mySegModel.rundir / "errMap")
+    
+    if not cfg.model.DEBUG:
+        wandb.finish()
+
+
+if __name__ == "__main__":
+    run_app()
diff --git a/main_s1s2_unet.py b/main_s1s2_unet.py
new file mode 100644
index 0000000..35a5f6f
--- /dev/null
+++ b/main_s1s2_unet.py
@@ -0,0 +1,418 @@
+# checkpoint: https://github.com/pytorch/examples/blob/42e5b996718797e45c46a25c55b031e6768f8440/imagenet/main.py#L89-L101
+
+import os, json
+import random
+from easydict import EasyDict as edict
+from pathlib import Path
+from prettyprinter import pprint
+from imageio import imread, imsave
+
+import hydra
+import wandb
+from omegaconf import DictConfig, OmegaConf
+
+
+###################################################################################
+import os, sys
+import numpy as np
+from pathlib import Path
+import hydra
+from omegaconf import DictConfig, OmegaConf
+
+import copy
+import time
+import torch
+
+import torch.optim as optim
+import torch.nn as nn
+import torch.nn.functional as F
+from easydict import EasyDict as edict
+
+from tqdm import tqdm as tqdm
+
+import logging
+logger = logging.getLogger(__name__)
+
+import smp
+from smp.base.modules import Activation
+from models.model_selection import get_model
+import wandb
+
+# f_score = smp.utils.functional.f_score
+# Dice/F1 score - https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient
+# IoU/Jaccard score - https://en.wikipedia.org/wiki/Jaccard_index
+# diceLoss = smp.utils.losses.DiceLoss(eps=1)
+from models.loss_ref import soft_dice_loss, soft_dice_loss_balanced, jaccard_like_loss, jaccard_like_balanced_loss
+
+AverageValueMeter =  smp.utils.train.AverageValueMeter
+
+# Augmentations
+from dataset.augument import get_training_augmentation, \
+    get_validation_augmentation, get_preprocessing
+
+from torch.utils.data import DataLoader
+from dataset.wildfire import S1S2 as Dataset # ------------------------------------------------------- Dataset
+
+from models.lr_schedule import get_cosine_schedule_with_warmup, PolynomialLRDecay
+
+
+def format_logs(logs):
+    str_logs = ['{}: {:.4}'.format(k, v) for k, v in logs.items()]
+    s = ', '.join(str_logs)
+    return s
+
+
+def loss_fun(CFG, DEVICE='cuda'):
+    if CFG.MODEL.LOSS_TYPE == 'BCELoss':
+        criterion = nn.BCELoss()
+
+    elif CFG.MODEL.LOSS_TYPE == 'BCEWithLogitsLoss':
+        criterion = nn.BCEWithLogitsLoss() # includes sigmoid activation
+
+    elif CFG.MODEL.LOSS_TYPE == 'DiceLoss':
+        criterion = smp.utils.losses.DiceLoss(eps=1, activation=CFG.MODEL.ACTIVATION)
+
+    elif CFG.MODEL.LOSS_TYPE == 'CrossEntropyLoss':
+        # balance_weight = [CFG.MODEL.NEGATIVE_WEIGHT, CFG.MODEL.POSITIVE_WEIGHT]
+        # balance_weight = torch.tensor(balance_weight).float().to(DEVICE)
+        # criterion = nn.CrossEntropyLoss(weight = balance_weight)
+
+        balance_weight = [class_weight for class_weight in CFG.MODEL.CLASS_WEIGHTS]
+        balance_weight = torch.tensor(balance_weight).float().to(DEVICE)
+        criterion = nn.CrossEntropyLoss(weight = balance_weight, ignore_index=-1)
+        # criterion = nn.CrossEntropyLoss(ignore_index=-1)
+        
+    elif CFG.MODEL.LOSS_TYPE == 'SoftDiceLoss':
+        criterion = soft_dice_loss 
+    elif CFG.MODEL.LOSS_TYPE == 'SoftDiceBalancedLoss':
+        criterion = soft_dice_loss_balanced
+    elif CFG.MODEL.LOSS_TYPE == 'JaccardLikeLoss':
+        criterion = jaccard_like_loss
+    elif CFG.MODEL.LOSS_TYPE == 'ComboLoss':
+        criterion = lambda pred, gts: F.binary_cross_entropy_with_logits(pred, gts) + soft_dice_loss(pred, gts)
+    elif CFG.MODEL.LOSS_TYPE == 'WeightedComboLoss':
+        criterion = lambda pred, gts: F.binary_cross_entropy_with_logits(pred, gts) + 10 * soft_dice_loss(pred, gts)
+    elif CFG.MODEL.LOSS_TYPE == 'FrankensteinLoss':
+        criterion = lambda pred, gts: F.binary_cross_entropy_with_logits(pred, gts) + jaccard_like_balanced_loss(pred, gts)
+
+    return criterion
+
+class SegModel(object):
+    def __init__(self, cfg) -> None:
+        super().__init__()
+        self.PROJECT_DIR = Path(hydra.utils.get_original_cwd())
+
+        self.cfg = cfg
+        self.DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
+        # self.DEVICE = 'cpu'
+
+        self.model = get_model(cfg)
+        self.activation = Activation(self.cfg.MODEL.ACTIVATION)
+
+        # self.MODEL_URL = str(self.PROJECT_DIR / "outputs" / "best_model.pth")
+        self.RUN_DIR = self.PROJECT_DIR / self.cfg.EXP.OUTPUT
+        self.MODEL_URL = str(self.RUN_DIR / "model.pth")
+
+        # if self.cfg.MODEL.ENCODER is not None:
+        #     self.preprocessing_fn = \
+        #         smp.encoders.get_preprocessing_fn(self.cfg.MODEL.ENCODER, self.cfg.MODEL.ENCODER_WEIGHTS)
+
+        self.metrics = [smp.utils.metrics.IoU(threshold=0.5, activation=None),
+                        smp.utils.metrics.Fscore(activation=None)
+                    ]
+
+        '''--------------> need to improve <-----------------'''
+        # specify data folder
+        self.TRAIN_DIR = Path(self.cfg.DATA.DIR) / 'train'
+        self.VALID_DIR = Path(self.cfg.DATA.DIR) / 'test'
+        '''--------------------------------------------------'''
+    
+    def get_dataloaders(self) -> dict:
+
+        # if self.cfg.MODEL.NUM_CLASSES == 1:
+        #     classes = ['burned']
+        # elif self.cfg.MODEL.NUM_CLASSES == 2:
+        #     classes = ['unburn', 'burned']
+        # elif self.cfg.MODEL.NUM_CLASSES > 2:
+        #     print(" ONLY ALLOW ONE or TWO CLASSES SO FAR !!!")
+        #     pass
+
+        classes = self.cfg.MODEL.CLASS_NAMES
+
+        """ Data Preparation """
+        train_dataset = Dataset(
+            self.TRAIN_DIR, 
+            self.cfg, 
+            # augmentation=get_training_augmentation(), 
+            # preprocessing=get_preprocessing(self.preprocessing_fn),
+            classes=classes,
+        )
+
+        valid_dataset = Dataset(
+            self.VALID_DIR, 
+            self.cfg, 
+            # augmentation=get_validation_augmentation(), 
+            # preprocessing=get_preprocessing(self.preprocessing_fn),
+            classes=classes,
+        )
+
+        generator=torch.Generator().manual_seed(self.cfg.RAND.SEED)
+        train_size = int(len(train_dataset) * self.cfg.DATA.TRAIN_RATIO)
+        valid_size = len(train_dataset) - train_size
+        train_set, val_set = torch.utils.data.random_split(train_dataset, [train_size, valid_size], generator=generator)
+
+        train_loader = DataLoader(train_set, batch_size=self.cfg.MODEL.BATCH_SIZE, shuffle=True, num_workers=4, generator=generator)
+        valid_loader = DataLoader(val_set, batch_size=self.cfg.MODEL.BATCH_SIZE, shuffle=True, num_workers=4, generator=generator)
+        test_loader = DataLoader(valid_dataset, batch_size=self.cfg.MODEL.BATCH_SIZE, shuffle=True, num_workers=4, generator=generator)
+
+# means = []
+# stds = []
+# for img in list(iter(train_loader)):
+#     print(img.shape)
+#     means.append(torch.mean(img))
+#     stds.append(torch.std(img))
+
+# mean = torch.mean(torch.tensor(means))
+# std = torch.mean(torch.tensor(stds))
+        
+        dataloaders = { 
+                        'train': train_loader, \
+                        'valid': valid_loader, \
+                        'test': test_loader, \
+
+                        'train_size': train_size, \
+                        'valid_size': valid_size, \
+                        'test_size': len(valid_dataset)
+                    }
+
+        return dataloaders
+
+
+    def run(self) -> None:
+        self.model.to(self.DEVICE)
+        self.criterion = loss_fun(self.cfg)
+
+        self.dataloaders = self.get_dataloaders()
+        self.optimizer = torch.optim.Adam([dict(
+                params=self.model.parameters(), 
+                lr=self.cfg.MODEL.LEARNING_RATE, 
+                weight_decay=self.cfg.MODEL.WEIGHT_DECAY)])
+
+        """ ===================== >> learning rate scheduler << ========================= """
+        per_epoch_steps = self.dataloaders['train_size'] // self.cfg.MODEL.BATCH_SIZE
+        total_training_steps = self.cfg.MODEL.MAX_EPOCH * per_epoch_steps
+
+        self.USE_LR_SCHEDULER = True \
+            if self.cfg.MODEL.LR_SCHEDULER in ['cosine_warmup', 'polynomial'] \
+            else True
+
+        if self.cfg.MODEL.LR_SCHEDULER == 'cosine':
+            ''' cosine scheduler '''
+            warmup_steps = self.cfg.MODEL.COSINE_SCHEDULER.WARMUP * per_epoch_steps
+            self.lr_scheduler = get_cosine_schedule_with_warmup(self.optimizer, warmup_steps, total_training_steps)
+
+        elif self.cfg.MODEL.LR_SCHEDULER == 'poly':
+            ''' polynomial '''
+            self.lr_scheduler = PolynomialLRDecay(self.optimizer, 
+                            max_decay_steps=total_training_steps, 
+                            end_learning_rate=self.cfg.MODEL.POLY_SCHEDULER.END_LR, #1e-5, 
+                            power=self.cfg.MODEL.POLY_SCHEDULER.POWER, #0.9
+                        )
+        else:
+            pass
+
+
+        # self.history_logs = edict()
+        # self.history_logs['train'] = []
+        # self.history_logs['valid'] = []
+        # self.history_logs['test'] = []
+
+        # --------------------------------- Train -------------------------------------------
+        max_score = self.cfg.MODEL.MAX_SCORE
+        self.iters = 0
+        for epoch in range(0, self.cfg.MODEL.MAX_EPOCH):
+            epoch = epoch + 1
+            print(f"\n==> train epoch: {epoch}/{self.cfg.MODEL.MAX_EPOCH}")
+            self.train_one_epoch(epoch)
+            valid_logs = self.valid_logs
+            
+            # do something (save model, change lr, etc.)
+            if valid_logs['iou_score'] > max_score:
+                max_score = valid_logs['iou_score']
+
+                if (1 == epoch) or (0 == (epoch % self.cfg.MODEL.SAVE_INTERVAL)):
+                    torch.save(self.model, self.MODEL_URL)
+                    # torch.save(self.model.state_dict(), self.MODEL_URL)
+                    print('Model saved!')
+
+            # if epoch % 50 == 0:
+            #     self.optimizer.param_groups[0]['lr'] = 0.1 * self.optimizer.param_groups[0]['lr']
+                        
+        
+    def train_one_epoch(self, epoch):
+    
+        # wandb.
+        for phase in ['train', 'valid', 'test']:
+            if phase == 'train':
+                self.model.train()
+            else:
+                self.model.eval()
+
+            logs = self.step(phase) 
+            # print(phase, logs)
+
+            currlr = self.optimizer.param_groups[0]['lr'] 
+            wandb.log({phase: logs, 'epoch': epoch, 'lr': currlr})
+
+            # temp = [logs["total_loss"]] + [logs[self.metrics[i].__name__] for i in range(0, len(self.metrics))]
+            # self.history_logs[phase].append(temp)
+
+            if phase == 'valid': self.valid_logs = logs
+
+
+    def step(self, phase) -> dict:
+        logs = {}
+        loss_meter = AverageValueMeter()
+        # metrics_meters = {metric.__name__: AverageValueMeter() for metric in self.metrics}
+        metrics_meters = {f"{metric.__name__}_class{cls}": AverageValueMeter() for metric in self.metrics for cls in range(0, max(2, self.cfg.MODEL.NUM_CLASSES))}
+
+        # if ('Train' in phase) and (self.cfg.useDataWoCAug):
+        #     dataLoader_woCAug = iter(self.dataloaders['Train_woCAug'])
+
+        with tqdm(iter(self.dataloaders[phase]), desc=phase, file=sys.stdout, disable=not self.cfg.MODEL.VERBOSE) as iterator:
+            for i, (x, y) in enumerate(iterator):
+                self.optimizer.zero_grad()
+
+                ''' move data to GPU '''
+                input = []
+                for x_ in x: input.append(x_.to(self.DEVICE))
+                y = y.to(self.DEVICE) # BCHW
+                # print(len(input))
+
+                ''' do prediction '''
+                if 'UNet_resnet' in self.cfg.MODEL.ARCH: 
+                    input = input[0]
+                    out = self.model.forward(input)
+                else:
+                    out = self.model.forward(input)[-1]
+
+                if 'softmax' in self.cfg.MODEL.ACTIVATION:
+                    y_gts = torch.argmax(y, dim=1) # BHW [0, 1, 2, NUM_CLASSES-1]
+                    y_pred = torch.argmax(self.activation(out), dim=1) # BHW
+                
+                ''' compute loss '''
+                # loss_ = self.criterion(out, y) # include activation
+                loss_ = self.criterion(out, y_gts) # Cross Entropy Loss
+
+                ''' Back Propergation (BP) '''
+                if phase == 'train':
+                    loss_.backward()
+                    self.optimizer.step()
+                    self.iters = self.iters + 1
+
+                    ''' Iteration-Wise log for train stage only '''
+                    if self.cfg.MODEL.STEP_WISE_LOG:
+                        self.iters = self.iters + 1
+                        currlr = self.optimizer.param_groups[0]['lr'] 
+                        # wandb.log({'x0.mean': x[0].mean()})
+                        wandb.log({phase: logs, 'iters': self.iters, 'lr': currlr})
+
+                    if self.USE_LR_SCHEDULER:
+                        self.lr_scheduler.step()
+
+
+                ''' update loss and metrics logs '''
+                # update loss logs
+                loss_value = loss_.cpu().detach().numpy()
+                loss_meter.add(loss_value)
+                # loss_logs = {criterion.__name__: loss_meter.mean}
+                loss_logs = {'total_loss': loss_meter.mean}
+                logs.update(loss_logs)
+
+                # update metrics logs
+                for metric_fn in self.metrics:
+                    # metric_value = metric_fn(y_pred, y_gts).cpu().detach().numpy()
+                    # metrics_meters[metric_fn.__name__].add(metric_value)
+
+                    for cls in range(0, max(2, self.cfg.MODEL.NUM_CLASSES)):
+                        # metric_value = metric_fn(y_pred[:,cls,...], y[:,cls,...]).cpu().detach().numpy()
+                        cls_pred = (y_pred==cls).type(torch.FloatTensor)
+                        cls_gts = (y_gts==cls).type(torch.FloatTensor)
+                        metric_value = metric_fn(cls_pred, cls_gts).cpu().detach().numpy()
+                        metrics_meters[f"{metric_fn.__name__}_class{cls}"].add(metric_value)
+
+                metrics_logs = {k: v.mean for k, v in metrics_meters.items()}
+                logs.update(metrics_logs)
+                # print(self.iters, x[0].mean().item(), loss_.item())
+
+                if self.cfg.MODEL.VERBOSE:
+                    s = format_logs(logs)
+                    iterator.set_postfix_str(s)
+
+        return logs
+##############################################################
+
+
+def set_random_seed(seed, deterministic=False):
+    """Set random seed.
+
+    Args:
+        seed (int): Seed to be used.
+        deterministic (bool): Whether to set the deterministic option for
+            CUDNN backend, i.e., set `torch.backends.cudnn.deterministic`
+            to True and `torch.backends.cudnn.benchmark` to False.
+            Default: False.
+    """
+    random.seed(seed)
+    np.random.seed(seed)
+    torch.manual_seed(seed)
+    torch.cuda.manual_seed_all(seed)
+    if deterministic:
+        torch.backends.cudnn.deterministic = True
+        torch.backends.cudnn.benchmark = False
+
+
+@hydra.main(config_path="./config", config_name="unet")
+def run_app(cfg : DictConfig) -> None:
+
+    ''' set randome seed '''
+    os.environ['HYDRA_FULL_ERROR'] = str(1)
+    os.environ['PYTHONHASHSEED'] = str(cfg.RAND.SEED) #cfg.RAND.SEED
+    if cfg.RAND.DETERMIN:
+        os.environ['CUBLAS_WORKSPACE_CONFIG']=":4096:8" #https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility
+        torch.use_deterministic_algorithms(True)
+    set_random_seed(cfg.RAND.SEED, deterministic=cfg.RAND.DETERMIN)
+
+    # wandb.init(config=cfg, project=cfg.project.name, name=cfg.EXP.name)
+    import pandas as pd
+    from prettyprinter import pprint
+    
+    cfg_dict = OmegaConf.to_container(cfg, resolve=True)
+    cfg_flat = pd.json_normalize(cfg_dict, sep='_').to_dict(orient='records')[0]
+
+    wandb.init(config=cfg_flat, project="wildfire-s1s2alos-canada-rse", entity=cfg.PROJECT.ENTITY, name=cfg.EXP.NAME)
+    # wandb.init(config=cfg_flat, project=cfg.PROJECT.NAME, entity=cfg.PROJECT.ENTITY, name=cfg.EXP.NAME)
+    pprint(cfg_flat)
+
+    ''' train '''
+    # from experiments.seg_model import SegModel
+    mySegModel = SegModel(cfg)
+    mySegModel.run()
+
+    ''' inference '''
+    from s1s2_evaluator import evaluate_model
+    evaluate_model(cfg, mySegModel.MODEL_URL, mySegModel.RUN_DIR / "errMap")
+
+    ''' compute IoU and F1 for all events '''
+    from utils.iou4all import compute_IoU_F1
+    compute_IoU_F1(phase="test_images", 
+                    result_dir=mySegModel.RUN_DIR / "errMap", 
+                    dataset_dir=cfg.DATA.DIR)
+    
+    # if not cfg.MODEL.DEBUG:
+    wandb.finish()
+
+
+if __name__ == "__main__":
+
+    run_app()
diff --git a/models/loss_ref.py b/models/loss_ref.py
new file mode 100644
index 0000000..c6bbaf0
--- /dev/null
+++ b/models/loss_ref.py
@@ -0,0 +1,109 @@
+import torch
+def soft_dice_loss(input:torch.Tensor, target:torch.Tensor):
+    input_sigmoid = torch.sigmoid(input)
+    eps = 1e-6
+
+    iflat = input_sigmoid.flatten()
+    tflat = target.flatten()
+    intersection = (iflat * tflat).sum()
+
+    return 1 - ((2. * intersection) /
+                (iflat.sum() + tflat.sum() + eps))
+
+def soft_dice_loss_multi_class(input:torch.Tensor, y:torch.Tensor):
+    p = torch.softmax(input, dim=1)
+    eps = 1e-6
+
+    sum_dims= (0, 2, 3) # Batch, height, width
+
+    intersection = (y * p).sum(dim=sum_dims)
+    denom =  (y.sum(dim=sum_dims) + p.sum(dim=sum_dims)).clamp(eps)
+
+    loss = 1 - (2. * intersection / denom).mean()
+    return loss
+
+def soft_dice_loss_multi_class_debug(input:torch.Tensor, y:torch.Tensor):
+    p = torch.softmax(input, dim=1)
+    eps = 1e-6
+
+    sum_dims= (0, 2, 3) # Batch, height, width
+
+    intersection = (y * p).sum(dim=sum_dims)
+    denom =  (y.sum(dim=sum_dims) + p.sum(dim=sum_dims)).clamp(eps)
+
+    loss = 1 - (2. * intersection / denom).mean()
+    loss_components = 1 - 2 * intersection/denom
+    return loss, loss_components
+
+def generalized_soft_dice_loss_multi_class(input:torch.Tensor, y:torch.Tensor):
+    p = torch.softmax(input, dim=1)
+    eps = 1e-12
+
+    # TODO [B, C, H, W] -> [C, B, H, W] because softdice includes all pixels
+
+    sum_dims= (0, 2, 3) # Batch, height, width
+    ysum = y.sum(dim=sum_dims)
+    wc = 1 / (ysum ** 2 + eps)
+    intersection = ((y * p).sum(dim=sum_dims) * wc).sum()
+    denom =  ((ysum + p.sum(dim=sum_dims)) * wc).sum()
+
+    loss = 1 - (2. * intersection / denom)
+    return loss
+
+def jaccard_like_loss_multi_class(input:torch.Tensor, y:torch.Tensor):
+    p = torch.softmax(input, dim=1)
+    eps = 1e-6
+
+    # TODO [B, C, H, W] -> [C, B, H, W] because softdice includes all pixels
+
+    sum_dims= (0, 2, 3) # Batch, height, width
+
+    intersection = (y * p).sum(dim=sum_dims)
+    denom =  (y ** 2 + p ** 2).sum(dim=sum_dims) + (y*p).sum(dim=sum_dims) + eps
+
+    loss = 1 - (2. * intersection / denom).mean()
+    return loss
+
+def jaccard_like_loss(input:torch.Tensor, target:torch.Tensor):
+    input_sigmoid = torch.sigmoid(input)
+    eps = 1e-6
+
+    iflat = input_sigmoid.flatten()
+    tflat = target.flatten()
+    intersection = (iflat * tflat).sum()
+    denom = (iflat**2 + tflat**2).sum() - (iflat * tflat).sum() + eps
+
+    return 1 - ((2. * intersection) / denom)
+def jaccard_like_balanced_loss(input:torch.Tensor, target:torch.Tensor):
+    input_sigmoid = torch.sigmoid(input)
+    eps = 1e-6
+
+    iflat = input_sigmoid.flatten()
+    tflat = target.flatten()
+    intersection = (iflat * tflat).sum()
+    denom = (iflat**2 + tflat**2).sum() - (iflat * tflat).sum() + eps
+    piccard = (2. * intersection)/denom
+
+    n_iflat = 1-iflat
+    n_tflat = 1-tflat
+    neg_intersection = (n_iflat * n_tflat).sum()
+    neg_denom = (n_iflat**2 + n_tflat**2).sum() - (n_iflat * n_tflat).sum()
+    n_piccard = (2. * neg_intersection)/neg_denom
+
+    return 1 - piccard - n_piccard
+
+def soft_dice_loss_balanced(input:torch.Tensor, target:torch.Tensor):
+    input_sigmoid = torch.sigmoid(input)
+    eps = 1e-6
+
+    iflat = input_sigmoid.flatten()
+    tflat = target.flatten()
+    intersection = (iflat * tflat).sum()
+
+    dice_pos = ((2. * intersection) /
+                (iflat.sum() + tflat.sum() + eps))
+
+    negatiev_intersection = ((1-iflat) * (1 - tflat)).sum()
+    dice_neg =  (2 * negatiev_intersection) / ((1-iflat).sum() + (1-tflat).sum() + eps)
+
+    return 1 - dice_pos - dice_neg
\ No newline at end of file
diff --git a/models/lr_schedule.py b/models/lr_schedule.py
index b88e1c0..a7d69f7 100644
--- a/models/lr_schedule.py
+++ b/models/lr_schedule.py
@@ -1,5 +1,9 @@
+# https://kiranscaria.com/learning-rate-schedules
+
+
 import torch
-import math 
+import math
+from torch.optim import lr_scheduler 
 from torch.optim.lr_scheduler import LambdaLR
 
 def get_cosine_schedule_with_warmup(optimizer,
@@ -25,3 +29,50 @@ def get_cosine_schedule_with_warmup(optimizer,
             float(max(1, num_training_steps - num_warmup_steps))
         return max(0., math.cos(math.pi * num_cycles * no_progress))  # this is correct
     return LambdaLR(optimizer, _lr_lambda, last_epoch)
+
+
+
+from torch.optim.lr_scheduler import _LRScheduler
+class PolynomialLRDecay(_LRScheduler):
+    """Polynomial learning rate decay until step reach to max_decay_step
+    
+    Args:
+        optimizer (Optimizer): Wrapped optimizer.
+        max_decay_steps: after this step, we stop decreasing learning rate
+        end_learning_rate: scheduler stoping learning rate decay, value of learning rate must be this value
+        power: The power of the polynomial.
+    """
+    
+    def __init__(self, optimizer, max_decay_steps, end_learning_rate=0.0001, power=1.0):
+        if max_decay_steps <= 1.:
+            raise ValueError('max_decay_steps should be greater than 1.')
+        self.max_decay_steps = max_decay_steps
+        self.end_learning_rate = end_learning_rate
+        self.power = power
+        self.last_step = 0
+        super().__init__(optimizer)
+        
+    def get_lr(self):
+        if self.last_step > self.max_decay_steps:
+            return [self.end_learning_rate for _ in self.base_lrs]
+
+        return [(base_lr - self.end_learning_rate) * 
+                ((1 - self.last_step / self.max_decay_steps) ** (self.power)) + 
+                self.end_learning_rate for base_lr in self.base_lrs]
+    
+    def step(self, step=None):
+        if step is None:
+            step = self.last_step + 1
+        self.last_step = step if step != 0 else 1
+        if self.last_step <= self.max_decay_steps:
+            decay_lrs = [(base_lr - self.end_learning_rate) * 
+                         ((1 - self.last_step / self.max_decay_steps) ** (self.power)) + 
+                         self.end_learning_rate for base_lr in self.base_lrs]
+            for param_group, lr in zip(self.optimizer.param_groups, decay_lrs):
+                param_group['lr'] = lr
+
+        # added by puzhao
+        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]
+
+
+    
\ No newline at end of file
diff --git a/models/mix_transformer.py b/models/mix_transformer.py
new file mode 100644
index 0000000..7ac46b1
--- /dev/null
+++ b/models/mix_transformer.py
@@ -0,0 +1,634 @@
+# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import math
+from functools import partial
+import numpy as np
+import os
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.nn.init import zeros_, ones_, trunc_normal_, normal_
+
+
+def to_2tuple(x):
+    return tuple([x] * 2)
+
+def drop_path(x, drop_prob=0., training=False):
+    """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).
+    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
+    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ...
+    """
+    if drop_prob == 0. or not training:
+        return x
+    keep_prob = torch.tensor(1 - drop_prob)
+    shape = (x.shape[0], ) + (1, ) * (x.ndim - 1)
+    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype)
+    random_tensor = torch.floor(random_tensor)  # binarize
+
+    # ------------- added by puzhao ---------------------
+    device = 'cuda' if torch.cuda.is_available() else 'cpu'
+    random_tensor = random_tensor.to(device) # added by puzhao
+    keep_prob = keep_prob.to(device) # added by puzhao
+    #--------------------------------------------------------
+
+    output = x.divide(keep_prob) * random_tensor
+    return output
+
+class DropPath(nn.Module):
+    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
+    """
+
+    def __init__(self, drop_prob=None):
+        super(DropPath, self).__init__()
+        self.drop_prob = drop_prob
+
+    def forward(self, x):
+        return drop_path(x, self.drop_prob, self.training)
+
+
+class Mlp(nn.Module):
+    def __init__(self,
+                 in_features,
+                 hidden_features=None,
+                 out_features=None,
+                 act_layer=nn.GELU,
+                 drop=0.):
+        super().__init__()
+        out_features = out_features or in_features
+        hidden_features = hidden_features or in_features
+        self.fc1 = nn.Linear(in_features, hidden_features)
+        self.dwconv = DWConv(hidden_features)
+        self.act = act_layer()
+        self.fc2 = nn.Linear(hidden_features, out_features)
+        self.drop = nn.Dropout(drop)
+
+        self.apply(self._init_weights)
+
+    def _init_weights(self, m):
+        if isinstance(m, nn.Linear):
+            trunc_normal_(m.weight, std=0.02)
+            if isinstance(m, nn.Linear) and m.bias is not None:
+                zeros_(m.bias)
+        elif isinstance(m, nn.LayerNorm):
+            zeros_(m.bias)
+            ones_(m.weight)
+        elif isinstance(m, nn.Conv2d):
+            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
+            fan_out //= m.groups
+            normal_(m.weight, 0, math.sqrt(2.0 / fan_out))
+            if m.bias is not None:
+                zeros_(m.bias)
+
+    def forward(self, x, H, W):
+        x = self.fc1(x)
+        x = self.dwconv(x, H, W)
+        x = self.act(x)
+        x = self.drop(x)
+        x = self.fc2(x)
+        x = self.drop(x)
+        return x
+
+
+class Attention(nn.Module):
+    def __init__(self,
+                 dim,
+                 num_heads=8,
+                 qkv_bias=False,
+                 qk_scale=None,
+                 attn_drop=0.,
+                 proj_drop=0.,
+                 sr_ratio=1):
+        super().__init__()
+        assert dim % num_heads == 0, f"dim {dim} should be divided by num_heads {num_heads}."
+
+        self.dim = dim
+        self.num_heads = num_heads
+        head_dim = dim // num_heads
+        self.scale = qk_scale or head_dim**-0.5
+        self.dim = dim
+
+        self.q = nn.Linear(dim, dim, bias=qkv_bias)
+        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)
+        self.attn_drop = nn.Dropout(attn_drop)
+        self.proj = nn.Linear(dim, dim)
+        self.proj_drop = nn.Dropout(proj_drop)
+
+        self.sr_ratio = sr_ratio
+        if sr_ratio > 1:
+            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)
+            self.norm = nn.LayerNorm(dim)
+
+        self.apply(self._init_weights)
+
+    def _init_weights(self, m):
+        if isinstance(m, nn.Linear):
+            trunc_normal_(m.weight, std=0.02)
+            if isinstance(m, nn.Linear) and m.bias is not None:
+                zeros_(m.bias)
+        elif isinstance(m, nn.LayerNorm):
+            zeros_(m.bias)
+            ones_(m.weight)
+        elif isinstance(m, nn.Conv2d):
+            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
+            fan_out //= m.groups
+            normal_(m.weight, 0, math.sqrt(2.0 / fan_out))
+            if m.bias is not None:
+                zeros_(m.bias)
+
+    def forward(self, x, H, W):
+        x_shape = x.shape #paddle.shape(x)
+        B, N = x_shape[0], x_shape[1]
+        C = self.dim
+
+        q = self.q(x).reshape([B, N, self.num_heads,
+                               C // self.num_heads]).transpose(2, 1)
+
+        if self.sr_ratio > 1:
+            x_ = x.permute([0, 2, 1]).reshape([B, C, H, W])
+            x_ = self.sr(x_).reshape([B, C, -1]).transpose(2, 1)
+            x_ = self.norm(x_)
+            kv = self.kv(x_).reshape(
+                [B, -1, 2, self.num_heads,
+                 C // self.num_heads]).permute([2, 0, 3, 1, 4])
+        else:
+            kv = self.kv(x).reshape(
+                [B, -1, 2, self.num_heads,
+                 C // self.num_heads]).permute([2, 0, 3, 1, 4])
+        k, v = kv[0], kv[1]
+
+        attn = (q @ k.permute([0, 1, 3, 2])) * self.scale
+        attn = F.softmax(attn, dim=-1)
+        attn = self.attn_drop(attn)
+
+        x = (attn @ v).permute([0, 2, 1, 3]).reshape([B, N, C])
+        x = self.proj(x)
+        x = self.proj_drop(x)
+
+        return x
+
+
+class Block(nn.Module):
+    def __init__(self,
+                 dim,
+                 num_heads,
+                 mlp_ratio=4.,
+                 qkv_bias=False,
+                 qk_scale=None,
+                 drop=0.,
+                 attn_drop=0.,
+                 drop_path=0.,
+                 act_layer=nn.GELU,
+                 norm_layer=nn.LayerNorm,
+                 sr_ratio=1):
+        super(Block, self).__init__()
+        self.norm1 = norm_layer(dim)
+        self.attn = Attention(
+            dim,
+            num_heads=num_heads,
+            qkv_bias=qkv_bias,
+            qk_scale=qk_scale,
+            attn_drop=attn_drop,
+            proj_drop=drop,
+            sr_ratio=sr_ratio)
+        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.norm2 = norm_layer(dim)
+        mlp_hidden_dim = int(dim * mlp_ratio)
+        self.mlp = Mlp(
+            in_features=dim,
+            hidden_features=mlp_hidden_dim,
+            act_layer=act_layer,
+            drop=drop)
+
+        self.apply(self._init_weights)
+
+    def _init_weights(self, m):
+        if isinstance(m, nn.Linear):
+            trunc_normal_(m.weight, std=0.02)
+            if isinstance(m, nn.Linear) and m.bias is not None:
+                zeros_(m.bias)
+        elif isinstance(m, nn.LayerNorm):
+            zeros_(m.bias)
+            ones_(m.weight)
+        elif isinstance(m, nn.Conv2d):
+            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
+            fan_out //= m.groups
+            normal_(m.weight, 0, math.sqrt(2.0 / fan_out))
+            if m.bias is not None:
+                zeros_(m.bias)
+
+    def forward(self, x, H, W):
+        x = x + self.drop_path(self.attn(self.norm1(x), H, W))
+        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))
+
+        return x
+
+
+class OverlapPatchEmbed(nn.Module):
+    """ Image to Patch Embedding
+    """
+
+    def __init__(self,
+                 img_size=224,
+                 patch_size=7,
+                 stride=4,
+                 in_chans=3,
+                 embed_dim=768):
+        super().__init__()
+        img_size = to_2tuple(img_size)
+        patch_size = to_2tuple(patch_size)
+
+        self.img_size = img_size
+        self.patch_size = patch_size
+        self.H, self.W = img_size[0] // patch_size[0], img_size[
+            1] // patch_size[1]
+        self.num_patches = self.H * self.W
+        self.proj = nn.Conv2d(
+            in_chans,
+            embed_dim,
+            kernel_size=patch_size,
+            stride=stride,
+            padding=(patch_size[0] // 2, patch_size[1] // 2))
+        self.norm = nn.LayerNorm(embed_dim)
+
+        self.apply(self._init_weights)
+
+    def _init_weights(self, m):
+        if isinstance(m, nn.Linear):
+            trunc_normal_(m.weight, std=0.02)
+            if isinstance(m, nn.Linear) and m.bias is not None:
+                zeros_(m.bias)
+        elif isinstance(m, nn.LayerNorm):
+            zeros_(m.bias)
+            ones_(m.weight)
+        elif isinstance(m, nn.Conv2d):
+            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
+            fan_out //= m.groups
+            normal_(m.weight, 0, math.sqrt(2.0 / fan_out))
+            if m.bias is not None:
+                zeros_(m.bias)
+
+    def forward(self, x):
+        x = self.proj(x)
+        x_shape = x.shape
+        H, W = x_shape[2], x_shape[3]
+        x = x.flatten(2).transpose(2, 1)
+        x = self.norm(x)
+
+        return x, H, W
+
+
+class MixVisionTransformer(nn.Module):
+    def __init__(self,
+                 img_size=224,
+                 patch_size=16,
+                 in_chans=3,
+                 num_classes=1000,
+                 embed_dims=[64, 128, 256, 512],
+                 num_heads=[1, 2, 4, 8],
+                 mlp_ratios=[4, 4, 4, 4],
+                 qkv_bias=False,
+                 qk_scale=None,
+                 drop_rate=0.,
+                 attn_drop_rate=0.,
+                 drop_path_rate=0.,
+                 norm_layer=nn.LayerNorm,
+                 depths=[3, 4, 6, 3],
+                 sr_ratios=[8, 4, 2, 1],
+                 pretrained=None):
+        super(MixVisionTransformer, self).__init__()
+        self.num_classes = num_classes
+        self.depths = depths
+        self.feat_channels = embed_dims[:]
+
+        # patch_embed
+        self.patch_embed1 = OverlapPatchEmbed(
+            img_size=img_size,
+            patch_size=7,
+            stride=4,
+            in_chans=in_chans,
+            embed_dim=embed_dims[0])
+        self.patch_embed2 = OverlapPatchEmbed(
+            img_size=img_size // 4,
+            patch_size=3,
+            stride=2,
+            in_chans=embed_dims[0],
+            embed_dim=embed_dims[1])
+        self.patch_embed3 = OverlapPatchEmbed(
+            img_size=img_size // 8,
+            patch_size=3,
+            stride=2,
+            in_chans=embed_dims[1],
+            embed_dim=embed_dims[2])
+        self.patch_embed4 = OverlapPatchEmbed(
+            img_size=img_size // 16,
+            patch_size=3,
+            stride=2,
+            in_chans=embed_dims[2],
+            embed_dim=embed_dims[3])
+
+        # transformer encoder
+        dpr = np.linspace(0, drop_path_rate, sum(depths))
+        # dpr = [
+        #     x.numpy() for x in paddle.linspace(0, drop_path_rate, sum(depths))
+        # ]  # stochastic depth decay rule
+        cur = 0
+        self.block1 = nn.Sequential(
+            *[Block(
+                dim=embed_dims[0],
+                num_heads=num_heads[0],
+                mlp_ratio=mlp_ratios[0],
+                qkv_bias=qkv_bias,
+                qk_scale=qk_scale,
+                drop=drop_rate,
+                attn_drop=attn_drop_rate,
+                drop_path=dpr[cur + i],
+                norm_layer=norm_layer,
+                sr_ratio=sr_ratios[0]) for i in range(depths[0])]
+        )
+        self.norm1 = norm_layer(embed_dims[0])
+
+        cur += depths[0]
+        self.block2 = nn.Sequential(*[
+            Block(
+                dim=embed_dims[1],
+                num_heads=num_heads[1],
+                mlp_ratio=mlp_ratios[1],
+                qkv_bias=qkv_bias,
+                qk_scale=qk_scale,
+                drop=drop_rate,
+                attn_drop=attn_drop_rate,
+                drop_path=dpr[cur + i],
+                norm_layer=norm_layer,
+                sr_ratio=sr_ratios[1]) for i in range(depths[1])
+        ])
+        self.norm2 = norm_layer(embed_dims[1])
+
+        cur += depths[1]
+        self.block3 = nn.Sequential(*[
+            Block(
+                dim=embed_dims[2],
+                num_heads=num_heads[2],
+                mlp_ratio=mlp_ratios[2],
+                qkv_bias=qkv_bias,
+                qk_scale=qk_scale,
+                drop=drop_rate,
+                attn_drop=attn_drop_rate,
+                drop_path=dpr[cur + i],
+                norm_layer=norm_layer,
+                sr_ratio=sr_ratios[2]) for i in range(depths[2])
+        ])
+        self.norm3 = norm_layer(embed_dims[2])
+
+        cur += depths[2]
+        self.block4 = nn.Sequential(*[
+            Block(
+                dim=embed_dims[3],
+                num_heads=num_heads[3],
+                mlp_ratio=mlp_ratios[3],
+                qkv_bias=qkv_bias,
+                qk_scale=qk_scale,
+                drop=drop_rate,
+                attn_drop=attn_drop_rate,
+                drop_path=dpr[cur + i],
+                norm_layer=norm_layer,
+                sr_ratio=sr_ratios[3]) for i in range(depths[3])
+        ])
+        self.norm4 = norm_layer(embed_dims[3])
+
+        self.pretrained = pretrained
+        self.init_weight()
+
+    def init_weight(self):
+        # if self.pretrained is not None:
+        #     load_pretrained_model(self, self.pretrained)
+        # else:
+        self.apply(self._init_weights)
+
+    def _init_weights(self, m):
+        if isinstance(m, nn.Linear):
+            trunc_normal_(m.weight, std=0.02)
+            if isinstance(m, nn.Linear) and m.bias is not None:
+                zeros_(m.bias)
+        elif isinstance(m, nn.LayerNorm):
+            zeros_(m.bias)
+            ones_(m.weight)
+        elif isinstance(m, nn.Conv2d):
+            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
+            fan_out //= m.groups
+            normal_(m.weight, 0, math.sqrt(2.0 / fan_out))
+            if m.bias is not None:
+                zeros_(m.bias)
+
+    def reset_drop_path(self, drop_path_rate):
+        # dpr = [
+        #     x.item()
+        #     for x in paddle.linspace(0, drop_path_rate, sum(self.depths))
+        # ]
+        dpr = np.linspace(0, drop_path_rate, sum(self.depths))
+        cur = 0
+        for i in range(self.depths[0]):
+            self.block1[i].drop_path.drop_prob = dpr[cur + i]
+
+        cur += self.depths[0]
+        for i in range(self.depths[1]):
+            self.block2[i].drop_path.drop_prob = dpr[cur + i]
+
+        cur += self.depths[1]
+        for i in range(self.depths[2]):
+            self.block3[i].drop_path.drop_prob = dpr[cur + i]
+
+        cur += self.depths[2]
+        for i in range(self.depths[3]):
+            self.block4[i].drop_path.drop_prob = dpr[cur + i]
+
+    def freeze_patch_emb(self):
+        self.patch_embed1.requires_grad = False
+
+    def get_classifier(self):
+        return self.head
+
+    def reset_classifier(self, num_classes, global_pool=''):
+        self.num_classes = num_classes
+        self.head = nn.Linear(
+            self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()
+
+    def forward_features(self, x):
+        B = x.shape[0]
+        outs = []
+
+        # stage 1
+        x, H, W = self.patch_embed1(x)
+        for i, blk in enumerate(self.block1):
+            x = blk(x, H, W)
+
+        x = self.norm1(x)
+        x = x.reshape([B, H, W, self.feat_channels[0]]).permute([0, 3, 1, 2])
+        outs.append(x)
+
+        # stage 2
+        x, H, W = self.patch_embed2(x)
+        for i, blk in enumerate(self.block2):
+            x = blk(x, H, W)
+        x = self.norm2(x)
+        x = x.reshape([B, H, W, self.feat_channels[1]]).permute([0, 3, 1, 2])
+        outs.append(x)
+
+        # stage 3
+        x, H, W = self.patch_embed3(x)
+        for i, blk in enumerate(self.block3):
+            x = blk(x, H, W)
+        x = self.norm3(x)
+        x = x.reshape([B, H, W, self.feat_channels[2]]).permute([0, 3, 1, 2])
+        outs.append(x)
+
+        # stage 4
+        x, H, W = self.patch_embed4(x)
+        for i, blk in enumerate(self.block4):
+            x = blk(x, H, W)
+        x = self.norm4(x)
+        x = x.reshape([B, H, W, self.feat_channels[3]]).permute([0, 3, 1, 2])
+        outs.append(x)
+
+        return outs
+
+    def forward(self, x):
+        x = self.forward_features(x)
+        # x = self.head(x)
+
+        return x
+
+
+class DWConv(nn.Module):
+    def __init__(self, dim=768):
+        super(DWConv, self).__init__()
+        self.dim = dim
+        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)
+
+    def forward(self, x, H, W):
+        x_shape = x.shape
+        B, N = x_shape[0], x_shape[1]
+        x = x.permute([0, 2, 1]).reshape([B, self.dim, H, W])
+        x = self.dwconv(x)
+        x = x.flatten(2).permute([0, 2, 1])
+
+        return x
+
+
+
+def MixVisionTransformer_B0(**kwargs):
+    return MixVisionTransformer(
+        patch_size=4,
+        embed_dims=[32, 64, 160, 256],
+        num_heads=[1, 2, 5, 8],
+        mlp_ratios=[4, 4, 4, 4],
+        qkv_bias=True,
+        norm_layer=partial(nn.LayerNorm, eps=1e-6),
+        depths=[2, 2, 2, 2],
+        sr_ratios=[8, 4, 2, 1],
+        drop_rate=0.0,
+        drop_path_rate=0.1,
+        **kwargs)
+
+
+
+def MixVisionTransformer_B1(**kwargs):
+    return MixVisionTransformer(
+        patch_size=4,
+        embed_dims=[64, 128, 320, 512],
+        num_heads=[1, 2, 5, 8],
+        mlp_ratios=[4, 4, 4, 4],
+        qkv_bias=True,
+        norm_layer=partial(nn.LayerNorm, eps=1e-6),
+        depths=[2, 2, 2, 2],
+        sr_ratios=[8, 4, 2, 1],
+        drop_rate=0.0,
+        drop_path_rate=0.1,
+        **kwargs)
+
+
+
+def MixVisionTransformer_B2(**kwargs):
+    return MixVisionTransformer(
+        patch_size=4,
+        embed_dims=[64, 128, 320, 512],
+        num_heads=[1, 2, 5, 8],
+        mlp_ratios=[4, 4, 4, 4],
+        qkv_bias=True,
+        norm_layer=partial(nn.LayerNorm, eps=1e-6),
+        depths=[3, 4, 6, 3],
+        sr_ratios=[8, 4, 2, 1],
+        drop_rate=0.0,
+        drop_path_rate=0.1,
+        **kwargs)
+
+
+
+def MixVisionTransformer_B3(**kwargs):
+    return MixVisionTransformer(
+        patch_size=4,
+        embed_dims=[64, 128, 320, 512],
+        num_heads=[1, 2, 5, 8],
+        mlp_ratios=[4, 4, 4, 4],
+        qkv_bias=True,
+        norm_layer=partial(nn.LayerNorm, eps=1e-6),
+        depths=[3, 4, 18, 3],
+        sr_ratios=[8, 4, 2, 1],
+        drop_rate=0.0,
+        drop_path_rate=0.1,
+        **kwargs)
+
+
+
+def MixVisionTransformer_B4(**kwargs):
+    return MixVisionTransformer(
+        patch_size=4,
+        embed_dims=[64, 128, 320, 512],
+        num_heads=[1, 2, 5, 8],
+        mlp_ratios=[4, 4, 4, 4],
+        qkv_bias=True,
+        norm_layer=partial(nn.LayerNorm, eps=1e-6),
+        depths=[3, 8, 27, 3],
+        sr_ratios=[8, 4, 2, 1],
+        drop_rate=0.0,
+        drop_path_rate=0.1,
+        **kwargs)
+
+
+
+def MixVisionTransformer_B5(**kwargs):
+    return MixVisionTransformer(
+        patch_size=4,
+        embed_dims=[64, 128, 320, 512],
+        num_heads=[1, 2, 5, 8],
+        mlp_ratios=[4, 4, 4, 4],
+        qkv_bias=True,
+        norm_layer=partial(nn.LayerNorm, eps=1e-6),
+        depths=[3, 6, 40, 3],
+        sr_ratios=[8, 4, 2, 1],
+        drop_rate=0.0,
+        drop_path_rate=0.1,
+        **kwargs)
+
+
+BACKBONES = {
+    'MixVisionTransformer_B0': MixVisionTransformer_B0,
+    'MixVisionTransformer_B1': MixVisionTransformer_B1,
+    'MixVisionTransformer_B2': MixVisionTransformer_B2,
+    'MixVisionTransformer_B3': MixVisionTransformer_B3,
+    'MixVisionTransformer_B4': MixVisionTransformer_B4,
+    'MixVisionTransformer_B5': MixVisionTransformer_B5,
+}
\ No newline at end of file
diff --git a/models/model_selection.py b/models/model_selection.py
new file mode 100644
index 0000000..6a5ea01
--- /dev/null
+++ b/models/model_selection.py
@@ -0,0 +1,183 @@
+
+from logging import error
+import smp
+from models.unet import UNet, UNet_dualHeads
+from models.attention_unet import AttentionUNet
+
+from models.siam_unet import SiamUnet_conc, SiamUnet_diff, DualUnet_LF
+from models.unet_cdc import UNet as cdc_unet
+from models.siam_unet_resnet import SiamResUnet
+
+from models.unet_distill import UNet as distill_unet
+from models.segformer import segformer_models
+
+
+model_zoo = {
+    'UNet': UNet,
+    'att_UNet': AttentionUNet,
+    'UNet_dualHeads': UNet_dualHeads,
+}
+
+def get_model(cfg):
+
+    ########################### COMPUTE INPUT & OUTPUT CHANNELS ############################
+    print("Satellites: ", cfg.DATA.SATELLITES)
+    print("NUM_CLASSES:", cfg.MODEL.NUM_CLASSES)
+
+    # cfg.MODEL.NUM_CLASSES = cfg.MODEL.cfg.MODEL.NUM_CLASSES
+
+    INPUT_CHANNELS_DICT = {}
+    INPUT_CHANNELS_LIST = []
+    for sat in cfg.DATA.SATELLITES:
+        INPUT_CHANNELS_DICT[sat] = len(list(cfg.DATA.INPUT_BANDS[sat]))
+        if cfg.DATA.STACKING: INPUT_CHANNELS_DICT[sat] = len(cfg.DATA.PREPOST) * INPUT_CHANNELS_DICT[sat]
+        INPUT_CHANNELS_LIST.append(INPUT_CHANNELS_DICT[sat])
+    
+    ########################### MODEL SELECTION ############################
+    if cfg.MODEL.ARCH in model_zoo.keys():
+        INPUT_CHANNELS = sum(INPUT_CHANNELS_LIST)
+        MODEL = model_zoo[cfg.MODEL.ARCH]
+        return MODEL(INPUT_CHANNELS, 
+                    num_classes=cfg.MODEL.NUM_CLASSES, 
+                    topo=cfg.MODEL.TOPO,
+                    use_deconv=cfg.MODEL.USE_DECONV) #'FC-EF'
+
+    # if cfg.MODEL.ARCH == "UNet":
+    #     INPUT_CHANNELS = sum(INPUT_CHANNELS_LIST)
+    #     model = UNet(INPUT_CHANNELS, 
+    #                     num_classes=cfg.MODEL.NUM_CLASSES, 
+    #                     topo=cfg.MODEL.TOPO,
+    #                     use_deconv=cfg.MODEL.USE_DECONV) #'FC-EF'
+
+
+    if cfg.MODEL.ARCH == 'distill_unet':
+        INPUT_CHANNELS = INPUT_CHANNELS_LIST[0] # defined by the first sensor
+        return UNet(INPUT_CHANNELS, 
+                        num_classes=cfg.MODEL.NUM_CLASSES, 
+                        topo=cfg.MODEL.TOPO, 
+                    ) #'FC-Siam-diff'
+
+    if cfg.MODEL.ARCH in segformer_models.keys():
+        INPUT_CHANNELS = sum(INPUT_CHANNELS_LIST)
+        return segformer_models[cfg.MODEL.ARCH](in_channels=INPUT_CHANNELS, num_classes=cfg.MODEL.NUM_CLASSES)
+    
+    if cfg.MODEL.ARCH in ['SiamUnet_conc', 'SiamUnet_diff', 'DualUnet_LF']:
+        if len(INPUT_CHANNELS_LIST) == 1: # single sensor
+            INPUT_CHANNELS = INPUT_CHANNELS_LIST[0]
+        elif len(INPUT_CHANNELS_LIST) == 2: # two sensors
+            if INPUT_CHANNELS_LIST[0] == INPUT_CHANNELS_LIST[1]: 
+                INPUT_CHANNELS = INPUT_CHANNELS_LIST[0]
+            else:
+                INPUT_CHANNELS = INPUT_CHANNELS_LIST
+                print("INPUT_CHANNELS is a list, pleae fix it!")
+
+        if cfg.MODEL.ARCH == "SiamUnet_conc":
+            return SiamUnet_conc(INPUT_CHANNELS, cfg.MODEL.NUM_CLASSES, \
+                                topo=cfg.MODEL.TOPO, 
+                                share_encoder=cfg.MODEL.SHARE_ENCODER,
+                                use_deconv=cfg.MODEL.USE_DECONV
+                            ) #'FC-Siam-conc'
+
+        if cfg.MODEL.ARCH == "SiamUnet_diff":
+            return SiamUnet_diff(INPUT_CHANNELS, cfg.MODEL.NUM_CLASSES, 
+                                topo=cfg.MODEL.TOPO, 
+                                share_encoder=cfg.MODEL.SHARE_ENCODER,
+                                use_deconv=cfg.MODEL.USE_DECONV
+                            ) #'FC-Siam-diff'
+
+        if cfg.MODEL.ARCH == "DualUnet_LF":
+            return DualUnet_LF(INPUT_CHANNELS, cfg.MODEL.NUM_CLASSES, 
+                                topo=cfg.MODEL.TOPO, 
+                                share_encoder=cfg.MODEL.SHARE_ENCODER,
+                                use_deconv=cfg.MODEL.USE_DECONV
+                            ) #'FC-Siam-diff'
+
+    
+    if cfg.MODEL.ARCH == "SiamUnet_minDiff":
+        return SiamUnet_diff(INPUT_CHANNELS, cfg.MODEL.NUM_CLASSES, topo=cfg.MODEL.TOPO) #'FC-Siam-diff'
+
+    if cfg.MODEL.ARCH == "cdc_unet":
+        return cdc_unet(INPUT_CHANNELS, cfg.MODEL.NUM_CLASSES) #'FC-EF'
+
+    ########################### Residual UNet ############################
+    if cfg.MODEL.ARCH == f'UNet_{cfg.MODEL.ENCODER}':
+        INPUT_CHANNELS = sum(INPUT_CHANNELS_LIST)
+        print(f"===> Network Architecture: {cfg.MODEL.ARCH}")
+        # create segmentation model with pretrained encoder
+
+        return smp.Unet(
+            encoder_name = cfg.MODEL.ENCODER, 
+            encoder_weights = cfg.MODEL.ENCODER_WEIGHTS, 
+            encoder_depth=cfg.MODEL.ENCODER_DEPTH,
+            decoder_channels=cfg.MODEL.TOPO[::-1],
+            # decoder_attention_type="scse",
+            in_channels = INPUT_CHANNELS,
+            classes = cfg.MODEL.NUM_CLASSES, 
+            activation = None,
+        )
+
+    # DeepLabV3+
+    if cfg.MODEL.ARCH == 'DeepLabV3+':
+        print(f"===> Network Architecture: {cfg.MODEL.ARCH}")
+        # create segmentation model with pretrained encoder
+        model = smp.DeepLabV3Plus(
+            encoder_name = cfg.MODEL.ENCODER, 
+            encoder_weights = cfg.MODEL.ENCODER_WEIGHTS, 
+            classes = cfg.MODEL.NUM_CLASSES, 
+            activation = cfg.MODEL.ACTIVATION,
+            in_channels = INPUT_CHANNELS
+        )
+
+    
+    if cfg.MODEL.ARCH == 'SiamResUnet':
+        print(f"===> Network Architecture: {cfg.MODEL.ARCH}")
+        # create segmentation model with pretrained encoder
+
+        input_channels = []
+        for sat in cfg.DATA.SATELLITES:
+            if cfg.DATA.STACKING: tmp = len(cfg.DATA.PREPOST) * INPUT_CHANNELS_DICT[sat]
+            else: tmp = INPUT_CHANNELS_DICT[sat]
+            input_channels.append(tmp)
+
+        from models.siam_unet_resnet import SiamResUnet
+        return SiamResUnet(
+            encoder_name = cfg.MODEL.ENCODER, 
+            encoder_weights = cfg.MODEL.ENCODER_WEIGHTS, 
+            in_channels = input_channels,
+            classes = cfg.MODEL.NUM_CLASSES, 
+            activation = cfg.MODEL.ACTIVATION,
+        )
+
+    # print("==================================")
+    # print(model)
+    # print("==================================")
+    # print("INPUT_CHANNELS: ", INPUT_CHANNELS)
+    # print()
+
+    # return model
+
+
+
+
+
+if __name__ == "__main__":
+    from easydict import EasyDict as edict
+    cfg = edict({
+        "data": {
+            "stacking": True,
+            "CLASSES": ['burned'],
+            'prepost': ['pre', 'post'],
+            "satellites": ['S2'],
+            "INPUT_BANDS": {
+                'S1': ['ND','VH','VV'],
+                'S2': ['B4', 'B8', 'B12'],
+            }
+        },
+        "model": {
+            'ARCH': 'SiamUnet_conc',
+            'TOPO': [16,32,64,128],
+            'SHARE_ENCODER': True
+        }
+        })
+
+    get_model(cfg)
\ No newline at end of file
diff --git a/models/net_arch.py b/models/net_arch.py
deleted file mode 100644
index 2c3ff03..0000000
--- a/models/net_arch.py
+++ /dev/null
@@ -1,59 +0,0 @@
-
-import smp
-def init_model(cfg):
-
-    INPUT_CHANNELS_DICT = {}
-    INPUT_CHANNELS_DICT['S1'] = len(list(cfg.data.S1_INPUT_BANDS))
-    INPUT_CHANNELS_DICT['S2'] = len(list(cfg.data.S2_INPUT_BANDS))
-    INPUT_CHANNELS_DICT['ALOS'] = len(list(cfg.data.ALOS_INPUT_BANDS))
-
-    if cfg.data.stacking and (1==len(cfg.data.satellites)): 
-        INPUT_CHANNELS = len(cfg.data.prepost) * INPUT_CHANNELS_DICT[cfg.data.satellites[0]]
-    
-    # UNet
-    if cfg.model.ARCH == 'UNet':
-        print(f"===> Network Architecture: {cfg.model.ARCH}")
-        # create segmentation model with pretrained encoder
-
-        model = smp.Unet(
-            encoder_name = cfg.model.ENCODER, 
-            encoder_weights = cfg.model.ENCODER_WEIGHTS, 
-            in_channels = INPUT_CHANNELS,
-            classes = len(cfg.data.CLASSES), 
-            activation = cfg.model.ACTIVATION,
-        )
-        return model
-
-    # DeepLabV3+
-    if cfg.model.ARCH == 'DeepLabV3+':
-        print(f"===> Network Architecture: {cfg.model.ARCH}")
-        # create segmentation model with pretrained encoder
-        model = smp.DeepLabV3Plus(
-            encoder_name = cfg.model.ENCODER, 
-            encoder_weights = cfg.model.ENCODER_WEIGHTS, 
-            classes = len(cfg.data.CLASSES), 
-            activation = cfg.model.ACTIVATION,
-            in_channels = INPUT_CHANNELS
-        )
-        return model
-
-    
-    if cfg.model.ARCH == 'FuseUNet':
-        print(f"===> Network Architecture: {cfg.model.ARCH}")
-        # create segmentation model with pretrained encoder
-
-        input_channels = []
-        for sat in cfg.data.satellites:
-            if cfg.data.stacking: tmp = len(cfg.data.prepost) * INPUT_CHANNELS_DICT[sat]
-            else: tmp = INPUT_CHANNELS_DICT[sat]
-            input_channels.append(tmp)
-
-        from models.FuseUNet import FuseUnet
-        model = FuseUnet(
-            encoder_name = cfg.model.ENCODER, 
-            encoder_weights = cfg.model.ENCODER_WEIGHTS, 
-            in_channels = input_channels,
-            classes = len(cfg.data.CLASSES), 
-            activation = cfg.model.ACTIVATION,
-        )
-        return model
\ No newline at end of file
diff --git a/models/segformer.py b/models/segformer.py
new file mode 100644
index 0000000..4217493
--- /dev/null
+++ b/models/segformer.py
@@ -0,0 +1,207 @@
+""" SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers """
+## Official Segformer based on mmcvsegmentation
+# https://arxiv.org/pdf/2105.15203.pdf 
+# https://github.com/NVlabs/SegFormer
+
+## mmcvsegmention 
+# swin.py https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/backbones/swin.py
+# decoder_heads https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/segformer_head.py
+
+## PaddleSeg (Recommended to read first)
+# segformer_head: https://github.com/PaddlePaddle/PaddleSeg/blob/release/2.3/paddleseg/models/segformer.py
+
+
+
+# The SegFormer code was heavily based on https://github.com/NVlabs/SegFormer
+# Users should be careful about adopting these functions in any commercial matters.
+# https://github.com/NVlabs/SegFormer#license
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import numpy as np
+
+from models.mix_transformer import BACKBONES
+
+class MLP(nn.Module):
+    """
+    Linear Embedding
+    """
+
+    def __init__(self, input_dim=2048, embed_dim=768):
+        super().__init__()
+        self.proj = nn.Linear(input_dim, embed_dim)
+
+    def forward(self, x):
+        x = x.flatten(2).permute([0, 2, 1])
+        x = self.proj(x)
+        return x
+
+
+class SegFormer(nn.Module):
+    """
+    The SegFormer implementation based on PaddlePaddle.
+
+    The original article refers to
+    Xie, Enze, et al. "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers." arXiv preprint arXiv:2105.15203 (2021).
+
+    Args:
+        num_classes (int): The unique number of target classes.
+        backbone (Paddle.nn.Module): A backbone network.
+        embedding_dim (int): The MLP decoder channel dimension.
+        align_corners (bool): An argument of F.interpolate. It should be set to False when the output size of feature.
+            is even, e.g. 1024x512, otherwise it is True, e.g. 769x769. Default: False.
+        pretrained (str, optional): The path or url of pretrained model. Default: None.
+    """
+
+    def __init__(self,
+                 num_classes,
+                 backbone,
+                 embedding_dim,
+                 align_corners=False,
+                 pretrained=None):
+        super(SegFormer, self).__init__()
+
+        self.pretrained = pretrained
+        self.align_corners = align_corners
+        self.backbone = backbone
+        self.num_classes = num_classes
+        c1_in_channels, c2_in_channels, c3_in_channels, c4_in_channels = self.backbone.feat_channels
+
+        self.linear_c4 = MLP(input_dim=c4_in_channels, embed_dim=embedding_dim)
+        self.linear_c3 = MLP(input_dim=c3_in_channels, embed_dim=embedding_dim)
+        self.linear_c2 = MLP(input_dim=c2_in_channels, embed_dim=embedding_dim)
+        self.linear_c1 = MLP(input_dim=c1_in_channels, embed_dim=embedding_dim)
+
+        self.dropout = nn.Dropout2d(0.1)
+        self.linear_fuse = nn.intrinsic.ConvBnReLU2d(
+            nn.Conv2d(in_channels=embedding_dim * 4,
+            out_channels=embedding_dim,
+            kernel_size=1,
+            bias=False),
+            nn.BatchNorm2d(embedding_dim),
+            nn.ReLU()
+            )
+
+        self.linear_pred = nn.Conv2d(
+            embedding_dim, self.num_classes, kernel_size=1)
+
+        # self.init_weight()
+
+    # def init_weight(self):
+    #     if self.pretrained is not None:
+    #         utils.load_entire_model(self, self.pretrained)
+
+    def forward(self, x):
+        ''' x should be a list or tuple '''
+        x = torch.cat(x, dim=1) # concat all input tensors, added by puzhao
+
+        feats = self.backbone(x)
+        c1, c2, c3, c4 = feats
+
+        ############## MLP decoder on C1-C4 ###########
+        c1_shape = c1.shape
+        c2_shape = c2.shape
+        c3_shape = c3.shape
+        c4_shape = c4.shape
+
+        c4_ = self.linear_c4(c4).permute([0, 2, 1])
+        _c4 = c4_.reshape(*c4_.shape[:2], c4_shape[2], c4_shape[3])
+        _c4 = F.interpolate(
+            _c4,
+            size=c1_shape[2:],
+            mode='bilinear',
+            align_corners=self.align_corners)
+
+        c3_ = self.linear_c3(c3).permute([0, 2, 1])
+        _c3 = c3_.reshape(*c3_.shape[:2], c3_shape[2], c3_shape[3])
+        _c3 = F.interpolate(
+            _c3,
+            size=c1_shape[2:],
+            mode='bilinear',
+            align_corners=self.align_corners)
+        c2_ = self.linear_c2(c2).permute([0, 2, 1])
+        _c2 = c2_.reshape(*c2_.shape[:2], c2_shape[2], c2_shape[3])
+        _c2 = F.interpolate(
+            _c2,
+            size=c1_shape[2:],
+            mode='bilinear',
+            align_corners=self.align_corners)
+        c1_ = self.linear_c1(c1).permute([0, 2, 1])
+        _c1 = c1_.reshape(*c1_.shape[:2], c1_shape[2], c1_shape[3])
+
+        _c = self.linear_fuse(torch.cat([_c4, _c3, _c2, _c1], axis=1))
+
+        logit = self.dropout(_c)
+        logit = self.linear_pred(logit)
+        return [
+            F.interpolate(
+                logit,
+                size=x.shape[2:],
+                mode='bilinear',
+                align_corners=self.align_corners)
+        ]
+
+
+
+def SegFormer_B0(in_channels=3, **kwargs):
+    return SegFormer(
+        backbone=BACKBONES['MixVisionTransformer_B0'](in_chans=in_channels),
+        embedding_dim=256,
+        **kwargs)
+
+
+def SegFormer_B1(in_channels=3, **kwargs):
+    return SegFormer(
+        backbone=BACKBONES['MixVisionTransformer_B1'](in_chans=in_channels),
+        embedding_dim=256,
+        **kwargs)
+
+
+def SegFormer_B2(in_channels=3, **kwargs):
+    return SegFormer(
+        backbone=BACKBONES['MixVisionTransformer_B2'](in_chans=in_channels),
+        embedding_dim=768,
+        **kwargs)
+
+
+def SegFormer_B3(in_channels=3, **kwargs):
+    return SegFormer(
+        backbone=BACKBONES['MixVisionTransformer_B3'](in_chans=in_channels),
+        embedding_dim=768,
+        **kwargs)
+
+
+def SegFormer_B4(in_channels=3, **kwargs):
+    return SegFormer(
+        backbone=BACKBONES['MixVisionTransformer_B4'](in_chans=in_channels),
+        embedding_dim=768,
+        **kwargs)
+
+
+def SegFormer_B5(in_channels=3, **kwargs):
+    return SegFormer(
+        backbone=BACKBONES['MixVisionTransformer_B5'](in_chans=in_channels),
+        embedding_dim=768,
+        **kwargs)
+
+
+segformer_models = {
+    'SegFormer_B0': SegFormer_B0,
+    'SegFormer_B1': SegFormer_B1,
+    'SegFormer_B2': SegFormer_B2,
+    'SegFormer_B3': SegFormer_B3,
+    'SegFormer_B4': SegFormer_B4,
+    'SegFormer_B5': SegFormer_B5,
+}
+
+
+if __name__ == "__main__":
+    import torch
+    # from segformer import SegFormer_B2
+
+    
+    tensor = torch.randn(1, 3, 256, 256)
+    model = SegFormer_B0(in_channels=tensor.shape[1], num_classes=2)
+    out = model(tensor)
+    print(out[0].shape)
\ No newline at end of file
diff --git a/models/segformer/mix_transformer.py b/models/segformer/mix_transformer.py
new file mode 100644
index 0000000..7768241
--- /dev/null
+++ b/models/segformer/mix_transformer.py
@@ -0,0 +1,627 @@
+# copyright (c) 2021 PaddlePaddle Authors. All Rights Reserve.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import math
+from functools import partial
+import numpy as np
+import os
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.nn.init import zeros_, ones_, trunc_normal_, normal_
+
+
+def to_2tuple(x):
+    return tuple([x] * 2)
+
+def drop_path(x, drop_prob=0., training=False):
+    """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).
+    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
+    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ...
+    """
+    if drop_prob == 0. or not training:
+        return x
+    keep_prob = torch.tensor(1 - drop_prob)
+    shape = (x.shape[0], ) + (1, ) * (x.ndim - 1)
+    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype)
+    random_tensor = torch.floor(random_tensor)  # binarize
+    output = x.divide(keep_prob) * random_tensor
+    return output
+
+class DropPath(nn.Module):
+    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
+    """
+
+    def __init__(self, drop_prob=None):
+        super(DropPath, self).__init__()
+        self.drop_prob = drop_prob
+
+    def forward(self, x):
+        return drop_path(x, self.drop_prob, self.training)
+
+
+class Mlp(nn.Module):
+    def __init__(self,
+                 in_features,
+                 hidden_features=None,
+                 out_features=None,
+                 act_layer=nn.GELU,
+                 drop=0.):
+        super().__init__()
+        out_features = out_features or in_features
+        hidden_features = hidden_features or in_features
+        self.fc1 = nn.Linear(in_features, hidden_features)
+        self.dwconv = DWConv(hidden_features)
+        self.act = act_layer()
+        self.fc2 = nn.Linear(hidden_features, out_features)
+        self.drop = nn.Dropout(drop)
+
+        self.apply(self._init_weights)
+
+    def _init_weights(self, m):
+        if isinstance(m, nn.Linear):
+            trunc_normal_(m.weight, std=0.02)
+            if isinstance(m, nn.Linear) and m.bias is not None:
+                zeros_(m.bias)
+        elif isinstance(m, nn.LayerNorm):
+            zeros_(m.bias)
+            ones_(m.weight)
+        elif isinstance(m, nn.Conv2d):
+            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
+            fan_out //= m.groups
+            normal_(m.weight, 0, math.sqrt(2.0 / fan_out))
+            if m.bias is not None:
+                zeros_(m.bias)
+
+    def forward(self, x, H, W):
+        x = self.fc1(x)
+        x = self.dwconv(x, H, W)
+        x = self.act(x)
+        x = self.drop(x)
+        x = self.fc2(x)
+        x = self.drop(x)
+        return x
+
+
+class Attention(nn.Module):
+    def __init__(self,
+                 dim,
+                 num_heads=8,
+                 qkv_bias=False,
+                 qk_scale=None,
+                 attn_drop=0.,
+                 proj_drop=0.,
+                 sr_ratio=1):
+        super().__init__()
+        assert dim % num_heads == 0, f"dim {dim} should be divided by num_heads {num_heads}."
+
+        self.dim = dim
+        self.num_heads = num_heads
+        head_dim = dim // num_heads
+        self.scale = qk_scale or head_dim**-0.5
+        self.dim = dim
+
+        self.q = nn.Linear(dim, dim, bias=qkv_bias)
+        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)
+        self.attn_drop = nn.Dropout(attn_drop)
+        self.proj = nn.Linear(dim, dim)
+        self.proj_drop = nn.Dropout(proj_drop)
+
+        self.sr_ratio = sr_ratio
+        if sr_ratio > 1:
+            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)
+            self.norm = nn.LayerNorm(dim)
+
+        self.apply(self._init_weights)
+
+    def _init_weights(self, m):
+        if isinstance(m, nn.Linear):
+            trunc_normal_(m.weight, std=0.02)
+            if isinstance(m, nn.Linear) and m.bias is not None:
+                zeros_(m.bias)
+        elif isinstance(m, nn.LayerNorm):
+            zeros_(m.bias)
+            ones_(m.weight)
+        elif isinstance(m, nn.Conv2d):
+            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
+            fan_out //= m.groups
+            normal_(m.weight, 0, math.sqrt(2.0 / fan_out))
+            if m.bias is not None:
+                zeros_(m.bias)
+
+    def forward(self, x, H, W):
+        x_shape = x.shape #paddle.shape(x)
+        B, N = x_shape[0], x_shape[1]
+        C = self.dim
+
+        q = self.q(x).reshape([B, N, self.num_heads,
+                               C // self.num_heads]).transpose(2, 1)
+
+        if self.sr_ratio > 1:
+            x_ = x.permute([0, 2, 1]).reshape([B, C, H, W])
+            x_ = self.sr(x_).reshape([B, C, -1]).transpose(2, 1)
+            x_ = self.norm(x_)
+            kv = self.kv(x_).reshape(
+                [B, -1, 2, self.num_heads,
+                 C // self.num_heads]).permute([2, 0, 3, 1, 4])
+        else:
+            kv = self.kv(x).reshape(
+                [B, -1, 2, self.num_heads,
+                 C // self.num_heads]).permute([2, 0, 3, 1, 4])
+        k, v = kv[0], kv[1]
+
+        attn = (q @ k.permute([0, 1, 3, 2])) * self.scale
+        attn = F.softmax(attn, dim=-1)
+        attn = self.attn_drop(attn)
+
+        x = (attn @ v).permute([0, 2, 1, 3]).reshape([B, N, C])
+        x = self.proj(x)
+        x = self.proj_drop(x)
+
+        return x
+
+
+class Block(nn.Module):
+    def __init__(self,
+                 dim,
+                 num_heads,
+                 mlp_ratio=4.,
+                 qkv_bias=False,
+                 qk_scale=None,
+                 drop=0.,
+                 attn_drop=0.,
+                 drop_path=0.,
+                 act_layer=nn.GELU,
+                 norm_layer=nn.LayerNorm,
+                 sr_ratio=1):
+        super(Block, self).__init__()
+        self.norm1 = norm_layer(dim)
+        self.attn = Attention(
+            dim,
+            num_heads=num_heads,
+            qkv_bias=qkv_bias,
+            qk_scale=qk_scale,
+            attn_drop=attn_drop,
+            proj_drop=drop,
+            sr_ratio=sr_ratio)
+        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
+        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
+        self.norm2 = norm_layer(dim)
+        mlp_hidden_dim = int(dim * mlp_ratio)
+        self.mlp = Mlp(
+            in_features=dim,
+            hidden_features=mlp_hidden_dim,
+            act_layer=act_layer,
+            drop=drop)
+
+        self.apply(self._init_weights)
+
+    def _init_weights(self, m):
+        if isinstance(m, nn.Linear):
+            trunc_normal_(m.weight, std=0.02)
+            if isinstance(m, nn.Linear) and m.bias is not None:
+                zeros_(m.bias)
+        elif isinstance(m, nn.LayerNorm):
+            zeros_(m.bias)
+            ones_(m.weight)
+        elif isinstance(m, nn.Conv2d):
+            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
+            fan_out //= m.groups
+            normal_(m.weight, 0, math.sqrt(2.0 / fan_out))
+            if m.bias is not None:
+                zeros_(m.bias)
+
+    def forward(self, x, H, W):
+        x = x + self.drop_path(self.attn(self.norm1(x), H, W))
+        x = x + self.drop_path(self.mlp(self.norm2(x), H, W))
+
+        return x
+
+
+class OverlapPatchEmbed(nn.Module):
+    """ Image to Patch Embedding
+    """
+
+    def __init__(self,
+                 img_size=224,
+                 patch_size=7,
+                 stride=4,
+                 in_chans=3,
+                 embed_dim=768):
+        super().__init__()
+        img_size = to_2tuple(img_size)
+        patch_size = to_2tuple(patch_size)
+
+        self.img_size = img_size
+        self.patch_size = patch_size
+        self.H, self.W = img_size[0] // patch_size[0], img_size[
+            1] // patch_size[1]
+        self.num_patches = self.H * self.W
+        self.proj = nn.Conv2d(
+            in_chans,
+            embed_dim,
+            kernel_size=patch_size,
+            stride=stride,
+            padding=(patch_size[0] // 2, patch_size[1] // 2))
+        self.norm = nn.LayerNorm(embed_dim)
+
+        self.apply(self._init_weights)
+
+    def _init_weights(self, m):
+        if isinstance(m, nn.Linear):
+            trunc_normal_(m.weight, std=0.02)
+            if isinstance(m, nn.Linear) and m.bias is not None:
+                zeros_(m.bias)
+        elif isinstance(m, nn.LayerNorm):
+            zeros_(m.bias)
+            ones_(m.weight)
+        elif isinstance(m, nn.Conv2d):
+            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
+            fan_out //= m.groups
+            normal_(m.weight, 0, math.sqrt(2.0 / fan_out))
+            if m.bias is not None:
+                zeros_(m.bias)
+
+    def forward(self, x):
+        x = self.proj(x)
+        x_shape = x.shape
+        H, W = x_shape[2], x_shape[3]
+        x = x.flatten(2).transpose(2, 1)
+        x = self.norm(x)
+
+        return x, H, W
+
+
+class MixVisionTransformer(nn.Module):
+    def __init__(self,
+                 img_size=224,
+                 patch_size=16,
+                 in_chans=3,
+                 num_classes=1000,
+                 embed_dims=[64, 128, 256, 512],
+                 num_heads=[1, 2, 4, 8],
+                 mlp_ratios=[4, 4, 4, 4],
+                 qkv_bias=False,
+                 qk_scale=None,
+                 drop_rate=0.,
+                 attn_drop_rate=0.,
+                 drop_path_rate=0.,
+                 norm_layer=nn.LayerNorm,
+                 depths=[3, 4, 6, 3],
+                 sr_ratios=[8, 4, 2, 1],
+                 pretrained=None):
+        super(MixVisionTransformer, self).__init__()
+        self.num_classes = num_classes
+        self.depths = depths
+        self.feat_channels = embed_dims[:]
+
+        # patch_embed
+        self.patch_embed1 = OverlapPatchEmbed(
+            img_size=img_size,
+            patch_size=7,
+            stride=4,
+            in_chans=in_chans,
+            embed_dim=embed_dims[0])
+        self.patch_embed2 = OverlapPatchEmbed(
+            img_size=img_size // 4,
+            patch_size=3,
+            stride=2,
+            in_chans=embed_dims[0],
+            embed_dim=embed_dims[1])
+        self.patch_embed3 = OverlapPatchEmbed(
+            img_size=img_size // 8,
+            patch_size=3,
+            stride=2,
+            in_chans=embed_dims[1],
+            embed_dim=embed_dims[2])
+        self.patch_embed4 = OverlapPatchEmbed(
+            img_size=img_size // 16,
+            patch_size=3,
+            stride=2,
+            in_chans=embed_dims[2],
+            embed_dim=embed_dims[3])
+
+        # transformer encoder
+        dpr = np.linspace(0, drop_path_rate, sum(depths))
+        # dpr = [
+        #     x.numpy() for x in paddle.linspace(0, drop_path_rate, sum(depths))
+        # ]  # stochastic depth decay rule
+        cur = 0
+        self.block1 = nn.Sequential(
+            *[Block(
+                dim=embed_dims[0],
+                num_heads=num_heads[0],
+                mlp_ratio=mlp_ratios[0],
+                qkv_bias=qkv_bias,
+                qk_scale=qk_scale,
+                drop=drop_rate,
+                attn_drop=attn_drop_rate,
+                drop_path=dpr[cur + i],
+                norm_layer=norm_layer,
+                sr_ratio=sr_ratios[0]) for i in range(depths[0])]
+        )
+        self.norm1 = norm_layer(embed_dims[0])
+
+        cur += depths[0]
+        self.block2 = nn.Sequential(*[
+            Block(
+                dim=embed_dims[1],
+                num_heads=num_heads[1],
+                mlp_ratio=mlp_ratios[1],
+                qkv_bias=qkv_bias,
+                qk_scale=qk_scale,
+                drop=drop_rate,
+                attn_drop=attn_drop_rate,
+                drop_path=dpr[cur + i],
+                norm_layer=norm_layer,
+                sr_ratio=sr_ratios[1]) for i in range(depths[1])
+        ])
+        self.norm2 = norm_layer(embed_dims[1])
+
+        cur += depths[1]
+        self.block3 = nn.Sequential(*[
+            Block(
+                dim=embed_dims[2],
+                num_heads=num_heads[2],
+                mlp_ratio=mlp_ratios[2],
+                qkv_bias=qkv_bias,
+                qk_scale=qk_scale,
+                drop=drop_rate,
+                attn_drop=attn_drop_rate,
+                drop_path=dpr[cur + i],
+                norm_layer=norm_layer,
+                sr_ratio=sr_ratios[2]) for i in range(depths[2])
+        ])
+        self.norm3 = norm_layer(embed_dims[2])
+
+        cur += depths[2]
+        self.block4 = nn.Sequential(*[
+            Block(
+                dim=embed_dims[3],
+                num_heads=num_heads[3],
+                mlp_ratio=mlp_ratios[3],
+                qkv_bias=qkv_bias,
+                qk_scale=qk_scale,
+                drop=drop_rate,
+                attn_drop=attn_drop_rate,
+                drop_path=dpr[cur + i],
+                norm_layer=norm_layer,
+                sr_ratio=sr_ratios[3]) for i in range(depths[3])
+        ])
+        self.norm4 = norm_layer(embed_dims[3])
+
+        self.pretrained = pretrained
+        self.init_weight()
+
+    def init_weight(self):
+        # if self.pretrained is not None:
+        #     load_pretrained_model(self, self.pretrained)
+        # else:
+        self.apply(self._init_weights)
+
+    def _init_weights(self, m):
+        if isinstance(m, nn.Linear):
+            trunc_normal_(m.weight, std=0.02)
+            if isinstance(m, nn.Linear) and m.bias is not None:
+                zeros_(m.bias)
+        elif isinstance(m, nn.LayerNorm):
+            zeros_(m.bias)
+            ones_(m.weight)
+        elif isinstance(m, nn.Conv2d):
+            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
+            fan_out //= m.groups
+            normal_(m.weight, 0, math.sqrt(2.0 / fan_out))
+            if m.bias is not None:
+                zeros_(m.bias)
+
+    def reset_drop_path(self, drop_path_rate):
+        # dpr = [
+        #     x.item()
+        #     for x in paddle.linspace(0, drop_path_rate, sum(self.depths))
+        # ]
+        dpr = np.linspace(0, drop_path_rate, sum(self.depths))
+        cur = 0
+        for i in range(self.depths[0]):
+            self.block1[i].drop_path.drop_prob = dpr[cur + i]
+
+        cur += self.depths[0]
+        for i in range(self.depths[1]):
+            self.block2[i].drop_path.drop_prob = dpr[cur + i]
+
+        cur += self.depths[1]
+        for i in range(self.depths[2]):
+            self.block3[i].drop_path.drop_prob = dpr[cur + i]
+
+        cur += self.depths[2]
+        for i in range(self.depths[3]):
+            self.block4[i].drop_path.drop_prob = dpr[cur + i]
+
+    def freeze_patch_emb(self):
+        self.patch_embed1.requires_grad = False
+
+    def get_classifier(self):
+        return self.head
+
+    def reset_classifier(self, num_classes, global_pool=''):
+        self.num_classes = num_classes
+        self.head = nn.Linear(
+            self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()
+
+    def forward_features(self, x):
+        B = x.shape[0]
+        outs = []
+
+        # stage 1
+        x, H, W = self.patch_embed1(x)
+        for i, blk in enumerate(self.block1):
+            x = blk(x, H, W)
+
+        x = self.norm1(x)
+        x = x.reshape([B, H, W, self.feat_channels[0]]).permute([0, 3, 1, 2])
+        outs.append(x)
+
+        # stage 2
+        x, H, W = self.patch_embed2(x)
+        for i, blk in enumerate(self.block2):
+            x = blk(x, H, W)
+        x = self.norm2(x)
+        x = x.reshape([B, H, W, self.feat_channels[1]]).permute([0, 3, 1, 2])
+        outs.append(x)
+
+        # stage 3
+        x, H, W = self.patch_embed3(x)
+        for i, blk in enumerate(self.block3):
+            x = blk(x, H, W)
+        x = self.norm3(x)
+        x = x.reshape([B, H, W, self.feat_channels[2]]).permute([0, 3, 1, 2])
+        outs.append(x)
+
+        # stage 4
+        x, H, W = self.patch_embed4(x)
+        for i, blk in enumerate(self.block4):
+            x = blk(x, H, W)
+        x = self.norm4(x)
+        x = x.reshape([B, H, W, self.feat_channels[3]]).permute([0, 3, 1, 2])
+        outs.append(x)
+
+        return outs
+
+    def forward(self, x):
+        x = self.forward_features(x)
+        # x = self.head(x)
+
+        return x
+
+
+class DWConv(nn.Module):
+    def __init__(self, dim=768):
+        super(DWConv, self).__init__()
+        self.dim = dim
+        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)
+
+    def forward(self, x, H, W):
+        x_shape = x.shape
+        B, N = x_shape[0], x_shape[1]
+        x = x.permute([0, 2, 1]).reshape([B, self.dim, H, W])
+        x = self.dwconv(x)
+        x = x.flatten(2).permute([0, 2, 1])
+
+        return x
+
+
+
+def MixVisionTransformer_B0(**kwargs):
+    return MixVisionTransformer(
+        patch_size=4,
+        embed_dims=[32, 64, 160, 256],
+        num_heads=[1, 2, 5, 8],
+        mlp_ratios=[4, 4, 4, 4],
+        qkv_bias=True,
+        norm_layer=partial(nn.LayerNorm, eps=1e-6),
+        depths=[2, 2, 2, 2],
+        sr_ratios=[8, 4, 2, 1],
+        drop_rate=0.0,
+        drop_path_rate=0.1,
+        **kwargs)
+
+
+
+def MixVisionTransformer_B1(**kwargs):
+    return MixVisionTransformer(
+        patch_size=4,
+        embed_dims=[64, 128, 320, 512],
+        num_heads=[1, 2, 5, 8],
+        mlp_ratios=[4, 4, 4, 4],
+        qkv_bias=True,
+        norm_layer=partial(nn.LayerNorm, eps=1e-6),
+        depths=[2, 2, 2, 2],
+        sr_ratios=[8, 4, 2, 1],
+        drop_rate=0.0,
+        drop_path_rate=0.1,
+        **kwargs)
+
+
+
+def MixVisionTransformer_B2(**kwargs):
+    return MixVisionTransformer(
+        patch_size=4,
+        embed_dims=[64, 128, 320, 512],
+        num_heads=[1, 2, 5, 8],
+        mlp_ratios=[4, 4, 4, 4],
+        qkv_bias=True,
+        norm_layer=partial(nn.LayerNorm, eps=1e-6),
+        depths=[3, 4, 6, 3],
+        sr_ratios=[8, 4, 2, 1],
+        drop_rate=0.0,
+        drop_path_rate=0.1,
+        **kwargs)
+
+
+
+def MixVisionTransformer_B3(**kwargs):
+    return MixVisionTransformer(
+        patch_size=4,
+        embed_dims=[64, 128, 320, 512],
+        num_heads=[1, 2, 5, 8],
+        mlp_ratios=[4, 4, 4, 4],
+        qkv_bias=True,
+        norm_layer=partial(nn.LayerNorm, eps=1e-6),
+        depths=[3, 4, 18, 3],
+        sr_ratios=[8, 4, 2, 1],
+        drop_rate=0.0,
+        drop_path_rate=0.1,
+        **kwargs)
+
+
+
+def MixVisionTransformer_B4(**kwargs):
+    return MixVisionTransformer(
+        patch_size=4,
+        embed_dims=[64, 128, 320, 512],
+        num_heads=[1, 2, 5, 8],
+        mlp_ratios=[4, 4, 4, 4],
+        qkv_bias=True,
+        norm_layer=partial(nn.LayerNorm, eps=1e-6),
+        depths=[3, 8, 27, 3],
+        sr_ratios=[8, 4, 2, 1],
+        drop_rate=0.0,
+        drop_path_rate=0.1,
+        **kwargs)
+
+
+
+def MixVisionTransformer_B5(**kwargs):
+    return MixVisionTransformer(
+        patch_size=4,
+        embed_dims=[64, 128, 320, 512],
+        num_heads=[1, 2, 5, 8],
+        mlp_ratios=[4, 4, 4, 4],
+        qkv_bias=True,
+        norm_layer=partial(nn.LayerNorm, eps=1e-6),
+        depths=[3, 6, 40, 3],
+        sr_ratios=[8, 4, 2, 1],
+        drop_rate=0.0,
+        drop_path_rate=0.1,
+        **kwargs)
+
+
+BACKBONES = {
+    'MixVisionTransformer_B0': MixVisionTransformer_B0,
+    'MixVisionTransformer_B1': MixVisionTransformer_B1,
+    'MixVisionTransformer_B2': MixVisionTransformer_B2,
+    'MixVisionTransformer_B3': MixVisionTransformer_B3,
+    'MixVisionTransformer_B4': MixVisionTransformer_B4,
+    'MixVisionTransformer_B5': MixVisionTransformer_B5,
+}
\ No newline at end of file
diff --git a/models/segformer/segformer.py b/models/segformer/segformer.py
new file mode 100644
index 0000000..7d5767e
--- /dev/null
+++ b/models/segformer/segformer.py
@@ -0,0 +1,168 @@
+# The SegFormer code was heavily based on https://github.com/NVlabs/SegFormer
+# Users should be careful about adopting these functions in any commercial matters.
+# https://github.com/NVlabs/SegFormer#license
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import numpy as np
+
+from mix_transformer import BACKBONES
+
+class MLP(nn.Module):
+    """
+    Linear Embedding
+    """
+
+    def __init__(self, input_dim=2048, embed_dim=768):
+        super().__init__()
+        self.proj = nn.Linear(input_dim, embed_dim)
+
+    def forward(self, x):
+        x = x.flatten(2).permute([0, 2, 1])
+        x = self.proj(x)
+        return x
+
+
+class SegFormer(nn.Module):
+    """
+    The SegFormer implementation based on PaddlePaddle.
+
+    The original article refers to
+    Xie, Enze, et al. "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers." arXiv preprint arXiv:2105.15203 (2021).
+
+    Args:
+        num_classes (int): The unique number of target classes.
+        backbone (Paddle.nn.Module): A backbone network.
+        embedding_dim (int): The MLP decoder channel dimension.
+        align_corners (bool): An argument of F.interpolate. It should be set to False when the output size of feature.
+            is even, e.g. 1024x512, otherwise it is True, e.g. 769x769. Default: False.
+        pretrained (str, optional): The path or url of pretrained model. Default: None.
+    """
+
+    def __init__(self,
+                 num_classes,
+                 backbone,
+                 embedding_dim,
+                 align_corners=False,
+                 pretrained=None):
+        super(SegFormer, self).__init__()
+
+        self.pretrained = pretrained
+        self.align_corners = align_corners
+        self.backbone = backbone
+        self.num_classes = num_classes
+        c1_in_channels, c2_in_channels, c3_in_channels, c4_in_channels = self.backbone.feat_channels
+
+        self.linear_c4 = MLP(input_dim=c4_in_channels, embed_dim=embedding_dim)
+        self.linear_c3 = MLP(input_dim=c3_in_channels, embed_dim=embedding_dim)
+        self.linear_c2 = MLP(input_dim=c2_in_channels, embed_dim=embedding_dim)
+        self.linear_c1 = MLP(input_dim=c1_in_channels, embed_dim=embedding_dim)
+
+        self.dropout = nn.Dropout2d(0.1)
+        self.linear_fuse = nn.intrinsic.ConvBnReLU2d(
+            nn.Conv2d(in_channels=embedding_dim * 4,
+            out_channels=embedding_dim,
+            kernel_size=1,
+            bias=False),
+            nn.BatchNorm2d(embedding_dim),
+            nn.ReLU()
+            )
+
+        self.linear_pred = nn.Conv2d(
+            embedding_dim, self.num_classes, kernel_size=1)
+
+        # self.init_weight()
+
+    # def init_weight(self):
+    #     if self.pretrained is not None:
+    #         utils.load_entire_model(self, self.pretrained)
+
+    def forward(self, x):
+        feats = self.backbone(x)
+        c1, c2, c3, c4 = feats
+
+        ############## MLP decoder on C1-C4 ###########
+        c1_shape = c1.shape
+        c2_shape = c2.shape
+        c3_shape = c3.shape
+        c4_shape = c4.shape
+
+        c4_ = self.linear_c4(c4).permute([0, 2, 1])
+        _c4 = c4_.reshape(*c4_.shape[:2], c4_shape[2], c4_shape[3])
+        _c4 = F.interpolate(
+            _c4,
+            size=c1_shape[2:],
+            mode='bilinear',
+            align_corners=self.align_corners)
+
+        c3_ = self.linear_c3(c3).permute([0, 2, 1])
+        _c3 = c3_.reshape(*c3_.shape[:2], c3_shape[2], c3_shape[3])
+        _c3 = F.interpolate(
+            _c3,
+            size=c1_shape[2:],
+            mode='bilinear',
+            align_corners=self.align_corners)
+        c2_ = self.linear_c2(c2).permute([0, 2, 1])
+        _c2 = c2_.reshape(*c2_.shape[:2], c2_shape[2], c2_shape[3])
+        _c2 = F.interpolate(
+            _c2,
+            size=c1_shape[2:],
+            mode='bilinear',
+            align_corners=self.align_corners)
+        c1_ = self.linear_c1(c1).permute([0, 2, 1])
+        _c1 = c1_.reshape(*c2_.shape[:2], c1_shape[2], c1_shape[3])
+
+        _c = self.linear_fuse(torch.cat([_c4, _c3, _c2, _c1], axis=1))
+
+        logit = self.dropout(_c)
+        logit = self.linear_pred(logit)
+        return [
+            F.interpolate(
+                logit,
+                size=x.shape[2:],
+                mode='bilinear',
+                align_corners=self.align_corners)
+        ]
+
+
+def SegFormer_B0(**kwargs):
+    return SegFormer(
+        backbone=BACKBONES['MixVisionTransformer_B0'](),
+        embedding_dim=256,
+        **kwargs)
+
+
+def SegFormer_B1(**kwargs):
+    return SegFormer(
+        backbone=BACKBONES['MixVisionTransformer_B1'](),
+        embedding_dim=256,
+        **kwargs)
+
+
+def SegFormer_B2(**kwargs):
+    return SegFormer(
+        backbone=BACKBONES['MixVisionTransformer_B2'](),
+        embedding_dim=768,
+        **kwargs)
+
+
+def SegFormer_B3(**kwargs):
+    return SegFormer(
+        backbone=BACKBONES['MixVisionTransformer_B3'](),
+        embedding_dim=768,
+        **kwargs)
+
+
+def SegFormer_B4(**kwargs):
+    return SegFormer(
+        backbone=BACKBONES['MixVisionTransformer_B4'](),
+        embedding_dim=768,
+        **kwargs)
+
+
+def SegFormer_B5(**kwargs):
+    return SegFormer(
+        backbone=BACKBONES['MixVisionTransformer_B5'](),
+        embedding_dim=768,
+        **kwargs)
diff --git a/models/segformer/test.py b/models/segformer/test.py
new file mode 100644
index 0000000..1baf42e
--- /dev/null
+++ b/models/segformer/test.py
@@ -0,0 +1,7 @@
+import torch
+from segformer import SegFormer_B0
+
+model = SegFormer_B0(num_classes=2)
+tensor = torch.randn(1, 3, 256, 256)
+out = model(tensor)
+print(out[0].shape)
\ No newline at end of file
diff --git a/models/siam_unet.py b/models/siam_unet.py
new file mode 100644
index 0000000..1f5ad7b
--- /dev/null
+++ b/models/siam_unet.py
@@ -0,0 +1,420 @@
+# https://sourcegraph.com/github.com/PaddlePaddle/PaddleSeg/-/blob/paddleseg/models/unet.py?L155:16#tab=def
+
+# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# import paddle
+# import paddle.nn as nn
+# import paddle.nn.functional as F
+
+# from paddleseg import utils
+# from paddleseg.cvlibs import manager
+# from paddleseg.models import layers
+
+import os
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+
+# @manager.MODELS.add_component
+class SiamUnet_conc(nn.Module):
+    """
+    The UNet implementation based on PaddlePaddle.
+
+    The original article refers to
+    Olaf Ronneberger, et, al. "U-Net: Convolutional Networks for Biomedical Image Segmentation"
+    (https://arxiv.org/abs/1505.04597).
+
+    Args:
+        num_classes (int): The unique number of target classes.
+        align_corners (bool): An argument of F.interpolate. It should be set to False when the output size of feature
+            is even, e.g. 1024x512, otherwise it is True, e.g. 769x769.  Default: False.
+        use_deconv (bool, optional): A bool value indicates whether using deconvolution in upsampling.
+            If False, use resize_bilinear. Default: False.
+        pretrained (str, optional): The path or url of pretrained model for fine tuning. Default: None.
+    """
+
+    def __init__(self,
+                 input_channels=6,
+                 num_classes=1,
+                #  topo=[64, 128, 256, 512],
+                #  topo=[32, 64, 128, 256],
+                 topo=[16, 32, 64, 128],
+                 align_corners=False,
+                 use_deconv=False,
+                 pretrained=None,
+                 share_encoder=True):
+        super().__init__()
+
+        self.share_encoder = share_encoder
+        self.encode = Encoder(input_channels, topo=topo)
+        if not self.share_encoder:
+            self.encode_2 = Encoder(input_channels, topo=topo)
+
+        decoder_topo = topo[::-1]
+        # decoder_topo = [3*de_topo for de_topo in decoder_topo]
+        self.decode = Decoder(align_corners, use_deconv=use_deconv, topo=decoder_topo, multiplyer=3)
+        self.cls = self.conv = nn.Conv2d(
+            in_channels=topo[0], # *3 only for SiamUnet_conc
+            out_channels=num_classes,
+            kernel_size=3,
+            stride=1,
+            padding=1)
+
+        # self.softmax = nn.Softmax(dim=1)
+        # self.sigmoid = nn.Sigmoid()
+
+        # self.pretrained = pretrained
+        # self.init_weight()
+
+        print(f"Encoder TOPO: ", topo)
+        # print(f"Decoder TOPO: ", decoder_topo)
+
+    def forward(self, x):
+        ''' x1, x2 should come from different sensor, or different time '''
+        x1, x2 = x
+
+        logit_list = []
+
+        if not self.share_encoder: encoder_2 = self.encode_2
+        else: encoder_2 = self.encode
+
+        _, short_cuts1 = self.encode(x1)
+        x, short_cuts2 = encoder_2(x2)
+        # print(x.shape)
+
+        # xc = torch.cat((xc, short_cuts1[-1], short_cuts2[-1]), dim=1)
+        short_cut = []
+        for cut1, cut2 in zip(short_cuts1, short_cuts2):
+            stacked_feat = torch.cat((cut1, cut2), dim=1)
+            short_cut.append(stacked_feat)
+
+        x = self.decode(x, short_cut)
+        # print(x.shape)
+        x = self.cls(x) # output
+        logit_list.append(x)
+
+        # logit = nn.LogSoftmax(dim=1)(x)
+        # logit_list.append(logit)
+        return logit_list
+
+
+class DualUnet_LF(nn.Module):
+    """
+    The Dual UNet with late fusion (LF).
+    Args:
+        num_classes (int): The unique number of target classes.
+        align_corners (bool): An argument of F.interpolate. It should be set to False when the output size of feature
+            is even, e.g. 1024x512, otherwise it is True, e.g. 769x769.  Default: False.
+        use_deconv (bool, optional): A bool value indicates whether using deconvolution in upsampling.
+            If False, use resize_bilinear. Default: False.
+        pretrained (str, optional): The path or url of pretrained model for fine tuning. Default: None.
+    """
+
+    def __init__(self,
+                 input_channels=6,
+                 num_classes=1,
+                #  topo=[64, 128, 256, 512],
+                #  topo=[32, 64, 128, 256],
+                 topo=[16, 32, 64, 128],
+                 align_corners=False,
+                 use_deconv=False,
+                 pretrained=None,
+                 share_encoder=True):
+        super().__init__()
+
+        self.share_encoder = share_encoder
+        self.encode1 = Encoder(input_channels, topo=topo)
+        if not self.share_encoder:
+            self.encode2 = Encoder(input_channels, topo=topo)
+        else:
+            self.encode2 = self.encode1
+
+        decoder_topo = topo[::-1]
+        # decoder_topo = [3*de_topo for de_topo in decoder_topo]
+
+        self.decode1 = Decoder(align_corners, use_deconv=use_deconv, topo=decoder_topo, multiplyer=2)
+        if not self.share_encoder:
+            self.decode2 = Decoder(align_corners, use_deconv=use_deconv, topo=decoder_topo, multiplyer=2)
+        else:
+            self.decode2 = self.decode1
+
+        self.cls = self.conv = nn.Conv2d(
+            in_channels=topo[0] * 2, # *3 only for SiamUnet_conc
+            out_channels=num_classes,
+            kernel_size=3,
+            stride=1,
+            padding=1)
+
+        # self.pretrained = pretrained
+        # self.init_weight()
+
+        print(f"Encoder TOPO: ", topo)
+        # print(f"Decoder TOPO: ", decoder_topo)
+
+    def forward(self, x):
+        ''' x1, x2 should come from different sensor, or different time '''
+        x1, x2 = x
+
+        logit_list = []
+
+        xc1, short_cuts1 = self.encode1(x1)
+        x1 = self.decode1(xc1, short_cuts1)
+
+        xc2, short_cuts2 = self.encode2(x2)
+        x2 = self.decode2(xc2, short_cuts2)
+
+        x = torch.cat([x1, x2], dim=1)
+        x = self.cls(x) # output
+        logit_list.append(x)
+
+        # logit = nn.LogSoftmax(dim=1)(x)
+        # logit_list.append(logit)
+        return logit_list
+
+
+class SiamUnet_diff(nn.Module):
+    """
+    The UNet implementation based on PaddlePaddle.
+
+    The original article refers to
+    Olaf Ronneberger, et, al. "U-Net: Convolutional Networks for Biomedical Image Segmentation"
+    (https://arxiv.org/abs/1505.04597).
+
+    Args:
+        num_classes (int): The unique number of target classes.
+        align_corners (bool): An argument of F.interpolate. It should be set to False when the output size of feature
+            is even, e.g. 1024x512, otherwise it is True, e.g. 769x769.  Default: False.
+        use_deconv (bool, optional): A bool value indicates whether using deconvolution in upsampling.
+            If False, use resize_bilinear. Default: False.
+        pretrained (str, optional): The path or url of pretrained model for fine tuning. Default: None.
+    """
+
+    def __init__(self,
+                 input_channels=6,
+                 num_classes=1,
+                #  topo=[64, 128, 256, 512],
+                #  topo=[32, 64, 128, 256],
+                 topo=[16, 32, 64, 128],
+                 align_corners=False,
+                 use_deconv=False,
+                 pretrained=None,
+                 share_encoder=True):
+        super().__init__()
+
+        self.share_encoder = share_encoder
+        self.encode = Encoder(input_channels, topo=topo)
+        if not self.share_encoder:
+            self.encode_2 = Encoder(input_channels, topo=topo)
+
+        decoder_topo = topo[::-1]
+        # decoder_topo = [3*de_topo for de_topo in decoder_topo]
+        self.decode = Decoder(align_corners, use_deconv=use_deconv, topo=decoder_topo)
+        self.cls = self.conv = nn.Conv2d(
+            in_channels=topo[0], # *3 only for SiamUnet_conc
+            out_channels=num_classes,
+            kernel_size=3,
+            stride=1,
+            padding=1)
+
+        # self.pretrained = pretrained
+        # self.init_weight()
+
+    def forward(self, x, train=True):
+        # x = torch.cat((x1, x2), dim=1)
+        # x = torch.cat(x, dim=1)
+        x1, x2 = x
+
+        logit_list = []
+
+        if not self.share_encoder: encoder_2 = self.encode_2
+        else: encoder_2 = self.encode
+
+        _, short_cuts1 = self.encode(x1)
+        x, short_cuts2 = encoder_2(x2)
+        # print(x.shape)
+
+        # xc = torch.cat((xc, short_cuts1[-1], short_cuts2[-1]), dim=1)
+        short_cut = []
+        for cut1, cut2 in zip(short_cuts1, short_cuts2):
+            # stacked_feat = torch.cat((cut1, cut2), dim=1)
+            diff_feat = torch.subtract(cut1, cut2)
+            diff_feat = torch.abs(diff_feat)
+            short_cut.append(diff_feat)
+
+        # print("--------------------")
+        x = self.decode(x, short_cut)
+        # print(x.shape)
+        x = self.cls(x) # output
+        logit_list.append(x)
+
+        return logit_list
+
+
+class Encoder(nn.Module):
+    def __init__(self, input_channels=6, topo=[16, 32, 64, 128]):
+        super().__init__()
+
+        self.double_conv = nn.Sequential(
+            ConvBNReLU(input_channels, topo[0], 3), ConvBNReLU(topo[0], topo[0], 3))
+
+        down_channels = []
+        for i in range(0, len(topo)):
+            if i < len(topo) - 1: down_channels.append([topo[i], topo[i+1]])
+            else: down_channels.append([topo[i], topo[i]])
+        # print(down_channels)
+        # down_channels = [[64, 128], [128, 256], [256, 512], [512, 512]]
+
+        self.downLayerList = []
+        for channel in down_channels:
+            self.downLayerList.append(self.down_sampling(channel[0], channel[1]))
+        self.down_stages = nn.Sequential(*self.downLayerList)
+        
+
+    def down_sampling(self, in_channels, out_channels):
+        modules = []
+        modules.append(nn.MaxPool2d(kernel_size=2, stride=2))
+        modules.append(ConvBNReLU(in_channels, out_channels, 3))
+        modules.append(ConvBNReLU(out_channels, out_channels, 3))
+        return nn.Sequential(*modules)
+
+    def forward(self, x):
+        short_cuts = []
+
+        # print("---- Encoder ----")
+        x = self.double_conv(x)
+        for down_sample in self.downLayerList:
+            # print(x.shape)
+            short_cuts.append(x)
+            x = down_sample(x)
+        return x, short_cuts
+
+
+class Decoder(nn.Module):
+    def __init__(self, align_corners, use_deconv=False, topo=[16,32,64,128], multiplyer=2):
+        super().__init__()
+
+        decoder_topo = topo
+        # up_channels = [[512, 256], [256, 128], [128, 64], [64, 64]]
+        up_channels = []
+        for i in range(0, len(decoder_topo)):
+            if i < len(decoder_topo) - 1: up_channels.append([decoder_topo[i], decoder_topo[i+1]])
+            else: up_channels.append([decoder_topo[i], decoder_topo[i]])
+        # print(up_channels)
+
+        self.upLayerList = [
+            UpSampling(channel[0], channel[1], align_corners, use_deconv, multiplyer)
+            for channel in up_channels
+        ]
+
+        self.up_stages = nn.Sequential(*self.upLayerList)
+
+    def forward(self, x, short_cuts):
+        # print("---- UNet Dncoder ----")
+        for i in range(len(short_cuts)):
+            # print(x.shape)
+            x = self.up_stages[i](x, short_cuts[-(i + 1)])
+            # print(x.shape)
+
+        return x
+
+
+class UpSampling(nn.Module):
+    def __init__(self,
+                 in_channels,
+                 out_channels,
+                 align_corners,
+                 use_deconv=False,
+                 multiplyer=2):
+        super().__init__()
+
+        self.align_corners = align_corners
+
+        self.use_deconv = use_deconv
+        if self.use_deconv:
+            self.deconv = nn.ConvTranspose2d(
+                in_channels,
+                out_channels // 2,
+                kernel_size=2,
+                stride=2,
+                padding=0)
+            in_channels = in_channels + out_channels // 2
+        else:
+            in_channels *= multiplyer # 2 -> 3 for FC-Siam-conc
+
+        self.double_conv = nn.Sequential(
+            ConvBNReLU(in_channels, out_channels, 3),
+            ConvBNReLU(out_channels, out_channels, 3))
+
+    def forward(self, x, short_cut):
+        if self.use_deconv:
+            x = self.deconv(x)
+        else:
+            x = F.interpolate(
+                x,
+                (short_cut.shape)[2:],
+                mode='bilinear',
+                align_corners=self.align_corners)
+        x = torch.cat([x, short_cut], dim=1)
+        # print(x.shape)
+        x = self.double_conv(x)
+        return x
+
+
+# https://sourcegraph.com/github.com/PaddlePaddle/PaddleSeg/-/blob/paddleseg/models/layers/layer_libs.py?L98
+
+class ConvBNReLU(nn.Module):
+    def __init__(self,
+                 in_channels,
+                 out_channels,
+                 kernel_size,
+                 padding='same',
+                 **kwargs):
+        super().__init__()
+
+        self._conv = nn.Conv2d(
+            # in_channels, out_channels, kernel_size, **kwargs)
+            in_channels, out_channels, kernel_size, padding=1, **kwargs)
+
+        self._batch_norm = nn.BatchNorm2d(out_channels)
+        self._relu = nn.ReLU()
+
+    def forward(self, x):
+        x = self._conv(x)
+        x = self._batch_norm(x)
+        x = self._relu(x)
+        return x
+
+
+
+if __name__ == "__main__":
+
+    import numpy as np
+    from torchsummary import summary
+
+    x1 = np.random.rand(10,6,256,256)
+    x2 = np.random.rand(10,6,256,256)
+    x1 = torch.from_numpy(x1).type(torch.FloatTensor)#.cuda().type(torch.cuda.FloatTensor)
+    x2 = torch.from_numpy(x2).type(torch.FloatTensor)#.cuda().type(torch.cuda.FloatTensor)
+
+    myunet = DualUnet_LF(input_channels=6, share_encoder=True)
+    # myunet1 = DualUnet_LF(input_channels=6, share_encoder=True)
+    # myunet.cuda()
+
+    # print(myunet)
+
+    out = myunet.forward((x1, x2))[-1]
+    print(out.shape)
+    # summary(myunet, (3,256,256))
\ No newline at end of file
diff --git a/models/FuseUNet.py b/models/siam_unet_resnet.py
similarity index 95%
rename from models/FuseUNet.py
rename to models/siam_unet_resnet.py
index 134819b..2425f83 100644
--- a/models/FuseUNet.py
+++ b/models/siam_unet_resnet.py
@@ -9,7 +9,6 @@ from easydict import EasyDict as edict
 import torch
 from smp.base import initialization as init
 
-
 class SegmentationModel(torch.nn.Module):
 
     def initialize(self):
@@ -34,8 +33,8 @@ class SegmentationModel(torch.nn.Module):
         decoder2 = self.decoder2(*features2)
         decoder2 = self.segmentation_head2(decoder2)
 
-        decoder_output = torch.cat([decoder1, decoder2], dim=1)
-        masks = self.segmentation_head(decoder_output)
+        # decoder_output = torch.cat([decoder1, decoder2], dim=1)
+        masks = self.segmentation_head(decoder1)
 
         # if self.classification_head is not None:
         #     labels = self.classification_head(features[-1])
@@ -63,7 +62,7 @@ class SegmentationModel(torch.nn.Module):
 
 
 
-class FuseUnet(SegmentationModel):
+class SiamResUnet(SegmentationModel):
     """Unet_ is a fully convolution neural network for image semantic segmentation. Consist of *encoder* 
     and *decoder* parts connected with *skip connections*. Encoder extract features of different spatial 
     resolution (skip connections) which are used by decoder to define accurate segmentation mask. Use *concatenation*
@@ -149,7 +148,9 @@ class FuseUnet(SegmentationModel):
         self.encoder2, self.decoder2, self.segmentation_head2= get_UNet(in_channels[1])
         
         self.segmentation_head = SegmentationHead(
-                in_channels=classes*len(in_channels), # puzhao
+                # in_channels=classes*len(in_channels), # puzhao
+                # out_channels=classes,
+                in_channels=len(in_channels), # puzhao
                 out_channels=classes,
                 activation=activation,
                 kernel_size=3,
diff --git a/models/unet.py b/models/unet.py
new file mode 100644
index 0000000..cd6aecf
--- /dev/null
+++ b/models/unet.py
@@ -0,0 +1,327 @@
+# https://sourcegraph.com/github.com/PaddlePaddle/PaddleSeg/-/blob/paddleseg/models/unet.py?L155:16#tab=def
+
+# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# import paddle
+# import paddle.nn as nn
+# import paddle.nn.functional as F
+
+# from paddleseg import utils
+# from paddleseg.cvlibs import manager
+# from paddleseg.models import layers
+
+import os
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+
+# @manager.MODELS.add_component
+class UNet(nn.Module):
+    """
+    The UNet implementation based on PaddlePaddle.
+
+    The original article refers to
+    Olaf Ronneberger, et, al. "U-Net: Convolutional Networks for Biomedical Image Segmentation"
+    (https://arxiv.org/abs/1505.04597).
+
+    Args:
+        num_classes (int): The unique number of target classes.
+        align_corners (bool): An argument of F.interpolate. It should be set to False when the output size of feature
+            is even, e.g. 1024x512, otherwise it is True, e.g. 769x769.  Default: False.
+        use_deconv (bool, optional): A bool value indicates whether using deconvolution in upsampling.
+            If False, use resize_bilinear. Default: False.
+        pretrained (str, optional): The path or url of pretrained model for fine tuning. Default: None.
+    """
+
+    def __init__(self,
+            input_channels=6,
+            num_classes=2,
+            # topo=[64, 128, 256, 512],
+            topo=[16, 32, 64, 128],
+            # topo=[16, 32, 64],
+            align_corners=False,
+            use_deconv=False, # False
+            pretrained=None):
+        super().__init__()
+
+        self.encode = Encoder(input_channels, topo=topo)
+        decoder_topo = topo[::-1]
+        self.decode = Decoder(align_corners, use_deconv=use_deconv, topo=decoder_topo)
+        self.cls = self.conv = nn.Conv2d(
+            in_channels=topo[0],
+            out_channels=num_classes,
+            kernel_size=3,
+            stride=1,
+            padding=1)
+
+        # self.sigmoid = nn.Sigmoid()
+        # self.softmax = nn.Softmax(dim=1)
+
+        # self.pretrained = pretrained
+        # self.init_weight()
+
+    def forward(self, x):
+        ''' x should be a list or tuple '''
+        x = torch.cat(x, dim=1) # concat all input tensors
+
+        logit_list = []
+        xc, short_cuts = self.encode(x)
+        logit_list.append(xc) # most center features
+
+        # for shortcut in short_cuts:
+        #     print(shortcut.shape)
+
+        x = self.decode(xc, short_cuts)
+
+        x = self.cls(x) # output
+        logit_list.append(x)
+
+        return logit_list
+
+
+class UNet_dualHeads(nn.Module):
+    """
+    The UNet implementation based on PaddlePaddle.
+
+    The original article refers to
+    Olaf Ronneberger, et, al. "U-Net: Convolutional Networks for Biomedical Image Segmentation"
+    (https://arxiv.org/abs/1505.04597).
+
+    Args:
+        num_classes (int): The unique number of target classes.
+        align_corners (bool): An argument of F.interpolate. It should be set to False when the output size of feature
+            is even, e.g. 1024x512, otherwise it is True, e.g. 769x769.  Default: False.
+        use_deconv (bool, optional): A bool value indicates whether using deconvolution in upsampling.
+            If False, use resize_bilinear. Default: False.
+        pretrained (str, optional): The path or url of pretrained model for fine tuning. Default: None.
+    """
+
+    def __init__(self,
+            input_channels=6,
+            num_classes=2,
+            # topo=[64, 128, 256, 512],
+            topo=[16, 32, 64, 128],
+            # topo=[16, 32, 64],
+            align_corners=False,
+            use_deconv=False, # False
+            pretrained=None):
+        super().__init__()
+
+        self.encode = Encoder(input_channels, topo=topo)
+        decoder_topo = topo[::-1]
+        self.decode = Decoder(align_corners, use_deconv=use_deconv, topo=decoder_topo)
+        
+        self.cls = nn.Conv2d(
+            in_channels=topo[0],
+            out_channels=2, # burned areas
+            kernel_size=3,
+            stride=1,
+            padding=1)
+
+        self.reg_head = nn.Conv2d(
+            in_channels=topo[0],
+            out_channels=1,
+            kernel_size=3,
+            stride=1,
+            padding=1)
+
+        # self.sigmoid = nn.Sigmoid()
+        # self.softmax = nn.Softmax(dim=1)
+
+        # self.pretrained = pretrained
+        # self.init_weight()
+
+    def forward(self, x):
+        ''' x should be a list or tuple '''
+        x = torch.cat(x, dim=1) # concat all input tensors
+
+        logit_list = []
+        xc, short_cuts = self.encode(x)
+        # logit_list.append(xc) # most center features
+
+        # for shortcut in short_cuts:
+        #     print(shortcut.shape)
+
+        x = self.decode(xc, short_cuts)
+        reg_out = self.reg_head(x)
+        logit_list.append(reg_out)
+        
+        x = self.cls(x) # output
+        logit_list.append(x)
+
+        return logit_list
+
+
+
+class Encoder(nn.Module):
+    def __init__(self, input_channels=3, topo=[16, 32, 64, 128]):
+        super().__init__()
+
+        self.double_conv = nn.Sequential(
+            ConvBNReLU(input_channels, topo[0], 3), 
+            ConvBNReLU(topo[0], topo[0], 3)
+        )
+
+        down_channels = []
+        for i in range(0, len(topo)):
+            if i < len(topo) - 1: down_channels.append([topo[i], topo[i+1]])
+            else: down_channels.append([topo[i], topo[i]])
+        # print(down_channels)
+        # down_channels = [[64, 128], [128, 256], [256, 512], [512, 512]]
+
+        self.downLayerList = []
+        for channel in down_channels:
+            self.downLayerList.append(self.down_sampling(channel[0], channel[1]))
+        self.down_stages = nn.Sequential(*self.downLayerList)
+        
+
+    def down_sampling(self, in_channels, out_channels):
+        modules = []
+        modules.append(nn.MaxPool2d(kernel_size=2, stride=2))
+        modules.append(ConvBNReLU(in_channels, out_channels, 3))
+        modules.append(ConvBNReLU(out_channels, out_channels, 3))
+        return nn.Sequential(*modules)
+
+    def forward(self, x):
+        short_cuts = []
+
+        # print("---- Encoder ----")
+        x = self.double_conv(x)
+        for down_sample in self.downLayerList:
+            short_cuts.append(x)
+            x = down_sample(x)
+            # print(x.shape)
+        return x, short_cuts
+
+
+class Decoder(nn.Module):
+    def __init__(self, align_corners, use_deconv=False, topo=[16, 32, 64, 128]):
+        super().__init__()
+
+        # up_channels = [[512, 256], [256, 128], [128, 64], [64, 64]]
+        # [512, 256, 128, 64512]
+        up_channels = []
+        for i in range(0, len(topo)):
+            if i < len(topo)-1: up_channels.append([topo[i], topo[i+1]])
+            else: up_channels.append([topo[i], topo[i]])
+        print(up_channels)
+
+        self.upLayerList = [
+            UpSampling(channel[0], channel[1], align_corners, use_deconv)
+            for channel in up_channels
+        ]
+
+        self.up_stages = nn.Sequential(*self.upLayerList)
+
+    def forward(self, x, short_cuts):
+        # print("---- UNet Dncoder ----")
+        for i in range(len(short_cuts)):
+            # print(x.shape, short_cuts[-(i+1)].shape)
+            x = self.up_stages[i](x, short_cuts[-(i + 1)])
+            # print(x.shape)
+
+        return x
+
+class UpSampling(nn.Module):
+    def __init__(self,
+                 in_channels,
+                 out_channels,
+                 align_corners,
+                 use_deconv=True):
+        super().__init__()
+
+        self.align_corners = align_corners
+
+        self.use_deconv = use_deconv
+        if self.use_deconv:
+            self.deconv = nn.ConvTranspose2d(
+                in_channels,
+                out_channels // 2,
+                kernel_size=2,
+                stride=2,
+                padding=0)
+            in_channels = in_channels + out_channels // 2
+        else:
+            in_channels *= 2
+
+        self.double_conv = nn.Sequential(
+            ConvBNReLU(in_channels, out_channels, 3),
+            ConvBNReLU(out_channels, out_channels, 3))
+
+    def forward(self, x, short_cut):
+        # print("before inter: ", x.shape, short_cut.shape)
+
+        if self.use_deconv:
+            x = self.deconv(x)
+        else:
+            x = F.interpolate(
+                x,
+                (short_cut.shape)[2:],
+                mode='bilinear',
+                align_corners=self.align_corners)
+
+        # print(x.shape, short_cut.shape)
+        x = torch.cat([x, short_cut], dim=1)
+        x = self.double_conv(x)
+        # print(x.shape)
+        return x
+
+
+# https://sourcegraph.com/github.com/PaddlePaddle/PaddleSeg/-/blob/paddleseg/models/layers/layer_libs.py?L98
+class ConvBNReLU(nn.Module):
+    def __init__(self,
+                 in_channels,
+                 out_channels,
+                 kernel_size,
+                 padding='same',
+                 **kwargs):
+        super().__init__()
+
+        self._conv = nn.Conv2d(
+            # in_channels, out_channels, kernel_size, **kwargs)
+            in_channels, out_channels, kernel_size, padding=1, **kwargs)
+
+        self._batch_norm = nn.BatchNorm2d(out_channels)
+        self._relu = nn.ReLU()
+        # self._dropout = nn.Dropout2d(p=0.1) # added by puzhao
+
+    def forward(self, x):
+        x = self._conv(x)
+        x = self._batch_norm(x)
+        x = self._relu(x)
+        # x = self._dropout(x) # added by puzhao
+        return x
+
+
+
+if __name__ == "__main__":
+
+    import numpy as np
+    from torchsummary import summary
+
+    torch.use_deterministic_algorithms(True)
+
+    x1 = np.random.rand(10,3,256,256)
+    # x2 = np.random.rand(10,3,256,256)
+    x1 = torch.from_numpy(x1).type(torch.FloatTensor)#.cuda().type(torch.cuda.FloatTensor)
+    # x2 = torch.from_numpy(x2).type(torch.FloatTensor)#.cuda().type(torch.cuda.FloatTensor)
+
+    myunet = UNet(input_channels=x1.shape[1])
+    # myunet.cuda()
+
+    print(myunet)
+    print(myunet.forward([x1])[-1].shape)
+    # summary(myunet, (3,256,256))
\ No newline at end of file
diff --git a/models/unet_cdc.py b/models/unet_cdc.py
new file mode 100644
index 0000000..37f388d
--- /dev/null
+++ b/models/unet_cdc.py
@@ -0,0 +1,257 @@
+# https://sourcegraph.com/github.com/PaddlePaddle/PaddleSeg/-/blob/paddleseg/models/unet.py?L155:16#tab=def
+
+# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# import paddle
+# import paddle.nn as nn
+# import paddle.nn.functional as F
+
+# from paddleseg import utils
+# from paddleseg.cvlibs import manager
+# from paddleseg.models import layers
+
+import os
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+
+# @manager.MODELS.add_component
+class UNet(nn.Module):
+    """
+    The UNet implementation based on PaddlePaddle.
+
+    The original article refers to
+    Olaf Ronneberger, et, al. "U-Net: Convolutional Networks for Biomedical Image Segmentation"
+    (https://arxiv.org/abs/1505.04597).
+
+    Args:
+        num_classes (int): The unique number of target classes.
+        align_corners (bool): An argument of F.interpolate. It should be set to False when the output size of feature
+            is even, e.g. 1024x512, otherwise it is True, e.g. 769x769.  Default: False.
+        use_deconv (bool, optional): A bool value indicates whether using deconvolution in upsampling.
+            If False, use resize_bilinear. Default: False.
+        pretrained (str, optional): The path or url of pretrained model for fine tuning. Default: None.
+    """
+
+    def __init__(self,
+                 input_channels=6,
+                 num_classes=2,
+                #  topo=[64, 128, 256, 512],
+                #  topo=[32, 64, 128, 256],
+                 topo=[16, 32, 64, 128],
+                 align_corners=False,
+                 use_deconv=False,
+                 pretrained=None):
+        super().__init__()
+
+        self.encode = Encoder(input_channels, topo=topo)
+        self.encode_2 = Encoder(input_channels, topo=topo)
+        self.decode = Decoder(align_corners, use_deconv=use_deconv, topo=topo)
+        self.cls = self.conv = nn.Conv2d(
+            in_channels=topo[0],
+            out_channels=num_classes,
+            kernel_size=3,
+            stride=1,
+            padding=1)
+
+        self.softmax = nn.Softmax(dim=1)
+
+        # self.pretrained = pretrained
+        # self.init_weight()
+
+        print(f"UNet TOPO: ", topo)
+
+    def forward(self, x, train=True):
+        # x = torch.cat((x1, x2), dim=1)
+        # x = torch.cat(x, dim=1)
+        x1, x2 = x
+
+        logit_list = []
+        xc1, short_cuts1 = self.encode(x1)
+        x1 = self.decode(xc1, short_cuts1)
+        x1 = self.cls(x1) # output
+        x1 = self.softmax(x1)
+        logit_list.append(x1)
+
+        # for shortcut in short_cuts:
+        #     print(shortcut.shape)
+
+        if train:
+            xc2, short_cuts2 = self.encode_2(x2)
+            x2 = self.decode(xc2, short_cuts2)
+            x2 = self.cls(x2) # output
+            x2 = self.softmax(x2)
+            logit_list.append(x2)
+        else:
+            logit_list.append(torch.tensor([0]))
+
+        # logit = nn.LogSoftmax(dim=1)(x)
+        # logit_list.append(logit)
+        return logit_list
+
+
+
+class Encoder(nn.Module):
+    def __init__(self, input_channels=6, topo=[64,128,256,512]):
+        super().__init__()
+
+        self.double_conv = nn.Sequential(
+            ConvBNReLU(input_channels, topo[0], 3), ConvBNReLU(topo[0], topo[0], 3))
+
+        down_channels = []
+        for i in range(0, len(topo)):
+            if i < len(topo) - 1: down_channels.append([topo[i], topo[i+1]])
+            else: down_channels.append([topo[i], topo[i]])
+        # print(down_channels)
+        # down_channels = [[64, 128], [128, 256], [256, 512], [512, 512]]
+
+        self.downLayerList = []
+        for channel in down_channels:
+            self.downLayerList.append(self.down_sampling(channel[0], channel[1]))
+        self.down_stages = nn.Sequential(*self.downLayerList)
+        
+
+    def down_sampling(self, in_channels, out_channels):
+        modules = []
+        modules.append(nn.MaxPool2d(kernel_size=2, stride=2))
+        modules.append(ConvBNReLU(in_channels, out_channels, 3))
+        modules.append(ConvBNReLU(out_channels, out_channels, 3))
+        return nn.Sequential(*modules)
+
+    def forward(self, x):
+        short_cuts = []
+
+        # print("---- Encoder ----")
+        x = self.double_conv(x)
+        for down_sample in self.downLayerList:
+            short_cuts.append(x)
+            x = down_sample(x)
+            # print(x.shape)
+        return x, short_cuts
+
+
+class Decoder(nn.Module):
+    def __init__(self, align_corners, use_deconv=False, topo=[64,128,256,512]):
+        super().__init__()
+
+        # up_channels = [[512, 256], [256, 128], [128, 64], [64, 64]]
+        up_channels = []
+        decoder_topo = topo[::-1]
+        for i in range(0, len(decoder_topo)):
+            if i < len(topo) - 1: up_channels.append([decoder_topo[i], decoder_topo[i+1]])
+            else: up_channels.append([decoder_topo[i], decoder_topo[i]])
+        # print(up_channels)
+
+        self.upLayerList = [
+            UpSampling(channel[0], channel[1], align_corners, use_deconv)
+            for channel in up_channels
+        ]
+
+        self.up_stages = nn.Sequential(*self.upLayerList)
+
+    def forward(self, x, short_cuts):
+        # print("---- UNet Dncoder ----")
+        for i in range(len(short_cuts)):
+            x = self.up_stages[i](x, short_cuts[-(i + 1)])
+            # print(x.shape)
+
+        return x
+
+
+class UpSampling(nn.Module):
+    def __init__(self,
+                 in_channels,
+                 out_channels,
+                 align_corners,
+                 use_deconv=False):
+        super().__init__()
+
+        self.align_corners = align_corners
+
+        self.use_deconv = use_deconv
+        if self.use_deconv:
+            self.deconv = nn.ConvTranspose2d(
+                in_channels,
+                out_channels // 2,
+                kernel_size=2,
+                stride=2,
+                padding=0)
+            in_channels = in_channels + out_channels // 2
+        else:
+            in_channels *= 2
+
+        self.double_conv = nn.Sequential(
+            ConvBNReLU(in_channels, out_channels, 3),
+            ConvBNReLU(out_channels, out_channels, 3))
+
+    def forward(self, x, short_cut):
+        if self.use_deconv:
+            x = self.deconv(x)
+        else:
+            x = F.interpolate(
+                x,
+                (short_cut.shape)[2:],
+                mode='bilinear',
+                align_corners=self.align_corners)
+        x = torch.cat([x, short_cut], dim=1)
+        x = self.double_conv(x)
+        return x
+
+
+# https://sourcegraph.com/github.com/PaddlePaddle/PaddleSeg/-/blob/paddleseg/models/layers/layer_libs.py?L98
+
+class ConvBNReLU(nn.Module):
+    def __init__(self,
+                 in_channels,
+                 out_channels,
+                 kernel_size,
+                 padding='same',
+                 **kwargs):
+        super().__init__()
+
+        self._conv = nn.Conv2d(
+            # in_channels, out_channels, kernel_size, **kwargs)
+            in_channels, out_channels, kernel_size, padding=1, **kwargs)
+
+        self._batch_norm = nn.BatchNorm2d(out_channels)
+        self._relu = nn.ReLU()
+
+    def forward(self, x):
+        x = self._conv(x)
+        x = self._batch_norm(x)
+        x = self._relu(x)
+        return x
+
+
+
+if __name__ == "__main__":
+
+    import numpy as np
+    from torchsummary import summary
+
+    x1 = np.random.rand(10,3,256,256)
+    x2 = np.random.rand(10,3,256,256)
+    x1 = torch.from_numpy(x1).type(torch.FloatTensor)#.cuda().type(torch.cuda.FloatTensor)
+    x2 = torch.from_numpy(x2).type(torch.FloatTensor)#.cuda().type(torch.cuda.FloatTensor)
+
+    myunet = UNet(input_channels=3)
+    # myunet.cuda()
+
+    # print(myunet)
+
+    out1, out2 = myunet.forward((x1, x2))
+    print(out1.shape)
+    # summary(myunet, (3,256,256))
\ No newline at end of file
diff --git a/models/unet_distill.py b/models/unet_distill.py
new file mode 100644
index 0000000..8b6be3d
--- /dev/null
+++ b/models/unet_distill.py
@@ -0,0 +1,251 @@
+# https://sourcegraph.com/github.com/PaddlePaddle/PaddleSeg/-/blob/paddleseg/models/unet.py?L155:16#tab=def
+
+# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# import paddle
+# import paddle.nn as nn
+# import paddle.nn.functional as F
+
+# from paddleseg import utils
+# from paddleseg.cvlibs import manager
+# from paddleseg.models import layers
+
+import os
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+
+# @manager.MODELS.add_component
+class UNet(nn.Module):
+    """
+    The UNet implementation based on PaddlePaddle.
+
+    The original article refers to
+    Olaf Ronneberger, et, al. "U-Net: Convolutional Networks for Biomedical Image Segmentation"
+    (https://arxiv.org/abs/1505.04597).
+
+    Args:
+        num_classes (int): The unique number of target classes.
+        align_corners (bool): An argument of F.interpolate. It should be set to False when the output size of feature
+            is even, e.g. 1024x512, otherwise it is True, e.g. 769x769.  Default: False.
+        use_deconv (bool, optional): A bool value indicates whether using deconvolution in upsampling.
+            If False, use resize_bilinear. Default: False.
+        pretrained (str, optional): The path or url of pretrained model for fine tuning. Default: None.
+    """
+
+    def __init__(self,
+            input_channels=6,
+            num_classes=2,
+            # topo=[64, 128, 256, 512],
+            topo=[16, 32, 64, 128],
+            # topo=[16, 32, 64],
+            align_corners=False,
+            use_deconv=False,
+            pretrained=None):
+        super().__init__()
+
+        self.encode = Encoder(input_channels, topo=topo)
+        decoder_topo = topo[::-1]
+        self.decode = Decoder(align_corners, use_deconv=use_deconv, topo=decoder_topo)
+        self.cls = self.conv = nn.Conv2d(
+            in_channels=topo[0],
+            out_channels=num_classes,
+            kernel_size=3,
+            stride=1,
+            padding=1)
+
+        # self.sigmoid = nn.Sigmoid()
+        # self.softmax = nn.Softmax(dim=1)
+
+        # self.pretrained = pretrained
+        # self.init_weight()
+
+    def forward(self, x):
+        ''' x should be a list or tuple '''
+        x = torch.cat(x, dim=1) # concat all input tensors
+
+        logit_list = []
+        xc, short_cuts = self.encode(x)
+        logit_list.append(xc) # most center features
+
+        # for shortcut in short_cuts:
+        #     print(shortcut.shape)
+
+        x = self.decode(xc, short_cuts)
+        x = self.cls(x) # output
+        logit_list.append(x)
+
+        # logit = self.sigmoid(x)
+        # logit_list.append(logit)
+        return logit_list
+
+
+
+class Encoder(nn.Module):
+    def __init__(self, input_channels=3, topo=[16, 32, 64, 128]):
+        super().__init__()
+
+        self.double_conv = nn.Sequential(
+            ConvBNReLU(input_channels, topo[0], 3), 
+            ConvBNReLU(topo[0], topo[0], 3)
+        )
+
+        down_channels = []
+        for i in range(0, len(topo)):
+            if i < len(topo) - 1: down_channels.append([topo[i], topo[i+1]])
+            else: down_channels.append([topo[i], topo[i]])
+        # print(down_channels)
+        # down_channels = [[64, 128], [128, 256], [256, 512], [512, 512]]
+
+        self.downLayerList = []
+        for channel in down_channels:
+            self.downLayerList.append(self.down_sampling(channel[0], channel[1]))
+        self.down_stages = nn.Sequential(*self.downLayerList)
+        
+
+    def down_sampling(self, in_channels, out_channels):
+        modules = []
+        modules.append(nn.MaxPool2d(kernel_size=2, stride=2))
+        modules.append(ConvBNReLU(in_channels, out_channels, 3))
+        modules.append(ConvBNReLU(out_channels, out_channels, 3))
+        return nn.Sequential(*modules)
+
+    def forward(self, x):
+        short_cuts = []
+
+        # print("---- Encoder ----")
+        x = self.double_conv(x)
+        for down_sample in self.downLayerList:
+            short_cuts.append(x)
+            x = down_sample(x)
+            # print(x.shape)
+        return x, short_cuts
+
+
+class Decoder(nn.Module):
+    def __init__(self, align_corners, use_deconv=False, topo=[16, 32, 64, 128]):
+        super().__init__()
+
+        # up_channels = [[512, 256], [256, 128], [128, 64], [64, 64]]
+        # [512, 256, 128, 64512]
+        up_channels = []
+        for i in range(0, len(topo)):
+            if i < len(topo)-1: up_channels.append([topo[i], topo[i+1]])
+            else: up_channels.append([topo[i], topo[i]])
+        # print(up_channels)
+
+        self.upLayerList = [
+            UpSampling(channel[0], channel[1], align_corners, use_deconv)
+            for channel in up_channels
+        ]
+
+        self.up_stages = nn.Sequential(*self.upLayerList)
+
+    def forward(self, x, short_cuts):
+        # print("---- UNet Dncoder ----")
+        for i in range(len(short_cuts)):
+            # print(x.shape, short_cuts[-(i+1)].shape)
+            x = self.up_stages[i](x, short_cuts[-(i + 1)])
+            # print(x.shape)
+
+        return x
+
+class UpSampling(nn.Module):
+    def __init__(self,
+                 in_channels,
+                 out_channels,
+                 align_corners,
+                 use_deconv=False):
+        super().__init__()
+
+        self.align_corners = align_corners
+
+        self.use_deconv = use_deconv
+        if self.use_deconv:
+            self.deconv = nn.ConvTranspose2d(
+                in_channels,
+                out_channels // 2,
+                kernel_size=2,
+                stride=2,
+                padding=0)
+            in_channels = in_channels + out_channels // 2
+        else:
+            in_channels *= 2
+
+        self.double_conv = nn.Sequential(
+            ConvBNReLU(in_channels, out_channels, 3),
+            ConvBNReLU(out_channels, out_channels, 3))
+
+    def forward(self, x, short_cut):
+        # print("before inter: ", x.shape, short_cut.shape)
+
+        if self.use_deconv:
+            x = self.deconv(x)
+        else:
+            x = F.interpolate(
+                x,
+                (short_cut.shape)[2:],
+                mode='bilinear',
+                align_corners=self.align_corners)
+
+        # print(x.shape, short_cut.shape)
+        x = torch.cat([x, short_cut], dim=1)
+        x = self.double_conv(x)
+        # print(x.shape)
+        return x
+
+
+# https://sourcegraph.com/github.com/PaddlePaddle/PaddleSeg/-/blob/paddleseg/models/layers/layer_libs.py?L98
+class ConvBNReLU(nn.Module):
+    def __init__(self,
+                 in_channels,
+                 out_channels,
+                 kernel_size,
+                 padding='same',
+                 **kwargs):
+        super().__init__()
+
+        self._conv = nn.Conv2d(
+            # in_channels, out_channels, kernel_size, **kwargs)
+            in_channels, out_channels, kernel_size, padding=1, **kwargs)
+
+        self._batch_norm = nn.BatchNorm2d(out_channels)
+        self._relu = nn.ReLU()
+
+    def forward(self, x):
+        x = self._conv(x)
+        x = self._batch_norm(x)
+        x = self._relu(x)
+        return x
+
+
+
+if __name__ == "__main__":
+
+    import numpy as np
+    from torchsummary import summary
+
+    x1 = np.random.rand(100,3,256,256)
+    # x2 = np.random.rand(10,3,256,256)
+    x1 = torch.from_numpy(x1).type(torch.FloatTensor)#.cuda().type(torch.cuda.FloatTensor)
+    # x2 = torch.from_numpy(x2).type(torch.FloatTensor)#.cuda().type(torch.cuda.FloatTensor)
+
+    myunet = UNet(input_channels=x1.shape[1])
+    # myunet.cuda()
+
+    # print(myunet)
+    print(myunet.forward([x1]).shape)
+    # summary(myunet, (3,256,256))
\ No newline at end of file
diff --git a/note.md b/note.md
new file mode 100644
index 0000000..ce9e3aa
--- /dev/null
+++ b/note.md
@@ -0,0 +1,9 @@
+
+## 2021-12-16
+Dice loss only accept binarized input, fixed.
+
+
+# Torch or Paddle 
+## paddle models 
+## mmsegmentation (SegTransformer)
+
diff --git a/playground/test_IoU.py b/playground/test_IoU.py
new file mode 100644
index 0000000..8898643
--- /dev/null
+++ b/playground/test_IoU.py
@@ -0,0 +1,82 @@
+import io
+import os
+import numpy as np
+import torch
+from pathlib import Path
+from imageio import imread, imsave
+
+# from smp.utils import train
+
+def IoU_score(gt, pr, eps=1e-3):
+    intersection = np.sum(gt * pr)
+    union = np.sum(gt) + np.sum(pr) - intersection + eps
+
+    return intersection / union, intersection, union
+
+
+phase = 'test_images'
+event_dir = Path(f"/home/p/u/puzhao/wildfire-s1s2-dataset-ca-tiles/{phase}/S2/post")
+result_dir = Path("/home/p/u/puzhao/smp-seg-pytorch/outputs/run_s1s2_UNet_['S2']_allBands_20220107T222557/errMap")
+
+
+# Fresh
+# result_dir = Path("/home/p/u/puzhao/smp-seg-pytorch/outputs/run_s1s2_Paddle_unet_resnet18_['S1']_20211224T210446/errMap_tiles")
+# event_dir = Path("/home/p/u/puzhao/wildfire-s1s2-dataset-ca-tiles/test/S2/post")
+
+# # V0
+# phase = 'test_images'
+# result_dir = Path(f"/home/p/u/puzhao/smp-seg-pytorch/outputs/run_s1s2_UNet_resnet18_['S2']_TEST_20220107T175600/errMap")
+# event_dir = Path(f"/home/p/u/puzhao/wildfire-s1s2-dataset-ca-tiles/{phase}/S2/post")
+
+# /home/p/u/puzhao/smp-seg-pytorch/outputs/run_s1s2_Paddle_unet_resnet18_['S1']_V0_20211225T005247/errMap_train_tiles
+
+
+eventList = [filename[:-4] for filename in os.listdir(event_dir)]
+print(len(eventList))
+
+if False:
+    ss = np.random.randint(0, len(eventList),len(eventList))
+    eventList = [eventList[i] for i in ss]
+
+step = len(eventList) #, valid batch size
+
+IS = []
+UN = []
+IoU = []
+IoU_per10 = []
+for i, event in enumerate(eventList):
+    print()
+    print(event)
+
+    gt = imread(result_dir / f"{event}_gts.png")[:,:,0] / 255
+    pr = imread(result_dir / f"{event}_pred.png")[:,:,0] / 255
+
+    # print(gt.shape)
+    # print(pr.shape)
+
+    iou, intersection, union = IoU_score(gt, pr)
+    print(intersection, union)
+    print(f"IoU: {iou:.4f}")
+
+    IS.append(intersection)
+    UN.append(union)
+    IoU.append(iou)
+
+    mIoU = sum(IS) / sum(UN)
+    aIoU = sum(IoU) / len(IoU)
+
+    print(f"mIoU: {mIoU:.4f}, aIoU: {aIoU:.4f}")
+
+
+print("----> batch IoU <-----")
+group_IS = []
+group_UN = []
+group_IoU = []
+
+for i in range(0, len(eventList), step):
+    group_IS = sum(IS[i:i+step])
+    group_UN = sum(UN[i:i+step])
+    group_IoU.append(group_IS / group_UN)
+
+    batch_mIoU = sum(group_IoU) / len(group_IoU)
+    print(f"{i}, {len(group_IoU)}, batch_mIoU: {batch_mIoU:.4f}")
\ No newline at end of file
diff --git a/playground/test_wildfire_dataset.py b/playground/test_wildfire_dataset.py
new file mode 100644
index 0000000..db60378
--- /dev/null
+++ b/playground/test_wildfire_dataset.py
@@ -0,0 +1,32 @@
+sat_list = ['S2', 'S1']
+prepost = ['pre', 'post']
+stacking = False
+
+image_list = []
+for sat in (sat_list):
+    # post_fps = self.fps_dict[sat][1]
+    # image_post = tiff.imread(post_fps[i]) # C*H*W
+    # image_post = np.nan_to_num(image_post, 0)
+    # if sat in ['S1', 'ALOS']: image_post = (np.clip(image_post, -30, 0) + 30) / 30
+    # image_post = image_post[self.band_index_dict[sat],] # select bands
+
+    image_post = f"{sat}_post"
+
+    if 'pre' in prepost:
+        # pre_fps = self.fps_dict[sat][0]
+        # image_pre = tiff.imread(pre_fps[i])
+        # image_pre = np.nan_to_num(image_pre, 0)
+        # if sat in ['S1', 'ALOS']: image_pre = (np.clip(image_pre, -30, 0) + 30) / 30
+        # image_pre = image_pre[self.band_index_dict[sat],] # select bands
+        image_pre = f"{sat}_pre"
+        
+        if stacking: # if stacking bi-temporal data
+            # stacked = np.concatenate((image_pre, image_post), axis=0) 
+            stacked = f"{image_pre}_{image_post}"
+            image_list.append(stacked) #[x1, x2]
+        else:
+            image_list += [image_pre, image_post] #[t1, t2]
+    else:
+        image_list.append(image_post) #[x1_t2, x2_t2]
+
+print(image_list)
\ No newline at end of file
diff --git a/run_on_geoinfo/geo_run_s1s2_fuse_unet.sh b/run_on_geoinfo/geo_run_s1s2_fuse_unet.sh
new file mode 100644
index 0000000..3a7babc
--- /dev/null
+++ b/run_on_geoinfo/geo_run_s1s2_fuse_unet.sh
@@ -0,0 +1,33 @@
+#!/bin/bash
+#SBATCH -N 1
+#SBATCH --gres=gpu:1
+#SBATCH --mem 100GB
+#SBATCH --cpus-per-task 8
+#SBATCH -t 7-00:00:00
+#SBATCH --job-name fuse-unet
+
+echo "start"
+echo "Starting job ${SLURM_JOB_ID} on ${SLURMD_NODENAME}"
+
+nvidia-smi
+. /geoinfo_vol1/puzhao/miniforge3/etc/profile.d/conda.sh
+
+CDC=(0 0.01 0.1 1)
+CFG=${CDC[$SLURM_ARRAY_TASK_ID]}
+echo "Running simulation $CFG"
+# echo "singularity exec --nv /cephyr/users/puzhao/Alvis/PyTorch_v1.7.0-py3.sif python main_s1s2_fuse_unet_V1.py model.cross_domain_coef=$CFG"
+echo "---------------------------------------------------------------------------------------------------------------"
+
+conda activate pytorch
+PYTHONUNBUFFERED=1; python3 main_s1s2_cdc_unet.py model.cross_domain_coef=$CFG model.batch_size=16
+
+# rsync -a $TMPDIR/temporal-consistency/outputs/* $exp_dir
+# kill $LOOPPID
+
+rm -rf $SLURM_SUBMIT_DIR/*.log
+# rm -rf $SLURM_SUBMIT_DIR/*.out
+
+echo "finish"
+
+# # run
+# sbatch --array=0-2 run_s1s2_fuse_unet.sh
\ No newline at end of file
diff --git a/run_on_geoinfo/run_all_cfg.sh b/run_on_geoinfo/run_all_cfg.sh
new file mode 100644
index 0000000..0958399
--- /dev/null
+++ b/run_on_geoinfo/run_all_cfg.sh
@@ -0,0 +1,50 @@
+
+# Test training benchmark for several models.
+
+# Use docker： paddlepaddle/paddle:latest-gpu-cuda10.1-cudnn7  paddle=2.1.2  py=37
+
+# Usage:
+#   git clone git clone https://github.com/PaddlePaddle/PaddleSeg.git
+#   cd PaddleSeg
+#   bash benchmark/run_all.sh
+
+# pip install -r requirements.txt
+
+# # Download test dataset and save it to PaddleSeg/data
+# # It automatic downloads the pretrained models saved in ~/.paddleseg
+# mkdir -p data
+# wget https://paddleseg.bj.bcebos.com/dataset/cityscapes_30imgs.tar.gz \
+#     -O data/cityscapes_30imgs.tar.gz
+# tar -zxf data/cityscapes_30imgs.tar.gz -C data/
+
+# bash run_on_geoinfo/run_all_cfg.sh
+model_name_list=(UNet UNet_resnet18 SegFormer_B2)
+# fp_item_list=(fp32)     # set fp32 or fp16, segformer_b0 doesn't support fp16 with Paddle2.1.2
+bs_list=(16)
+max_iters=100           # control the test time
+num_workers=4           # num_workers for dataloader
+weight_decay_list=(0.05 0.01)
+
+for model_name in ${model_name_list[@]}; do
+      for fp_item in ${fp_item_list[@]}; do
+          for bs_item in ${bs_list[@]}; do
+            for weight_decay in ${weight_decay_list[@]}; do
+                echo "index is speed, 1gpus, begin, ${model_name}"
+                run_mode=sp
+                CUDA_VISIBLE_DEVICES=0 
+                sbatch run_on_geoinfo/run_benchmark.sh ${run_mode} ${bs_item} \
+                    ${max_epochs} ${model_name} ${num_workers} ${weight_decay}
+                sleep 60
+
+                # echo "index is speed, 8gpus, run_mode is multi_process, begin, ${model_name}"
+                # run_mode=mp
+                # CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 bash benchmark/run_benchmarksh ${run_mode} ${bs_item} ${fp_item} \
+                #     ${max_iters} ${model_name} ${num_workers}
+                # sleep 60
+                done
+            done
+      done
+done
+
+# rm -rf data/*
+# rm -rf ~/.paddleseg
diff --git a/run_on_geoinfo/run_benchmark.sh b/run_on_geoinfo/run_benchmark.sh
new file mode 100644
index 0000000..d0e91d4
--- /dev/null
+++ b/run_on_geoinfo/run_benchmark.sh
@@ -0,0 +1,92 @@
+#!/bin/bash
+#SBATCH -N 1
+#SBATCH --gres=gpu:1
+#SBATCH --mem 36GB
+#SBATCH --cpus-per-task 8
+#SBATCH -t 7-00:00:00
+#SBATCH --job-name pdunet
+#SBATCH --output=slurm-%x-%A_%a.out
+
+echo "start"
+echo "Starting job ${SLURM_JOB_ID} on ${SLURMD_NODENAME}"
+echo
+nvidia-smi
+. /geoinfo_vol1/puzhao/miniforge3/etc/profile.d/conda.sh
+
+conda activate paddle2
+PYTHONUNBUFFERED=1; 
+
+#!/usr/bin/env bash
+# set -xe
+
+# Test training benchmark for a model.
+
+# Usage：CUDA_VISIBLE_DEVICES=0 bash run_benchmark.sh ${run_mode} ${bs_item} ${fp_item} ${max_iter} ${model_name} ${num_workers}
+
+function _set_params(){
+    run_mode=${1:-"sp"}         # sp or mp
+    batch_size=${2:-"16"}
+    # fp_item=${3:-"fp32"}        # fp32 or fp16
+    satellites=${3:-"S1"}
+    max_epochs=${4:-"100"}
+    model_name=${5:-"UNet"}
+    num_workers=${6:-"4"}
+    weight_decay=${7:-"0.01"}
+    run_log_path=${TRAIN_LOG_DIR:-$(pwd)}
+
+    device=${CUDA_VISIBLE_DEVICES//,/ }
+    arr=(${device})
+    num_gpu_devices=${#arr[*]}
+    log_file=${run_log_path}/${model_name}_${satellites}_${run_mode}_bs${batch_size}_${num_gpu_devices}
+}
+
+function _train(){
+    echo "Train on ${num_gpu_devices} GPUs"
+    echo "current CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES, gpus=$num_gpu_devices, batch_size=$batch_size"
+
+    # train_cmd="--config=configs/${model_name}.yml \
+    #            --batch_size=${batch_size} \
+    #            --iters=${max_epochs} \
+    #            --num_workers=${num_workers}"
+
+    # python3 main_s1s2_unet.py \
+    train_cmd="--config-name=unet.yaml \
+            RAND.SEED=0 \
+            RAND.DETERMIN=True \
+            DATA.SATELLITES=[$satellites] \
+            DATA.INPUT_BANDS.S2=['B4','B8','B12'] \
+            MODEL.ARCH=$model_name \
+            MODEL.WEIGHT_DECAY=$weight_decay \
+            MODEL.NUM_CLASSES=1 \
+            MODEL.LOSS_TYPE=DiceLoss \
+            MODEL.LR_SCHEDULER=cosine \
+            MODEL.ACTIVATION=sigmoid \
+            MODEL.BATCH_SIZE=${batch_size} \
+            MODEL.MAX_EPOCH=${max_epochs}"
+
+    case ${run_mode} in
+    sp) train_cmd="python -u train.py ${train_cmd}" ;;
+    mp)
+        train_cmd="python -m torch.distributed.launch --log_dir=./mylog --gpus=$CUDA_VISIBLE_DEVICES \
+                  train.py ${train_cmd}" ;;
+    *) echo "choose run_mode(sp or mp)"; exit 1;
+    esac
+
+    timeout 15m ${train_cmd} > ${log_file} 2>&1
+    if [ $? -ne 0 ];then
+        echo -e "${model_name}, FAIL"
+        export job_fail_flag=1
+    else
+        echo -e "${model_name}, SUCCESS"
+        export job_fail_flag=0
+    fi
+    kill -9 `ps -ef|grep 'python'|awk '{print $2}'`
+
+    if [ $run_mode = "mp" -a -d mylog ]; then
+        rm ${log_file}
+        cp mylog/workerlog.0 ${log_file}
+    fi
+}
+
+_set_params $@
+_train
diff --git a/run_on_geoinfo/run_distill_unet.sh b/run_on_geoinfo/run_distill_unet.sh
new file mode 100644
index 0000000..b529d11
--- /dev/null
+++ b/run_on_geoinfo/run_distill_unet.sh
@@ -0,0 +1,90 @@
+#!/bin/bash
+#SBATCH -N 1
+#SBATCH --gres=gpu:1
+#SBATCH --mem 36GB
+#SBATCH --cpus-per-task 8
+#SBATCH -t 7-00:00:00
+#SBATCH --job-name distill-unet
+#SBATCH --output /home/p/u/puzhao/run_logs/%x-%A_%a.out
+
+echo "start"
+echo "Starting job ${SLURM_JOB_ID} on ${SLURMD_NODENAME}"
+echo
+nvidia-smi
+. /geoinfo_vol1/puzhao/miniforge3/etc/profile.d/conda.sh
+
+# module --ignore-cache load "intel"
+# PROJECT_DIR=/cephyr/NOBACKUP/groups/snic2021-7-104/puzhao-snic-500G/smp-seg-pytorch
+
+# # Choose different config files
+# # DIRS=($(find /cephyr/NOBACKUP/groups/snic2021-7-104/puzhao-snic-500G/smp-seg-pytorch/config/s1s2_cfg/))
+# DIRS=($(find $PROJECT_DIR/config/s1s2_unet/))
+# DIRS=${DIRS[@]:1}
+# CFG=${DIRS[$SLURM_ARRAY_TASK_ID]}
+# echo "Running simulation $CFG"
+
+# # Choose different sensors
+SAT=('ND' 'VH' 'VV')
+CFG=${SAT[$SLURM_ARRAY_TASK_ID]}
+echo "Running simulation $CFG"
+# echo "python3 main_s1s2_unet.py model.batch_size=32"
+echo "---------------------------------------------------------------------------------------------------------------"
+
+# singularity exec --nv /cephyr/users/puzhao/Alvis/PyTorch_v1.7.0-py3.sif python main_s1s2_unet.py s1s2_unet=$CFG
+conda activate pytorch
+PYTHONUNBUFFERED=1; 
+
+
+# sbatch run_on_geoinfo/run_distill_unet.sh
+# python3 main_s1s2_unet.py \
+#             --config-name=distill_unet.yaml \
+#             data.satellites=['S1'] \
+#             model.LOSS_COEF=[0,0] \
+#             model.ARCH=distill_unet \
+#             model.DISTILL=True \
+#             model.batch_size=16 \
+#             model.max_epoch=100 \
+#             experiment.note=S1_pretrain
+
+# echo "-------------------- distill-unet: PRETRAIN ------------------------"
+# python3 main_s1s2_distill_unet.py \
+#             --config-name=distill_unet.yaml \
+#             data.satellites=['S1','S2'] \
+#             data.INPUT_BANDS.S2=['B4','B8','B12']\
+#             model.LOSS_COEF=[0,0] \
+#             model.ARCH=distill_unet \
+#             model.DISTILL=False \
+#             model.batch_size=16 \
+#             model.max_epoch=100 \
+#             experiment.note=S2_pretrain_B4812
+
+# # sbatch run_on_geoinfo/run_distill_unet.sh
+# echo "-------------------- distill-unet: DISTILL ------------------------"
+python3 main_s1s2_distill_unet.py \
+            --config-name=distill_unet.yaml \
+            RAND.SEED=0 \
+            RAND.DETERMIN=False \
+            DATA.SATELLITES=['S1','S2'] \
+            DATA.STACKING=True \
+            DATA.INPUT_BANDS.S1=['ND','VH','VV'] \
+            DATA.INPUT_BANDS.S2=['B4','B8','B12'] \
+            MODEL.ARCH=distill_unet \
+            MODEL.L2_NORM=False \
+            MODEL.USE_DECONV=True \
+            MODEL.WEIGHT_DECAY=0.01 \
+            MODEL.NUM_CLASSES=1 \
+            MODEL.LOSS_TYPE=DiceLoss \
+            MODEL.LOSS_COEF=[1,0,0] \
+            MODEL.LR_SCHEDULER=cosine \
+            MODEL.ACTIVATION=sigmoid \
+            MODEL.BATCH_SIZE=16 \
+            MODEL.MAX_EPOCH=100 \
+            EXP.NOTE=1_0_0_Jan15
+
+#rm -rf $SLURM_SUBMIT_DIR/*.log
+# rm -rf $SLURM_SUBMIT_DIR/*.out
+
+echo "finish"
+
+## run
+# sbatch --array=1-2 geo_run_s1s2_unet.sh
diff --git a/run_on_geoinfo/run_eval.sh b/run_on_geoinfo/run_eval.sh
new file mode 100644
index 0000000..17ab3fc
--- /dev/null
+++ b/run_on_geoinfo/run_eval.sh
@@ -0,0 +1,59 @@
+#!/bin/bash
+#SBATCH -N 1
+#SBATCH --gres=gpu:1
+#SBATCH --mem 36GB
+#SBATCH --cpus-per-task 8
+#SBATCH -t 7-00:00:00
+#SBATCH --job-name eval
+#SBATCH --output /home/p/u/puzhao/smp-seg-pytorch/run_logs/%x-%A_%a.out
+
+
+echo "start"
+echo "Starting job ${SLURM_JOB_ID} on ${SLURMD_NODENAME}"
+echo
+nvidia-smi
+. /geoinfo_vol1/puzhao/miniforge3/etc/profile.d/conda.sh
+
+# module --ignore-cache load "intel"
+# PROJECT_DIR=/cephyr/NOBACKUP/groups/snic2021-7-104/puzhao-snic-500G/smp-seg-pytorch
+
+
+# # Choose different config files
+# # DIRS=($(find /cephyr/NOBACKUP/groups/snic2021-7-104/puzhao-snic-500G/smp-seg-pytorch/config/s1s2_cfg/))
+# DIRS=($(find $PROJECT_DIR/config/s1s2_unet/))
+# DIRS=${DIRS[@]:1}
+# CFG=${DIRS[$SLURM_ARRAY_TASK_ID]}
+# echo "Running simulation $CFG"
+
+# # Choose different sensors
+# SAT=('S1' 'S2' 'ALOS')
+# SAT=('alos' 's1' 's2')
+# CFG=${SAT[$SLURM_ARRAY_TASK_ID]}
+# echo "Running simulation $CFG"
+# echo "python3 main_s1s2_unet.py s1s2_unet=$CFG model.batch_size=32"
+# echo "---------------------------------------------------------------------------------------------------------------"
+
+# singularity exec --nv /cephyr/users/puzhao/Alvis/PyTorch_v1.7.0-py3.sif python main_s1s2_unet.py s1s2_unet=$CFG
+conda activate pytorch
+PYTHONUNBUFFERED=1; 
+
+# sbatch run_on_geoinfo/run_eval.sh
+# python3 s1s2_evaluator.py \
+#     --config-name=distill_unet.yaml \
+#     model.ARCH=distill_unet \
+#     experiment.note=eval
+
+# sbatch run_on_geoinfo/run_eval.sh
+python3 s1s2_evaluator_prg.py \
+            --config-name=s1s2_cfg_prg.yaml \
+            DATA.SATELLITES=['S1'] \
+            DATA.INPUT_BANDS.S2=['B4','B8','B12'] \
+
+
+#rm -rf $SLURM_SUBMIT_DIR/*.log
+# rm -rf $SLURM_SUBMIT_DIR/*.out
+
+echo "finish"
+
+## run
+# sbatch --array=1-2 geo_run_s1s2_unet.sh
diff --git a/run_on_geoinfo/run_geoinfo_gpu.sh b/run_on_geoinfo/run_geoinfo_gpu.sh
new file mode 100644
index 0000000..8cc823d
--- /dev/null
+++ b/run_on_geoinfo/run_geoinfo_gpu.sh
@@ -0,0 +1,13 @@
+#!/bin/bash
+#SBATCH -N 1
+#SBATCH --gres=gpu:5
+#SBATCH --mem 36GB
+#SBATCH --cpus-per-task 1
+#SBATCH -t 7-00:00:00
+
+echo "start"
+nvidia-smi
+. /geoinfo_vol1/puzhao/miniforge3/etc/profile.d/conda.sh
+conda activate pytorch
+PYTHONUNBUFFERED=1; python3 test_geoinfo_gpu.py
+echo "finish"
diff --git a/run_on_geoinfo/run_segformer.sh b/run_on_geoinfo/run_segformer.sh
new file mode 100644
index 0000000..406e849
--- /dev/null
+++ b/run_on_geoinfo/run_segformer.sh
@@ -0,0 +1,21 @@
+#!/bin/bash
+#SBATCH -N 1
+#SBATCH --gres=gpu:1
+#SBATCH --mem 36GB
+#SBATCH --cpus-per-task 8
+#SBATCH -t 7-00:00:00
+#SBATCH --job-name segformer
+#SBATCH --output /home/p/u/puzhao/smp-seg-pytorch/run_logs/%x-%A_%a.out
+
+# sbatch run_on_geoinfo/run_segformer.sh
+python3 main_s1s2_unet.py \
+            --config-name=segformer.yaml \
+            data.satellites=['S1'] \
+            data.INPUT_BANDS.S2=['B4','B8','B12'] \
+            model.ARCH=SegFormer_B0 \
+            model.LOSS_TYPE=BCEWithLogitsLoss \
+            model.ACTIVATION=argmax2d \
+            model.DEBUG=False \
+            model.batch_size=16 \
+            model.max_epoch=100 \
+            experiment.note=BS16-BCE
\ No newline at end of file
diff --git a/run_on_geoinfo/run_siamunet.sh b/run_on_geoinfo/run_siamunet.sh
new file mode 100644
index 0000000..c8cd291
--- /dev/null
+++ b/run_on_geoinfo/run_siamunet.sh
@@ -0,0 +1,138 @@
+#!/bin/bash
+#SBATCH -N 1
+#SBATCH --gres=gpu:1
+#SBATCH --mem 36GB
+#SBATCH --cpus-per-task 8
+#SBATCH -t 7-00:00:00
+#SBATCH --job-name dual-unet
+#SBATCH --output /home/p/u/puzhao/run_logs/%x-%A_%a.out
+
+echo "start"
+echo "Starting job ${SLURM_JOB_ID} on ${SLURMD_NODENAME}"
+echo
+nvidia-smi
+. /geoinfo_vol1/puzhao/miniforge3/etc/profile.d/conda.sh
+
+# module --ignore-cache load "intel"
+# PROJECT_DIR=/cephyr/NOBACKUP/groups/snic2021-7-104/puzhao-snic-500G/smp-seg-pytorch
+
+# # Choose different config files
+# # DIRS=($(find /cephyr/NOBACKUP/groups/snic2021-7-104/puzhao-snic-500G/smp-seg-pytorch/config/s1s2_cfg/))
+# DIRS=($(find $PROJECT_DIR/config/s1s2_unet/))
+# DIRS=${DIRS[@]:1}
+# CFG=${DIRS[$SLURM_ARRAY_TASK_ID]}
+# echo "Running simulation $CFG"
+
+# # # Choose different sensors
+# SAT=('DualUnet_LF' 'SiamUnet_conc' 'SiamUnet_diff')
+# CFG=${SAT[$SLURM_ARRAY_TASK_ID]}
+# echo "Running simulation $CFG"
+# # echo "python3 main_s1s2_unet.py model.batch_size=32"
+# echo "---------------------------------------------------------------------------------------------------------------"
+
+# singularity exec --nv /cephyr/users/puzhao/Alvis/PyTorch_v1.7.0-py3.sif python main_s1s2_unet.py s1s2_unet=$CFG
+conda activate pytorch
+PYTHONUNBUFFERED=1; 
+
+##########################################################
+## ---- Same Siam-UNet for Single-Sensor Data ----
+##########################################################
+# sbatch --array=0-2 run_on_geoinfo/run_siamunet.sh
+
+# # Choose different sensors
+SAT=('S1' 'S2' 'ALOS')
+CFG=${SAT[$SLURM_ARRAY_TASK_ID]}
+echo "Running simulation $CFG"
+# echo "python3 main_s1s2_unet.py model.batch_size=32"
+echo "---------------------------------------------------------------------------------------------------------------"
+
+python3 main_s1s2_unet.py \
+            --config-name=siam_unet.yaml \
+            RAND.SEED=0 \
+            RAND.DETERMIN=False \
+            DATA.TRAIN_MASK=poly \
+            DATA.SATELLITES=[$CFG] \
+            DATA.STACKING=False \
+            DATA.INPUT_BANDS.S1=['ND','VH','VV'] \
+            DATA.INPUT_BANDS.S2=['B4','B8','B12'] \
+            MODEL.ARCH='DualUnet_LF' \
+            MODEL.SHARE_ENCODER=True \
+            MODEL.ENCODER=resnet18 \
+            MODEL.ENCODER_WEIGHTS=imagenet \
+            MODEL.USE_DECONV=False \
+            MODEL.WEIGHT_DECAY=0.01 \
+            MODEL.NUM_CLASSES=1 \
+            MODEL.LOSS_TYPE=DiceLoss \
+            MODEL.LR_SCHEDULER=cosine \
+            MODEL.ACTIVATION=sigmoid \
+            MODEL.BATCH_SIZE=16 \
+            MODEL.MAX_EPOCH=100 \
+            EXP.NOTE=LF
+
+##########################################################
+## ---- Different Architecture for Multi-Sensor Data ----
+##########################################################
+# sbatch --array=0-1 run_on_geoinfo/run_siamunet.sh
+
+# # Choose different sensors
+# SAT=('DualUnet_LF' 'SiamUnet_conc' 'SiamUnet_diff')
+# CFG=${SAT[$SLURM_ARRAY_TASK_ID]}
+# echo "Running simulation $CFG"
+# # echo "python3 main_s1s2_unet.py model.batch_size=32"
+# echo "---------------------------------------------------------------------------------------------------------------"
+
+# python3 main_s1s2_unet.py \
+#             --config-name=siam_unet.yaml \
+#             RAND.SEED=0 \
+#             RAND.DETERMIN=False \
+#             DATA.TRAIN_MASK=poly \
+#             DATA.SATELLITES=['S1','S2'] \
+#             DATA.STACKING=True \
+#             DATA.INPUT_BANDS.S1=['ND','VH','VV'] \
+#             DATA.INPUT_BANDS.S2=['B4','B8','B12'] \
+#             MODEL.ARCH=$CFG \
+#             MODEL.SHARE_ENCODER=True \
+#             MODEL.ENCODER=resnet18 \
+#             MODEL.ENCODER_WEIGHTS=imagenet \
+#             MODEL.USE_DECONV=False \
+#             MODEL.WEIGHT_DECAY=0.01 \
+#             MODEL.NUM_CLASSES=1 \
+#             MODEL.LOSS_TYPE=DiceLoss \
+#             MODEL.LR_SCHEDULER=cosine \
+#             MODEL.ACTIVATION=sigmoid \
+#             MODEL.BATCH_SIZE=16 \
+#             MODEL.MAX_EPOCH=100 \
+#             EXP.NOTE=Feb26
+
+
+# python3 main_s1s2_unet.py \
+#             --config-name=siam_unet.yaml \
+#             RAND.SEED=0 \
+#             RAND.DETERMIN=False \
+#             DATA.SATELLITES=['S1','S2'] \
+#             DATA.STACKING=True \
+#             DATA.INPUT_BANDS.S1=['ND','VH','VV'] \
+#             DATA.INPUT_BANDS.S2=['B4','B8','B12'] \
+#             MODEL.ARCH=$CFG \
+#             MODEL.SHARE_ENCODER=True \
+#             MODEL.ENCODER=resnet18 \
+#             MODEL.ENCODER_WEIGHTS=imagenet \
+#             MODEL.USE_DECONV=False \
+#             MODEL.WEIGHT_DECAY=0.01 \
+#             MODEL.NUM_CLASSES=1 \
+#             MODEL.LOSS_TYPE=DiceLoss \
+#             MODEL.LR_SCHEDULER=cosine \
+#             MODEL.ACTIVATION=sigmoid \
+#             MODEL.BATCH_SIZE=16 \
+#             MODEL.MAX_EPOCH=100 \
+#             EXP.NOTE=Jan15
+
+
+
+#rm -rf $SLURM_SUBMIT_DIR/*.log
+# rm -rf $SLURM_SUBMIT_DIR/*.out
+
+echo "finish"
+
+## run
+# sbatch --array=1-2 geo_run_s1s2_unet.sh
diff --git a/run_on_geoinfo/run_unet.sh b/run_on_geoinfo/run_unet.sh
new file mode 100644
index 0000000..3ff581e
--- /dev/null
+++ b/run_on_geoinfo/run_unet.sh
@@ -0,0 +1,298 @@
+#!/bin/bash
+#SBATCH -N 1
+#SBATCH --gres=gpu:1
+#SBATCH --mem 36GB
+#SBATCH --cpus-per-task 1
+#SBATCH -t 7-00:00:00
+#SBATCH --job-name train
+#SBATCH --output /home/p/u/puzhao/run_logs/%x-%A_%a.out
+
+echo "start"
+echo "Starting job ${SLURM_JOB_ID} on ${SLURMD_NODENAME}"
+echo
+nvidia-smi
+. /geoinfo_vol1/puzhao/miniforge3/etc/profile.d/conda.sh
+
+conda activate pytorch
+PYTHONUNBUFFERED=1; 
+
+# module --ignore-cache load "intel"
+# PROJECT_DIR=/cephyr/NOBACKUP/groups/snic2021-7-104/puzhao-snic-500G/smp-seg-pytorch
+
+# # Choose different config files
+# # DIRS=($(find /cephyr/NOBACKUP/groups/snic2021-7-104/puzhao-snic-500G/smp-seg-pytorch/config/s1s2_cfg/))
+# DIRS=($(find $PROJECT_DIR/config/s1s2_unet/))
+# DIRS=${DIRS[@]:1}
+# CFG=${DIRS[$SLURM_ARRAY_TASK_ID]}
+# echo "Running simulation $CFG"
+
+
+## sbatch --array=0-2 run_on_geoinfo/run_unet.sh
+# SAT=('S1' 'S2', 'ALOS')
+# CFG=${SAT[$SLURM_ARRAY_TASK_ID]}
+# echo "Running simulation $CFG"
+# echo "---------------------------------------------------------------------------------------------------------------"
+
+# python3 test.py
+
+# python3 main_s1s2_unet.py \
+#             --config-name=unet.yaml \
+#             RAND.SEED=0 \
+#             RAND.DETERMIN=False \
+#             DATA.TRAIN_MASK=poly \
+#             DATA.SATELLITES=[$CFG] \
+#             DATA.PREPOST=['pre','post'] \
+#             DATA.STACKING=True \
+#             DATA.INPUT_BANDS.S2=['B4','B8','B12'] \
+#             MODEL.ARCH=UNet \
+#             MODEL.USE_DECONV=False \
+#             MODEL.WEIGHT_DECAY=0.01 \
+#             MODEL.NUM_CLASSES=1 \
+#             MODEL.LOSS_TYPE=DiceLoss \
+#             MODEL.LR_SCHEDULER=cosine \
+#             MODEL.ACTIVATION=sigmoid \
+#             MODEL.BATCH_SIZE=16 \
+#             MODEL.MAX_EPOCH=100 \
+#             EXP.NOTE=EF-no-deconv
+
+
+
+##########################################################
+## ---- U-Net multiple runs with different seeds ----
+##########################################################
+## sbatch --array=0-5 run_on_geoinfo/run_unet.sh
+CFG=$SLURM_ARRAY_TASK_ID
+echo "Running simulation $CFG"
+echo "---------------------------------------------------------------------------------------------------------------"
+
+python3 main_s1s2_unet.py \
+            --config-name=unet.yaml \
+            RAND.SEED=$CFG \
+            RAND.DETERMIN=False \
+            DATA.TRAIN_MASK=poly \
+            DATA.SATELLITES=['S1'] \
+            DATA.STACKING=True \
+            MODEL.ARCH=UNet \
+            MODEL.ENCODER=resnet18 \
+            MODEL.ENCODER_WEIGHTS=imagenet \
+            MODEL.USE_DECONV=False \
+            MODEL.WEIGHT_DECAY=0.01 \
+            MODEL.NUM_CLASSES=2 \
+            MODEL.LOSS_TYPE=CrossEntropyLoss \
+            MODEL.LR_SCHEDULER=cosine \
+            MODEL.ACTIVATION=softmax2d \
+            MODEL.BATCH_SIZE=16 \
+            MODEL.MAX_EPOCH=5 \
+            EXP.FOLDER=Canada_RSE_2022 \
+            EXP.NOTE=EF
+
+
+##########################################################
+## ---- U-Net MTBS ----
+##########################################################
+# sbatch run_on_geoinfo/run_unet.sh
+# python3 main_s1s2_unet_mtbs.py \
+#             --config-name=mtbs.yaml \
+#             RAND.SEED=0 \
+#             RAND.DETERMIN=False \
+#             DATA.AUGMENT=True \
+#             DATA.TRAIN_MASK=mtbs \
+#             DATA.TEST_MASK=mtbs \
+#             DATA.SATELLITES=['S2'] \
+#             DATA.PREPOST=['pre','post'] \
+#             DATA.STACKING=True \
+#             DATA.INPUT_BANDS.S2=['B4','B8','B12'] \
+#             MODEL.ARCH=UNet_dualHeads \
+#             MODEL.CLASS_WEIGHTS=[0.1,0.2,0.3,0.4] \
+#             MODEL.USE_DECONV=False \
+#             MODEL.WEIGHT_DECAY=0.01 \
+#             MODEL.LR_SCHEDULER=cosine \
+#             MODEL.BATCH_SIZE=32 \
+#             MODEL.MAX_EPOCH=100 \
+#             MODEL.STEP_WISE_LOG=False \
+#             EXP.NOTE=mtbs-mse-x10
+
+##########################################################
+## ---- U-Net MTBS 2 Classes ----
+##########################################################
+# sbatch run_on_geoinfo/run_unet.sh
+# python3 main_s1s2_unet_mtbs.py \
+#             --config-name=mtbs.yaml \
+#             RAND.SEED=0 \
+#             RAND.DETERMIN=False \
+#             DATA.TRAIN_MASK=poly \
+#             DATA.TRAIN_MASK=poly \
+#             DATA.SATELLITES=['S2'] \
+#             DATA.PREPOST=['pre','post'] \
+#             DATA.STACKING=True \
+#             DATA.INPUT_BANDS.S2=['B4','B8','B12'] \
+#             MODEL.ARCH=UNet \
+#             MODEL.NUM_CLASSES=2 \
+#             MODEL.CLASS_NAMES=['unburn','low'] \
+#             MODEL.CLASS_WEIGHTS=[0.5,0.5] \
+#             MODEL.USE_DECONV=True \
+#             MODEL.WEIGHT_DECAY=0.01 \
+#             MODEL.LR_SCHEDULER=cosine \
+#             MODEL.BATCH_SIZE=16 \
+#             MODEL.MAX_EPOCH=100 \
+#             EXP.NOTE=poly-ep100
+            
+
+
+
+
+##########################################################
+## ---- U-Net with Data Augmentation ----
+##########################################################
+# sbatch run_on_geoinfo/run_unet.sh
+# python3 main_s1s2_unet_aug.py \
+#             --config-name=unet.yaml \
+#             RAND.SEED=0 \
+#             RAND.DETERMIN=False \
+#             DATA.SATELLITES=['ALOS'] \
+#             DATA.TRAIN_MASK=poly \
+#             DATA.AUGMENT=False \
+#             MODEL.ARCH=UNet \
+#             MODEL.ENCODER=resnet18 \
+#             MODEL.ENCODER_WEIGHTS=imagenet \
+#             MODEL.USE_DECONV=False \
+#             MODEL.LEARNING_RATE=0.0001 \
+#             MODEL.WEIGHT_DECAY=0.01 \
+#             MODEL.NUM_CLASSES=1 \
+#             MODEL.LOSS_TYPE=DiceLoss \
+#             MODEL.LR_SCHEDULER=cosine \
+#             MODEL.ACTIVATION=sigmoid \
+#             MODEL.BATCH_SIZE=16 \
+#             MODEL.MAX_EPOCH=100 \
+#             EXP.NOTE=EF-aug
+
+
+##########################################################
+## ---- Weakly Supervised Learning ----
+##########################################################
+# sbatch run_on_geoinfo/run_unet.sh
+# python3 main_s1s2_unet_wsl.py \
+#             --config-name=unet.yaml \
+#             RAND.SEED=0 \
+#             RAND.DETERMIN=False \
+#             DATA.SATELLITES=['ALOS'] \
+#             DATA.INPUT_BANDS.S2=['B8','B11','B12'] \
+#             DATA.TRAIN_MASK=modis \
+#             DATA.AUGMENT=True \
+#             MODEL.ARCH=UNet \
+#             MODEL.ENCODER=resnet18 \
+#             MODEL.ENCODER_WEIGHTS=imagenet \
+#             MODEL.USE_DECONV=False \
+#             MODEL.LEARNING_RATE=0.0001 \
+#             MODEL.WEIGHT_DECAY=0.01 \
+#             MODEL.NUM_CLASSES=1 \
+#             MODEL.LOSS_TYPE=DiceLoss \
+#             MODEL.LR_SCHEDULER=cosine \
+#             MODEL.ACTIVATION=sigmoid \
+#             MODEL.BATCH_SIZE=16 \
+#             MODEL.MAX_EPOCH=100 \
+#             EXP.NOTE=EF-aug
+
+
+##########################################################
+## ---- Early Fusion for Single Bands for S1 and ALOS ----
+##########################################################          
+# # sbatch --array=0-2 run_on_geoinfo/run_unet.sh
+# SAT=('ND' 'VH' 'VV')
+# CFG=${SAT[$SLURM_ARRAY_TASK_ID]}
+# echo "Running simulation $CFG"
+# echo "---------------------------------------------------------------------------------------------------------------"
+
+# python3 main_s1s2_unet.py \
+#             --config-name=unet.yaml \
+#             RAND.SEED=0 \
+#             RAND.DETERMIN=False \
+#             DATA.TRAIN_MASK=poly \
+#             DATA.SATELLITES=['ALOS'] \
+#             DATA.INPUT_BANDS.ALOS=[$CFG] \
+#             MODEL.ARCH=UNet \
+#             MODEL.ENCODER=resnet18 \
+#             MODEL.ENCODER_WEIGHTS=imagenet \
+#             MODEL.USE_DECONV=True \
+#             MODEL.WEIGHT_DECAY=0.01 \
+#             MODEL.NUM_CLASSES=1 \
+#             MODEL.LOSS_TYPE=DiceLoss \
+#             MODEL.LR_SCHEDULER=cosine \
+#             MODEL.ACTIVATION=sigmoid \
+#             MODEL.BATCH_SIZE=16 \
+#             MODEL.MAX_EPOCH=100 \
+#             EXP.NOTE=$CFG
+
+
+
+##########################################################
+## ---- Early Fusion of Different Sensors ----
+##########################################################
+## sbatch --array=0-2 run_on_geoinfo/run_unet.sh
+# SAT=('S1' 'S2')
+# CFG=${SAT[$SLURM_ARRAY_TASK_ID]}
+# echo "Running simulation $CFG"
+# echo "---------------------------------------------------------------------------------------------------------------"
+
+# python3 main_s1s2_unet.py \
+#             --config-name=unet.yaml \
+#             RAND.SEED=0 \
+#             RAND.DETERMIN=False \
+#             DATA.TRAIN_MASK=poly \
+#             DATA.SATELLITES=[$CFG,'ALOS'] \
+#             DATA.PREPOST=['pre','post'] \
+#             DATA.STACKING=True \
+#             DATA.INPUT_BANDS.S2=['B4','B8','B12'] \
+#             MODEL.ARCH=UNet \
+#             MODEL.ENCODER=resnet18 \
+#             MODEL.ENCODER_WEIGHTS=imagenet \
+#             MODEL.USE_DECONV=True \
+#             MODEL.WEIGHT_DECAY=0.01 \
+#             MODEL.NUM_CLASSES=1 \
+#             MODEL.LOSS_TYPE=DiceLoss \
+#             MODEL.LR_SCHEDULER=cosine \
+#             MODEL.ACTIVATION=sigmoid \
+#             MODEL.BATCH_SIZE=16 \
+#             MODEL.MAX_EPOCH=100 \
+#             EXP.NOTE=EF
+
+
+
+##########################################################
+## --- Use Post-Fire Data alone for single sensor ---
+##########################################################
+## sbatch --array=0-2 run_on_geoinfo/run_unet.sh
+# SAT=('S1' 'S2' 'ALOS')
+# CFG=${SAT[$SLURM_ARRAY_TASK_ID]}
+# echo "Running simulation $CFG"
+# echo "---------------------------------------------------------------------------------------------------------------"
+
+# python3 main_s1s2_unet.py \
+#             --config-name=unet.yaml \
+#             RAND.SEED=0 \
+#             RAND.DETERMIN=False \
+#             DATA.TRAIN_MASK=poly \
+#             DATA.SATELLITES=[$CFG] \
+#             DATA.PREPOST=['post'] \
+#             DATA.STACKING=False \
+#             DATA.INPUT_BANDS.S2=['B4','B8','B12'] \
+#             MODEL.ARCH=UNet \
+#             MODEL.ENCODER=resnet18 \
+#             MODEL.ENCODER_WEIGHTS=imagenet \
+#             MODEL.USE_DECONV=True \
+#             MODEL.WEIGHT_DECAY=0.01 \
+#             MODEL.NUM_CLASSES=1 \
+#             MODEL.LOSS_TYPE=DiceLoss \
+#             MODEL.LR_SCHEDULER=cosine \
+#             MODEL.ACTIVATION=sigmoid \
+#             MODEL.BATCH_SIZE=16 \
+#             MODEL.MAX_EPOCH=100 \
+#             EXP.NOTE=post
+
+#rm -rf $SLURM_SUBMIT_DIR/*.log
+# rm -rf $SLURM_SUBMIT_DIR/*.out
+
+echo "finish"
+
+## run
+# sbatch --array=1-2 geo_run_s1s2_unet.sh
diff --git a/run_on_snic/run_eval.sh b/run_on_snic/run_eval.sh
new file mode 100644
index 0000000..bc9df05
--- /dev/null
+++ b/run_on_snic/run_eval.sh
@@ -0,0 +1,38 @@
+#!/bin/bash
+#SBATCH -A SNIC2021-7-104
+#SBATCH -N 1
+#SBATCH --gpus-per-node=T4:1 
+#SBATCH -t 7-00:00:00
+#SBATCH --job-name eval
+
+echo "start"
+
+# git clone -b multi-res-label https://github.com/puzhao89/temporal-consistency.git $TMPDIR/temporal-consistency
+
+# cd $TMPDIR/temporal-consistency
+# pwd
+
+# rsync -a $SLURM_SUBMIT_DIR/data_for_snic/data $TMPDIR/temporal-consistency/
+
+# ls $TMPDIR/temporal-consistency/data/
+
+# exp_dir=$SLURM_SUBMIT_DIR/tc4wildfire_outputs
+# mkdir $exp_dir
+
+# while sleep 20m
+# do
+#     rsync -a $TMPDIR/temporal-consistency/outputs/* $exp_dir
+# done &
+# LOOPPID=$!
+
+# singularity exec --nv /cephyr/users/puzhao/Alvis/PyTorch_v1.7.0-py3.sif python main_s1s2_unet.py
+
+singularity exec --nv /cephyr/users/puzhao/Alvis/PyTorch_v1.7.0-py3.sif python s1s2_evaluator.py 
+
+# rsync -a $TMPDIR/temporal-consistency/outputs/* $exp_dir
+# kill $LOOPPID
+
+rm -rf $SLURM_SUBMIT_DIR/*.log
+# rm -rf $SLURM_SUBMIT_DIR/*.out
+
+echo "finish"
\ No newline at end of file
diff --git a/run_on_snic/run_s1s2_fcnn4cd.sh b/run_on_snic/run_s1s2_fcnn4cd.sh
new file mode 100644
index 0000000..ae0a1e0
--- /dev/null
+++ b/run_on_snic/run_s1s2_fcnn4cd.sh
@@ -0,0 +1,50 @@
+#!/bin/bash
+#SBATCH -A SNIC2021-7-104
+#SBATCH -N 1
+#SBATCH --gpus-per-node=T4:1
+#SBATCH -t 7-00:00:00
+#SBATCH --job-name fcnn4cd
+
+echo "start"
+echo "Starting job ${SLURM_JOB_ID} on ${SLURMD_NODENAME}"
+nvidia-smi
+
+# git clone -b multi-res-label https://github.com/puzhao89/temporal-consistency.git $TMPDIR/temporal-consistency
+
+# cd $TMPDIR/temporal-consistency
+# pwd
+
+# rsync -a $SLURM_SUBMIT_DIR/data_for_snic/data $TMPDIR/temporal-consistency/
+
+# ls $TMPDIR/temporal-consistency/data/
+
+# exp_dir=$SLURM_SUBMIT_DIR/tc4wildfire_outputs
+# mkdir $exp_dir
+
+# while sleep 20m
+# do
+#     rsync -a $TMPDIR/temporal-consistency/outputs/* $exp_dir
+# done &
+# LOOPPID=$!
+
+CDC=("Paddle_unet" "Vanilla_unet" "SiamUnet_conc" "SiamUnet_diff")
+CFG=${CDC[$SLURM_ARRAY_TASK_ID]}
+echo "Running simulation $CFG"
+echo "singularity exec --nv /cephyr/users/puzhao/Alvis/PyTorch_v1.7.0-py3.sif python main_s1s2_fcnn4cd.py model.ARCH=$CFG"
+echo "---------------------------------------------------------------------------------------------------------------"
+
+# singularity exec --nv /cephyr/users/puzhao/Alvis/PyTorch_v1.7.0-py3.sif python3 main_s1s2_fcnn4cd.py data.satellites=['S2'] model.ARCH=$CFG model.max_epoch=20
+singularity exec --nv /cephyr/users/puzhao/Alvis/PyTorch_v1.7.0-py3.sif python3 puzhao-snic-500G/smp-seg-pytorch/fcnn4cd/unet_paddle.py
+
+# singularity exec --nv /cephyr/users/puzhao/Alvis/PyTorch_v1.7.0-py3.sif python3 s1s2_evaluator.py
+
+# rsync -a $TMPDIR/temporal-consistency/outputs/* $exp_dir
+# kill $LOOPPID
+
+rm -rf $SLURM_SUBMIT_DIR/*.log
+# rm -rf $SLURM_SUBMIT_DIR/*.out
+
+echo "finish"
+
+# # run
+# sbatch --array=0-2 run_s1s2_fcnn4cd.sh
\ No newline at end of file
diff --git a/run_on_snic/run_s1s2_fuse_unet.sh b/run_on_snic/run_s1s2_fuse_unet.sh
new file mode 100644
index 0000000..9da0b70
--- /dev/null
+++ b/run_on_snic/run_s1s2_fuse_unet.sh
@@ -0,0 +1,32 @@
+#!/bin/bash
+#SBATCH -A SNIC2021-7-104
+#SBATCH -N 1
+#SBATCH --gpus-per-node=V100:1
+#SBATCH -t 7-00:00:00
+#SBATCH --job-name fuse-unet
+
+echo "start"
+echo "Starting job ${SLURM_JOB_ID} on ${SLURMD_NODENAME}"
+nvidia-smi
+
+CDC=(0 0.01 0.1 1)
+CFG=${CDC[$SLURM_ARRAY_TASK_ID]}
+echo "Running simulation $CFG"
+echo "singularity exec --nv /cephyr/users/puzhao/Alvis/PyTorch_v1.7.0-py3.sif python main_s1s2_fuse_unet_V1.py model.cross_domain_coef=$CFG"
+echo "---------------------------------------------------------------------------------------------------------------"
+
+singularity exec --nv /cephyr/users/puzhao/Alvis/PyTorch_v1.7.0-py3.sif python main_s1s2_fuse_unet_V1.py model.cross_domain_coef=$CFG
+# singularity exec --nv /cephyr/users/puzhao/Alvis/PyTorch_v1.7.0-py3.sif python main_s1s2_fuse_unet_V1.py model.cross_domain_coef=0.1
+
+# singularity exec --nv /cephyr/users/puzhao/Alvis/PyTorch_v1.7.0-py3.sif python test.py 
+
+# rsync -a $TMPDIR/temporal-consistency/outputs/* $exp_dir
+# kill $LOOPPID
+
+rm -rf $SLURM_SUBMIT_DIR/*.log
+# rm -rf $SLURM_SUBMIT_DIR/*.out
+
+echo "finish"
+
+# # run
+# sbatch --array=0-2 run_s1s2_fuse_unet.sh
\ No newline at end of file
diff --git a/run_on_snic/run_s1s2_unet.sh b/run_on_snic/run_s1s2_unet.sh
new file mode 100644
index 0000000..b00f84c
--- /dev/null
+++ b/run_on_snic/run_s1s2_unet.sh
@@ -0,0 +1,38 @@
+#!/bin/bash
+#SBATCH -A SNIC2021-7-104
+#SBATCH -N 1
+#SBATCH --gpus-per-node=T4:1
+#SBATCH -t 7-00:00:00
+#SBATCH --job-name prepost
+
+echo "start"
+
+echo "Starting job ${SLURM_JOB_ID} on ${SLURMD_NODENAME}"
+nvidia-smi
+
+module --ignore-cache load "intel"
+PROJECT_DIR=/cephyr/NOBACKUP/groups/snic2021-7-104/puzhao-snic-500G/smp-seg-pytorch
+
+# # Choose different config files
+# # DIRS=($(find /cephyr/NOBACKUP/groups/snic2021-7-104/puzhao-snic-500G/smp-seg-pytorch/config/s1s2_cfg/))
+# DIRS=($(find $PROJECT_DIR/config/s1s2_unet/))
+# DIRS=${DIRS[@]:1}
+# CFG=${DIRS[$SLURM_ARRAY_TASK_ID]}
+# echo "Running simulation $CFG"
+
+# # Choose different sensors
+SAT=('S1' 'S2' 'ALOS')
+CFG=${SAT[$SLURM_ARRAY_TASK_ID]}
+echo "Running simulation $CFG"
+echo "singularity exec --nv /cephyr/users/puzhao/Alvis/PyTorch_v1.7.0-py3.sif python main_s1s2_unet.py s1s2_unet=$CFG"
+echo "---------------------------------------------------------------------------------------------------------------"
+
+singularity exec --nv /cephyr/users/puzhao/Alvis/PyTorch_v1.7.0-py3.sif python main_s1s2_unet.py 
+
+rm -rf $SLURM_SUBMIT_DIR/*.log
+# rm -rf $SLURM_SUBMIT_DIR/*.out
+
+echo "finish"
+
+## run
+# sbatch --array=0-2 run_s1s2_unet.sh
\ No newline at end of file
diff --git a/run.sh b/run_on_snic/s1s2_fuse_unet.sh
similarity index 89%
rename from run.sh
rename to run_on_snic/s1s2_fuse_unet.sh
index 1b65d67..4d1d03c 100644
--- a/run.sh
+++ b/run_on_snic/s1s2_fuse_unet.sh
@@ -4,9 +4,10 @@
 #SBATCH --gpus-per-node=V100:1 
 #SBATCH -t 7-00:00:00
 #SBATCH --job-name s1s2-unet
-#SBATCH --output s1s2-unet.out
+#SBATCH --output s1s2-fuse-unet.out
 
 echo "start"
+
 # git clone -b multi-res-label https://github.com/puzhao89/temporal-consistency.git $TMPDIR/temporal-consistency
 
 # cd $TMPDIR/temporal-consistency
@@ -25,7 +26,7 @@ echo "start"
 # done &
 # LOOPPID=$!
 
-singularity exec --nv /cephyr/users/puzhao/Alvis/PyTorch_v1.7.0-py3.sif python main_s1s2_wandb.py 
+singularity exec --nv /cephyr/users/puzhao/Alvis/PyTorch_v1.7.0-py3.sif python3 main_s1s2_fuse_unet.py 
 
 # rsync -a $TMPDIR/temporal-consistency/outputs/* $exp_dir
 # kill $LOOPPID
diff --git a/run_on_snic/s1s2_unet.sh b/run_on_snic/s1s2_unet.sh
new file mode 100644
index 0000000..c528bbb
--- /dev/null
+++ b/run_on_snic/s1s2_unet.sh
@@ -0,0 +1,38 @@
+#!/bin/bash
+#SBATCH -A SNIC2021-7-104
+#SBATCH -N 1
+#SBATCH --gpus-per-node=T4:1 
+#SBATCH -t 7-00:00:00
+#SBATCH --job-name tewt-unet
+
+echo "start"
+
+# git clone -b multi-res-label https://github.com/puzhao89/temporal-consistency.git $TMPDIR/temporal-consistency
+
+# cd $TMPDIR/temporal-consistency
+# pwd
+
+# rsync -a $SLURM_SUBMIT_DIR/data_for_snic/data $TMPDIR/temporal-consistency/
+
+# ls $TMPDIR/temporal-consistency/data/
+
+# exp_dir=$SLURM_SUBMIT_DIR/tc4wildfire_outputs
+# mkdir $exp_dir
+
+# while sleep 20m
+# do
+#     rsync -a $TMPDIR/temporal-consistency/outputs/* $exp_dir
+# done &
+# LOOPPID=$!
+
+# singularity exec --nv /cephyr/users/puzhao/Alvis/PyTorch_v1.7.0-py3.sif python main_s1s2_unet.py
+
+singularity exec --nv /cephyr/users/puzhao/Alvis/PyTorch_v1.7.0-py3.sif python s1s2_evaluator.py 
+
+# rsync -a $TMPDIR/temporal-consistency/outputs/* $exp_dir
+# kill $LOOPPID
+
+rm -rf $SLURM_SUBMIT_DIR/*.log
+# rm -rf $SLURM_SUBMIT_DIR/*.out
+
+echo "finish"
\ No newline at end of file
diff --git a/s1s2_evaluator.py b/s1s2_evaluator.py
index 477a5ff..54355ed 100644
--- a/s1s2_evaluator.py
+++ b/s1s2_evaluator.py
@@ -1,22 +1,24 @@
 from ntpath import join
 import os
+import random
 from cv2 import _InputArray_OPENGL_BUFFER
 import matplotlib.pyplot as plt
 from imageio import imread, imsave
 import torch
 import numpy as np
 from tqdm import tqdm
+
 from pathlib import Path
 import tifffile as tiff
-import smp
+from easydict import EasyDict as edict
+from smp.base.modules import Activation
 
 import matplotlib.pyplot as plt
 from matplotlib.colors import ListedColormap, LinearSegmentedColormap
-from utils.GeoTIFF import GeoTIFF
-geotiff = GeoTIFF()
+# from utils.GeoTIFF import GeoTIFF
+# geotiff = GeoTIFF()
 
 import wandb
-
 def image_padding(img, patchsize):
     def zero_padding(arr, patchsize):
         # print("zero_padding patchsize: {}".format(patchsize))
@@ -33,28 +35,53 @@ def image_padding(img, patchsize):
     img_pad = np.pad(img_pad0, ((0, 0), (padSize, padSize), (padSize, padSize)), mode='symmetric')
     return img_pad
 
+def get_band_index_dict(cfg):
+    ALL_BANDS = cfg.DATA.ALL_BANDS
+    INPUT_BANDS = cfg.DATA.INPUT_BANDS
+
+    def get_band_index(sat):
+        all_bands = list(ALL_BANDS[sat])
+        input_bands = list(INPUT_BANDS[sat])
+
+        band_index = []
+        for band in input_bands:
+            band_index.append(all_bands.index(band))
+        return band_index
+
+    band_index_dict = {}
+    for sat in ['S1', 'ALOS', 'S2']:
+        band_index_dict[sat] = get_band_index(sat)
+    
+    return band_index_dict
+
 def inference(model, test_dir, test_id, cfg):
 
-    patchsize = cfg.eval.patchsize
-    NUM_CLASS = len(list(cfg.data.CLASSES))
+    patchsize = cfg.EVAL.PATCHSIZE
+    # NUM_CLASS = cfg.MODEL.NUM_CLASSES
     # model.cpu()
-    model.to("cuda")
-    test_id = test_id.split("_")[0]
 
-    input_tensors = []
-    for sat in cfg.data.satellites:
+    if torch.cuda.is_available():
+        model.to("cuda")
+
+    ''' read input data '''
+    input_tensors = []  # [(S1_pre, S1_post), (S2_pre, S2_post), ...]
+    for sat in cfg.DATA.SATELLITES:
         
         post_url = test_dir / sat / "post" / f"{test_id}.tif"
         post_image = tiff.imread(post_url) # C*H*W
+        post_image = post_image[cfg.band_index_dict[sat],] # select bands
+
         if sat in ['S1', 'ALOS']: post_image = (np.clip(post_image, -30, 0) + 30) / 30
         post_image_pad = image_padding(post_image, patchsize)
 
         # img_preprocessed = self.preprocessing_fn(img_pad)
         post_image_tensor = torch.from_numpy(post_image_pad).unsqueeze(0) # n * C * H * W
         
-        if 'pre' in cfg.data.prepost:
+        if 'pre' in cfg.DATA.PREPOST:
             pre_url = test_dir / sat / "pre" / f"{test_id}.tif"
-            pre_image = tiff.imread(pre_url) 
+            pre_image = tiff.imread(pre_url)
+            pre_image = pre_image[cfg.band_index_dict[sat],] # select bands 
+
             if sat in ['S1', 'ALOS']: pre_image = (np.clip(pre_image, -30, 0) + 30) / 30
 
             pre_image_pad = image_padding(pre_image, patchsize)
@@ -63,13 +90,14 @@ def inference(model, test_dir, test_id, cfg):
             input_tensors.append((pre_image_tensor, post_image_tensor))
 
         else:
-            input_tensors.append(post_image_tensor)
+            input_tensors.append((post_image_tensor, ))
 
     C, H, W = post_image.shape
-    _, _, Height, Width = input_tensors[0][0].shape
-    pred_mask_pad = np.zeros((Height, Width))
-    prob_mask_pad = np.zeros((NUM_CLASS, Height, Width))
+    _, _, Height, Width = post_image_tensor.shape
+    pred_mask_pad = np.zeros((Height, Width)) #HxW
+    prob_mask_pad = np.zeros((Height, Width)) #HxW
 
+    ''' tile-wise inference '''
     input_patchsize = 2 * patchsize
     padSize = int(patchsize/2) 
     for i in tqdm(range(0, Height - input_patchsize + 1, patchsize)):
@@ -79,11 +107,11 @@ def inference(model, test_dir, test_id, cfg):
             ''' ------------> tile input data <---------- '''
             input_patchs = []
             for sat_tensor in input_tensors:
-                post_patch = (sat_tensor[1][..., i:i+input_patchsize, j:j+input_patchsize]).type(torch.cuda.FloatTensor)
-                if 'pre' in cfg.data.prepost: 
+                post_patch = (sat_tensor[-1][..., i:i+input_patchsize, j:j+input_patchsize]).type(torch.cuda.FloatTensor)
+                if 'pre' in cfg.DATA.PREPOST: 
                     pre_patch = (sat_tensor[0][..., i:i+input_patchsize, j:j+input_patchsize]).type(torch.cuda.FloatTensor)
                     
-                    if cfg.data.stacking: 
+                    if cfg.DATA.STACKING: 
                         inputPatch = torch.cat([pre_patch, post_patch], dim=1) # stacked inputs
                         input_patchs.append(inputPatch)
                     else:
@@ -92,31 +120,56 @@ def inference(model, test_dir, test_id, cfg):
                     input_patchs.append(post_patch)
 
             ''' ------------> apply model <--------------- '''
-            if 'Fuse' in cfg.model.ARCH:
-                predPatch, _ = model.forward(input_patchs)
-            else:
-                predPatch = model.forward(inputPatch)
-            ''' ------------------------------------------ '''
+            # if 'UNet' == cfg.MODEL.ARCH:
+            #     # if len(cfg.DATA.SATELLITES) == 1: input = input_patchs[0] # single sensor
+            #     # else: input = torch.cat(input_patchs) # stack multi-sensor data
+            #     out = model.forward(input_patchs)
+
+            if 'distill_unet' == cfg.MODEL.ARCH:
+                if cfg.MODEL.DISTILL:
+                    out = model.forward(input_patchs[:1])[-1] # ONLY USE S1 sensor in distill mode.
+                else:
+                    out = model.forward(input_patchs)[-1] # USE all data in pretrain mode.
+
+            elif 'UNet_resnet' in cfg.MODEL.ARCH:
+                out = model.forward(torch.cat(input_patchs, dim=1))
 
-            predPatch = predPatch.squeeze().cpu().detach().numpy()#.round()
-            predLabel = np.argmax(predPatch, axis=0).squeeze()
+            # elif 'SiamResUNet' in cfg.MODEL.ARCH:
+            #     out, decoder_out = model.forward(input_patchs, False)
 
-            pred_mask_pad[i+padSize:i+padSize+patchsize, j+padSize:j+padSize+patchsize] = predLabel[padSize:padSize+patchsize, padSize:padSize+patchsize]  # need to modify
-            prob_mask_pad[:, i+padSize:i+padSize+patchsize, j+padSize:j+padSize+patchsize] = predPatch[:, padSize:padSize+patchsize, padSize:padSize+patchsize]  # need to modify
+            # elif 'cdc_unet' in cfg.MODEL.ARCH:
+            #     out, decoder_out = model.forward(input_patchs, False)
+            
+            else: # UNet, SiamUnet
+                # NEW: input_patchs should be a list or tuple, the last one is the wanted output.
+                out = model.forward(input_patchs)[-1] 
 
+            ''' ------------------------------------------ '''
+            activation = Activation(name=cfg.MODEL.ACTIVATION)
+            predPatch = activation(out) #NCWH for sigmoid, NWH for argmax, N=1, C=1
+            if 'sigmoid' == cfg.MODEL.ACTIVATION:
+                predLabel = np.round(predPatch.squeeze().cpu().detach().numpy()) # binarized with 0.5
+            else: # 'argmax'
+                predLabel = torch.argmax(predPatch, dim=1)
+                predLabel = predLabel.squeeze().cpu().detach().numpy()
+            
+            ''' save predicted tile '''
+            pred_mask_pad[i+padSize:i+padSize+patchsize, j+padSize:j+padSize+patchsize] = predLabel[padSize:padSize+patchsize, padSize:padSize+patchsize]
+
+    ''' clip back into original shape '''        
     pred_mask = pred_mask_pad[padSize:padSize+H, padSize:padSize+W] # clip back to original shape
-    prod_mask = prob_mask_pad[:, padSize:padSize+H, padSize:padSize+W] # clip back to original shape
+    # prod_mask = prob_mask_pad[padSize:padSize+H, padSize:padSize+W] # clip back to original shape
 
-    return pred_mask, prod_mask
+    return pred_mask
 
 def gen_errMap(grouthTruth, preMap, save_url=False):
     errMap = np.zeros(preMap.shape)
     # errMap[np.where((OptREF==0) & (SARREF==0))] = 0
-    errMap[np.where((grouthTruth==1) & (preMap==1))] = 1.0 # TP
-    errMap[np.where((grouthTruth==1) & (preMap==0))] = 2.0 # FN, green
-    errMap[np.where((grouthTruth==0) & (preMap==1))] = 3.0 # FP
+    errMap[np.where((grouthTruth==1) & (preMap==1))] = 1.0 # TP, dark red
+    errMap[np.where((grouthTruth==1) & (preMap==0))] = 2.0 # FN, light red
+    errMap[np.where((grouthTruth==0) & (preMap==1))] = 3.0 # FP, green
 
-    num_color = len(np.unique(errMap))
+    num_color = int(1 + max(np.unique(errMap)))
     # color_tuple = ([1,1,1], [0.6,0,0], [0,0.8,0], [1, 0.6, 0.6])
     color_tuple = ([1,1,1], [0.6,0,0], [1, 0.6, 0.6], [0,0.8,0])
     my_cmap = ListedColormap(color_tuple[:num_color])
@@ -134,66 +187,106 @@ def gen_errMap(grouthTruth, preMap, save_url=False):
 
 
 def apply_model_on_event(model, test_id, output_dir, cfg):
-
+    output_dir = Path(output_dir)
     output_dir.mkdir(exist_ok=True)
-    data_dir = Path(cfg.data.dir) / "test_images"
+    data_dir = Path(cfg.DATA.DIR) / "test_images"
 
-    orbKeyLen = len(test_id.split("_")[-1]) + 1 
-    event = test_id[:(len(test_id)-orbKeyLen)]
+    # orbKeyLen = len(test_id.split("_")[-1]) + 1 
+    # event = test_id[:-orbKeyLen]
+    event = test_id
     print(event)
 
     print(f"------------------> {test_id} <-------------------")
 
-    predMask, probMask = inference(model, data_dir, test_id, cfg)
-
+    predMask = inference(model, data_dir, event, cfg)
     print(f"predMask shape: {predMask.shape}, unique: {np.unique(predMask)}")
-    print(f"probMask: [{probMask.min()}, {probMask.max()}]")
+    # print(f"probMask: [{probMask.min()}, {probMask.max()}]")
 
     # # mtbs_palette =  ["000000", "006400","7fffd4","ffff00","ff0000","7fff00"]
     # # [0,100/255,0]
     # mtbs_palette = [[0,100/255,0], [127/255,1,212/255], [1,1,0], [1,0,0], [127/255,1,0], [1,1,1]]
 
-    plt.imsave(output_dir / f"{test_id}_predLabel.png", predMask, cmap='gray', vmin=0, vmax=1)
-
-        # read and save true labels
-    if os.path.isfile(data_dir / "mask" / "poly" / f"{event}.tif"):
-        _, _, trueLabel = geotiff.read(data_dir / "mask" / "poly" / f"{event}.tif")
-        geotiff.save(output_dir / f"{test_id}_predLabel.tif", predMask[np.newaxis,]) 
+    tiff.imsave(output_dir / f"{test_id}_pred.tif", predMask)
+    # imsave(output_dir / f"{test_id}_pred.png", predMask)
+    
+    # read and save true labels
+    if os.path.isfile(data_dir / "mask" / cfg.DATA.TEST_MASK / f"{event}.tif"):
+        trueLabel = tiff.imread(data_dir / "mask" / cfg.DATA.TEST_MASK / f"{event}.tif")
+        # _, _, trueLabel = geotiff.read(data_dir / "mask" / "poly" / f"{event}.tif")
+        # geotiff.save(output_dir / f"{test_id}_predLabel.tif", predMask[np.newaxis,]) 
 
         trueLabel = trueLabel.squeeze()
-        # print(trueLabel.shape, predMask.shape)
 
-        plt.imsave(output_dir / f"{test_id}_trueLabel.png", trueLabel, cmap='gray', vmin=0, vmax=1)
-        gen_errMap(trueLabel, predMask, save_url=output_dir / f"{test_id}_errMap.png")
+        # plt.imsave(output_dir / f"{test_id}_gts.png", trueLabel, cmap='gray', vmin=0, vmax=1)
+        gen_errMap(trueLabel, predMask, save_url=output_dir / f"{test_id}.png")
 
 
+def evaluate_model(cfg, model_url, output_dir):
 
-def evaluate_model(cfg, SegModel):
+    # import json
+    # json_url = Path(cfg.DATA.DIR) / "train_test.json"
+    # with open(json_url) as json_file:
+    #     split_dict = json.load(json_file)
+    # test_id_list = split_dict['test']['sarname']
 
-    import json
-    json_url = Path(cfg.data.dir) / "train_test.json"
-    with open(json_url) as json_file:
-        split_dict = json.load(json_file)
-    test_id_list = split_dict['test']['sarname']
+    test_id_list = os.listdir(Path(cfg.DATA.DIR) / "test_images" / "S2" / "post")
+    test_id_list = [test_id[:-4] for test_id in test_id_list]
+    print(test_id_list[0])
 
-    model = torch.load(SegModel.model_url)
-    output_dir = Path(SegModel.project_dir) / 'outputs'
+    model = torch.load(model_url, map_location=torch.device('cpu'))
+    # output_dir = Path(SegModel.project_dir) / 'outputs'
     output_dir.mkdir(exist_ok=True)
 
+    band_index_dict = get_band_index_dict(cfg)
+    cfg = edict(cfg)
+    cfg.update({"band_index_dict": band_index_dict})
+    
     for test_id in test_id_list:
         apply_model_on_event(model, test_id, output_dir, cfg)
 
 
+def set_random_seed(seed, deterministic=False):
+    """Set random seed.
+
+    Args:
+        seed (int): Seed to be used.
+        deterministic (bool): Whether to set the deterministic option for
+            CUDNN backend, i.e., set `torch.backends.cudnn.deterministic`
+            to True and `torch.backends.cudnn.benchmark` to False.
+            Default: False.
+    """
+    random.seed(seed)
+    np.random.seed(seed)
+    torch.manual_seed(seed)
+    torch.cuda.manual_seed_all(seed)
+    if deterministic:
+        torch.backends.cudnn.deterministic = True
+        torch.backends.cudnn.benchmark = False
 
 import hydra
 import wandb
 from omegaconf import DictConfig, OmegaConf
 
-@hydra.main(config_path="./config", config_name="s1s2_fusion")
+@hydra.main(config_path="./config", config_name="unet")
 def run_app(cfg : DictConfig) -> None:
-    print(OmegaConf.to_yaml(cfg))
 
-    wandb.init(config=cfg, project=cfg.project.name, name=cfg.experiment.name)
+    ''' set randome seed '''
+    os.environ['HYDRA_FULL_ERROR'] = str(1)
+    os.environ['PYTHONHASHSEED'] = str(cfg.RAND.SEED) #cfg.RAND.SEED
+    if cfg.RAND.DETERMIN:
+        os.environ['CUBLAS_WORKSPACE_CONFIG']=":4096:8" #https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility
+        torch.use_deterministic_algorithms(True)
+    set_random_seed(cfg.RAND.SEED, deterministic=cfg.RAND.DETERMIN)
+
+    # wandb.init(config=cfg, project=cfg.project.name, name=cfg.EXP.name)
+    import pandas as pd
+    from prettyprinter import pprint
+    
+    cfg_dict = OmegaConf.to_container(cfg, resolve=True)
+    cfg_flat = pd.json_normalize(cfg_dict, sep='.').to_dict(orient='records')[0]
+    wandb.init(config=cfg_flat, project=cfg.PROJECT.NAME, entity=cfg.PROJECT.ENTITY, name=cfg.EXP.NAME)
+    pprint(cfg_flat)
+
     # project_dir = Path(hydra.utils.get_original_cwd())
     #########################################################################
 
@@ -209,6 +302,17 @@ def run_app(cfg : DictConfig) -> None:
 
     # for test_id in test_id_list:
     #     apply_model_on_event(model, test_id, output_dir, satellites=['S1', 'S2'])
+
+    run_dir = Path("/home/p/u/puzhao/smp-seg-pytorch/outputs/run_s1s2_UNet_['S2']_post_20220226T084623")
+    model_url = run_dir / "model.pth"
+    output_dir = run_dir / "errMap"
+    evaluate_model(cfg, model_url, output_dir)
+
+    ''' compute IoU and F1 for all events '''
+    from utils.iou4all import compute_IoU_F1
+    compute_IoU_F1(phase="test_images", 
+                    result_dir=run_dir / "errMap", 
+                    dataset_dir=cfg.DATA.DIR)
     
     #########################################################################
     wandb.finish()
diff --git a/s1s2_evaluator_prg.py b/s1s2_evaluator_prg.py
new file mode 100644
index 0000000..bb9e9f5
--- /dev/null
+++ b/s1s2_evaluator_prg.py
@@ -0,0 +1,299 @@
+from ntpath import join
+import os, glob
+from cv2 import _InputArray_OPENGL_BUFFER
+import matplotlib.pyplot as plt
+from imageio import imread, imsave
+import torch
+import numpy as np
+from tqdm import tqdm
+from pathlib import Path
+import tifffile as tiff
+import smp
+from easydict import EasyDict as edict
+from smp.base.modules import Activation
+
+import matplotlib.pyplot as plt
+from matplotlib.colors import ListedColormap, LinearSegmentedColormap
+# from utils.GeoTIFF import GeoTIFF
+# geotiff = GeoTIFF()
+
+import wandb
+
+def image_padding(img, patchsize):
+    def zero_padding(arr, patchsize):
+        # print("zero_padding patchsize: {}".format(patchsize))
+        (c, h, w) = arr.shape
+        pad_h = (1 + np.floor(h/patchsize)) * patchsize - h
+        pad_w = (1 + np.floor(w/patchsize)) * patchsize - w
+
+        arr_pad = np.pad(arr, ((0, 0), (0, int(pad_h)), (0, int(pad_w))), mode='symmetric')
+        return arr_pad
+
+    padSize = int(patchsize/2)
+
+    img_pad0 = zero_padding(img, patchsize) # pad img into a shape: (m*PATCHSIZE, n*PATCHSIZE)
+    img_pad = np.pad(img_pad0, ((0, 0), (padSize, padSize), (padSize, padSize)), mode='symmetric')
+    return img_pad
+
+def get_band_index_dict(cfg):
+    ALL_BANDS = cfg.DATA.ALL_BANDS
+    INPUT_BANDS = cfg.DATA.INPUT_BANDS
+
+    def get_band_index(sat):
+        all_bands = list(ALL_BANDS[sat])
+        input_bands = list(INPUT_BANDS[sat])
+
+        band_index = []
+        for band in input_bands:
+            band_index.append(all_bands.index(band))
+        return band_index
+
+    band_index_dict = {}
+    for sat in ['S1', 'ALOS', 'S2']:
+        band_index_dict[sat] = get_band_index(sat)
+    
+    return band_index_dict
+
+def inference(model, test_dir, test_id, cfg):
+
+    patchsize = cfg.EVAL.PATCHSIZE
+    NUM_CLASS = cfg.MODEL.NUM_CLASSES
+    # model.cpu()
+    model.to("cuda")
+
+    input_tensors = []
+    for sat in cfg.DATA.SATELLITES:
+        
+        post_url = test_dir / sat / "post" / f"{test_id}.tif"
+
+        post_image = tiff.imread(post_url).transpose(2,0,1) # C*H*W
+        post_image = np.nan_to_num(post_image, 0)
+        # print(f"post: {post_image.shape}")
+        # print(post_image.min(), post_image.max())
+        post_image = post_image[cfg.band_index_dict[sat],] # select bands
+
+        if sat in ['S1', 'ALOS']: post_image = (np.clip(post_image, -30, 0) + 30) / 30
+        post_image_pad = image_padding(post_image, patchsize)
+
+        # img_preprocessed = self.preprocessing_fn(img_pad)
+        post_image_tensor = torch.from_numpy(post_image_pad).unsqueeze(0) # n * C * H * W
+        
+        if 'pre' in cfg.DATA.PREPOST:
+            pre_folder = test_dir / sat / "pre"
+            # pre_url = pre_folder / os.listdir(pre_folder)[0]
+            query = test_id.split("_")[-1]
+            pre_url = glob.glob(str(pre_folder / f"*_{query}.tif"))[0]
+            print("pre_image: ", os.path.split(pre_url)[-1])
+
+            pre_image = tiff.imread(pre_url).transpose(2,0,1)
+            pre_image = np.nan_to_num(pre_image, 0)
+            pre_image = pre_image[cfg.band_index_dict[sat],] # select bands 
+
+            if sat in ['S1', 'ALOS']: pre_image = (np.clip(pre_image, -30, 0) + 30) / 30
+
+            pre_image_pad = image_padding(pre_image, patchsize)
+            pre_image_tensor = torch.from_numpy(pre_image_pad).unsqueeze(0) # n * C * H * W
+        
+            input_tensors.append((pre_image_tensor, post_image_tensor))
+
+        else:
+            input_tensors.append(post_image_tensor)
+
+    C, H, W = post_image.shape
+    _, _, Height, Width = input_tensors[0][0].shape
+    pred_mask_pad = np.zeros((Height, Width))
+    # prob_mask_pad = np.zeros((NUM_CLASS, Height, Width))
+
+    input_patchsize = 2 * patchsize
+    padSize = int(patchsize/2) 
+    for i in tqdm(range(0, Height - input_patchsize + 1, patchsize)):
+        for j in range(0, Width - input_patchsize + 1, patchsize):
+            # print(i, i+input_patchsize, j, j+input_patchsize)
+
+            ''' ------------> tile input data <---------- '''
+            input_patchs = []
+            for sat_tensor in input_tensors:
+                post_patch = (sat_tensor[1][..., i:i+input_patchsize, j:j+input_patchsize]).type(torch.cuda.FloatTensor)
+                if 'pre' in cfg.DATA.PREPOST: 
+                    pre_patch = (sat_tensor[0][..., i:i+input_patchsize, j:j+input_patchsize]).type(torch.cuda.FloatTensor)
+                    
+                    if cfg.DATA.STACKING: 
+                        inputPatch = torch.cat([pre_patch, post_patch], dim=1) # stacked inputs
+                        input_patchs.append(inputPatch)
+                    else:
+                        input_patchs += [pre_patch, post_patch]
+                else:
+                    input_patchs.append(post_patch)
+
+            if 'distill_unet' == cfg.MODEL.ARCH:
+                if cfg.MODEL.DISTILL:
+                    out = model.forward(input_patchs[:1])[-1] # ONLY USE S1 sensor in distill mode.
+                else:
+                    out = model.forward(input_patchs)[-1] # USE all data in pretrain mode.
+
+            elif 'UNet_resnet' in cfg.MODEL.ARCH:
+                out = model.forward(torch.cat(input_patchs, dim=1))
+
+            # elif 'SiamResUNet' in cfg.MODEL.ARCH:
+            #     out, decoder_out = model.forward(input_patchs, False)
+
+            # elif 'cdc_unet' in cfg.MODEL.ARCH:
+            #     out, decoder_out = model.forward(input_patchs, False)
+            
+            else: # UNet, SiamUnet
+                # NEW: input_patchs should be a list or tuple, the last one is the wanted output.
+                out = model.forward(input_patchs)[-1] 
+
+            ''' ------------------------------------------ '''
+            activation = Activation(name=cfg.MODEL.ACTIVATION)
+            predPatch = activation(out) #NCWH for sigmoid, NWH for argmax, N=1, C=1
+            predPatch = predPatch.squeeze() #1x1xWxH or 1xWxH -> WxH
+
+            predPatch = predPatch.cpu().detach().numpy()
+            if 'sigmoid' == cfg.MODEL.ACTIVATION:
+                predLabel = np.round(predPatch) # binarized with 0.5
+            else: # 'argmax'
+                predLabel = predPatch
+            
+            ''' save predicted tile '''
+            # prob_mask_pad[i+padSize:i+padSize+patchsize, j+padSize:j+padSize+patchsize] = predPatch[padSize:padSize+patchsize, padSize:padSize+patchsize]
+            pred_mask_pad[i+padSize:i+padSize+patchsize, j+padSize:j+padSize+patchsize] = predLabel[padSize:padSize+patchsize, padSize:padSize+patchsize]
+
+    ''' clip back into original shape '''    
+    # prob_mask = prob_mask_pad[padSize:padSize+H, padSize:padSize+W] # clip back to original shape    
+    pred_mask = pred_mask_pad[padSize:padSize+H, padSize:padSize+W] # clip back to original shape
+
+    return pred_mask
+
+def gen_errMap(grouthTruth, preMap, save_url=False):
+    errMap = np.zeros(preMap.shape)
+    # errMap[np.where((OptREF==0) & (SARREF==0))] = 0
+    errMap[np.where((grouthTruth==1) & (preMap==1))] = 1.0 # TP, dark red
+    errMap[np.where((grouthTruth==1) & (preMap==0))] = 2.0 # FN, light red
+    errMap[np.where((grouthTruth==0) & (preMap==1))] = 3.0 # FP, green
+
+    num_color = int(1 + max(np.unique(errMap)))
+    # color_tuple = ([1,1,1], [0.6,0,0], [0,0.8,0], [1, 0.6, 0.6])
+    color_tuple = ([1,1,1], [0.6,0,0], [1, 0.6, 0.6], [0,0.8,0])
+    my_cmap = ListedColormap(color_tuple[:num_color])
+
+    # plt.figure(figsize=(15, 15))
+    # plt.imshow(errMap, cmap=my_cmap)
+
+    if save_url:
+        plt.imsave(save_url, errMap, cmap=my_cmap)
+
+        saveName = os.path.split(save_url)[-1].split('.')[0]
+        errMap_rgb = imread(save_url)
+        wandb.log({f"test_errMap/{saveName}": wandb.Image(errMap_rgb)})
+    return errMap
+
+
+def apply_model_on_event(model, test_id, output_dir, cfg):
+    output_dir = Path(output_dir)
+    output_dir.mkdir(exist_ok=True)
+    data_dir = Path(cfg.DATA.DIR) #/ "test_images"
+
+    # orbKeyLen = len(test_id.split("_")[-1]) + 1 
+    # event = test_id[:-orbKeyLen]
+    event = test_id
+    print(event)
+
+    print(f"------------------> {test_id} <-------------------")
+
+    predMask = inference(model, data_dir, event, cfg)
+
+    print(f"predMask shape: {predMask.shape}, unique: {np.unique(predMask)}")
+    # print(f"probMask: [{probMask.min()}, {probMask.max()}]")
+
+    # # mtbs_palette =  ["000000", "006400","7fffd4","ffff00","ff0000","7fff00"]
+    # # [0,100/255,0]
+    # mtbs_palette = [[0,100/255,0], [127/255,1,212/255], [1,1,0], [1,0,0], [127/255,1,0], [1,1,1]]
+
+    plt.imsave(output_dir / f"{test_id}_predLabel.png", predMask, cmap='gray', vmin=0, vmax=1)
+    # errMap_rgb = imread(save_url)
+    wandb.log({f"predMap/{test_id}": wandb.Image(predMask)})
+    # plt.imsave(output_dir / f"{test_id}_probMap.png", predMask, cmap='gray', vmin=0, vmax=1)
+
+    #     # read and save true labels
+    # if os.path.isfile(data_dir / "mask" / "poly" / f"{event}.tif"):
+    #     trueLabel = tiff.imread(data_dir / "mask" / "poly" / f"{event}.tif")
+    #     # _, _, trueLabel = geotiff.read(data_dir / "mask" / "poly" / f"{event}.tif")
+    #     # geotiff.save(output_dir / f"{test_id}_predLabel.tif", predMask[np.newaxis,]) 
+
+    #     trueLabel = trueLabel.squeeze()
+    #     # print(trueLabel.shape, predMask.shape)
+
+    #     plt.imsave(output_dir / f"{test_id}_trueLabel.png", trueLabel, cmap='gray', vmin=0, vmax=1)
+    #     gen_errMap(trueLabel, predMask, save_url=output_dir / f"{test_id}.png")
+
+def evaluate_model(cfg, model_url, output_dir):
+
+    # import json
+    # json_url = Path(cfg.data.dir) / "train_test.json"
+    # with open(json_url) as json_file:
+    #     split_dict = json.load(json_file)
+    # test_id_list = split_dict['test']['sarname']
+
+    test_id_list = [filename[:-4] for filename in os.listdir(Path(cfg.DATA.DIR) / "S1" / "post")]
+
+    model = torch.load(model_url)
+    # output_dir = Path(SegModel.project_dir) / 'outputs'
+    output_dir.mkdir(exist_ok=True)
+
+    band_index_dict = get_band_index_dict(cfg)
+    cfg = edict(cfg)
+    cfg.update({"band_index_dict": band_index_dict})
+    
+    for test_id in test_id_list:
+        apply_model_on_event(model, test_id, output_dir, cfg)
+
+
+
+import hydra
+import wandb
+from omegaconf import DictConfig, OmegaConf
+
+@hydra.main(config_path="./config", config_name="s1s2_cfg_prg")
+def run_app(cfg : DictConfig) -> None:
+    print(OmegaConf.to_yaml(cfg))
+
+    # wandb.init(config=cfg, project=cfg.project.name, name=cfg.experiment.name)
+    wandb.init(config=cfg, project=cfg.PROJECT.NAME, entity=cfg.PROJECT.ENTITY, name=cfg.EXP.NAME)
+
+    # project_dir = Path(hydra.utils.get_original_cwd())
+    #########################################################################
+
+    # # load test_id list
+    # import json
+    # json_url = "D:\wildfire-s1s2-dataset-ak-tiles/train_test.json"
+    # with open(json_url) as json_file:
+    #     split_dict = json.load(json_file)
+    # test_id_list = split_dict['test']['sarname']
+
+    # model = torch.load("G:/PyProjects/smp-seg-pytorch/outputs/best_model_mse.pth")
+    # output_dir = Path(f"G:/PyProjects/smp-seg-pytorch/outputs/test_output_mse")
+
+    # for test_id in test_id_list:
+    #     apply_model_on_event(model, test_id, output_dir, satellites=['S1', 'S2'])
+
+    model_url = "/home/p/u/puzhao/smp-seg-pytorch/outputs-igarss/run_s1s2_UNet_['S1']_EF_20220116T212807/model.pth"
+    # output_dir = Path("/cephyr/NOBACKUP/groups/snic2021-7-104/puzhao-snic-500G/smp-seg-pytorch/outputs") / "errMap"
+    output_dir = Path("/home/p/u/puzhao/wildfire-progression-dataset/CA_2021_Kamloops/outputs/pred_small")
+    evaluate_model(cfg, model_url, output_dir)
+    
+    #########################################################################
+    wandb.finish()
+
+if __name__ == "__main__":
+    
+    run_app()
+
+
+    # model = torch.load("G:/PyProjects/smp-seg-pytorch/outputs/best_model_s1s2.pth")
+    # output_dir = Path(f"G:/PyProjects/smp-seg-pytorch/outputs/test_output_s1s2_")
+
+    # for test_id in test_id_list:
+    #     apply_model_on_event(model, test_id, output_dir, satellites=['S1', 'S2'])
+
+    # scp -r puzhao@alvis1.c3se.chalmers.se:/cephyr/NOBACKUP/groups/snic2021-7-104/puzhao-snic-500G/CA_2021_Kamloops/S1/pre pre/
\ No newline at end of file
diff --git a/s1s2_evaluator_tile.py b/s1s2_evaluator_tile.py
new file mode 100644
index 0000000..729de65
--- /dev/null
+++ b/s1s2_evaluator_tile.py
@@ -0,0 +1,267 @@
+from ntpath import join
+import os
+from cv2 import _InputArray_OPENGL_BUFFER
+import matplotlib.pyplot as plt
+from imageio import imread, imsave
+import torch
+import numpy as np
+from tqdm import tqdm
+from pathlib import Path
+import tifffile as tiff
+import smp
+from easydict import EasyDict as edict
+
+import matplotlib.pyplot as plt
+from matplotlib.colors import ListedColormap, LinearSegmentedColormap
+# from utils.GeoTIFF import GeoTIFF
+# geotiff = GeoTIFF()
+
+import wandb
+
+def image_padding(img, patchsize):
+    def zero_padding(arr, patchsize):
+        # print("zero_padding patchsize: {}".format(patchsize))
+        (c, h, w) = arr.shape
+        pad_h = (1 + np.floor(h/patchsize)) * patchsize - h
+        pad_w = (1 + np.floor(w/patchsize)) * patchsize - w
+
+        arr_pad = np.pad(arr, ((0, 0), (0, int(pad_h)), (0, int(pad_w))), mode='symmetric')
+        return arr_pad
+
+    padSize = int(patchsize/2)
+
+    img_pad0 = zero_padding(img, patchsize) # pad img into a shape: (m*PATCHSIZE, n*PATCHSIZE)
+    img_pad = np.pad(img_pad0, ((0, 0), (padSize, padSize), (padSize, padSize)), mode='symmetric')
+    return img_pad
+
+def get_band_index_dict(cfg):
+    ALL_BANDS = cfg.data.ALL_BANDS
+    INPUT_BANDS = cfg.data.INPUT_BANDS
+
+    def get_band_index(sat):
+        all_bands = list(ALL_BANDS[sat])
+        input_bands = list(INPUT_BANDS[sat])
+
+        band_index = []
+        for band in input_bands:
+            band_index.append(all_bands.index(band))
+        return band_index
+
+    band_index_dict = {}
+    for sat in ['S1', 'ALOS', 'S2']:
+        band_index_dict[sat] = get_band_index(sat)
+    
+    return band_index_dict
+
+def inference(model, test_dir, test_id, cfg):
+
+    patchsize = cfg.eval.patchsize
+    NUM_CLASS = len(list(cfg.data.CLASSES))
+    # model.cpu()
+
+    if torch.cuda.is_available():
+        model.to("cuda")
+
+    input_tensors = []
+    for sat in cfg.data.satellites:
+        
+        post_url = test_dir / sat / "post" / f"{test_id}.tif"
+        post_image = tiff.imread(post_url) # C*H*W
+        post_image = post_image[cfg.band_index_dict[sat],] # select bands
+
+        if sat in ['S1', 'ALOS']: post_image = (np.clip(post_image, -30, 0) + 30) / 30
+        # post_image_pad = image_padding(post_image, patchsize)
+
+        # img_preprocessed = self.preprocessing_fn(img_pad)
+        post_image_tensor = torch.from_numpy(post_image).unsqueeze(0) # n * C * H * W
+        
+        if 'pre' in cfg.data.prepost:
+            pre_url = test_dir / sat / "pre" / f"{test_id}.tif"
+            pre_image = tiff.imread(pre_url)
+            pre_image = pre_image[cfg.band_index_dict[sat],] # select bands 
+
+            if sat in ['S1', 'ALOS']: pre_image = (np.clip(pre_image, -30, 0) + 30) / 30
+
+            # pre_image_pad = image_padding(pre_image, patchsize)
+            pre_image_tensor = torch.from_numpy(pre_image).unsqueeze(0) # n * C * H * W
+        
+            input_tensors.append((pre_image_tensor, post_image_tensor))
+
+        else:
+            input_tensors.append(post_image_tensor)
+
+    ''' ------------> tile input data <---------- '''
+    input_patchs = []
+    for sat_tensor in input_tensors:
+        post_patch = (sat_tensor[1]).type(torch.cuda.FloatTensor)
+        if 'pre' in cfg.data.prepost: 
+            pre_patch = (sat_tensor[0]).type(torch.cuda.FloatTensor)
+            
+            if cfg.data.stacking: 
+                inputPatch = torch.cat([pre_patch, post_patch], dim=1) # stacked inputs
+                input_patchs.append(inputPatch)
+            else:
+                input_patchs += [pre_patch, post_patch]
+        else:
+            input_patchs.append(post_patch)
+
+    ''' ------------> apply model <--------------- '''
+    if 'Paddle_unet' == cfg.model.ARCH:
+        predPatch = model.forward(input_patchs[0])
+
+    elif 'FuseUNet' in cfg.model.ARCH:
+        predPatch, decoder_out = model.forward(input_patchs, False)
+
+    elif 'cdc_unet' in cfg.model.ARCH:
+        predPatch, decoder_out = model.forward(input_patchs, False)
+    
+    else:
+        predPatch = model.forward(input_patchs)
+    ''' ------------------------------------------ '''
+
+    # predPatch = decoder_out[1].squeeze().cpu().detach().numpy()#.round()
+    # predLabel = 1 - np.argmax(predPatch, axis=0).squeeze()
+    
+    predPatch = torch.sigmoid(predPatch)
+    predPatch = predPatch.cpu().detach().numpy()#.round()
+    if predPatch.shape[0] > 1:
+        pred_mask = np.argmax(predPatch, axis=0).squeeze()
+        probility_mask = predPatch[1,]
+
+    else:
+        pred_mask = np.round(predPatch.squeeze())
+        probility_mask = predPatch.squeeze()
+    
+    return pred_mask, probility_mask
+
+def gen_errMap(grouthTruth, preMap, save_url=False):
+    errMap = np.zeros(preMap.shape)
+    # errMap[np.where((OptREF==0) & (SARREF==0))] = 0
+    errMap[np.where((grouthTruth==1) & (preMap==1))] = 1.0 # TP, dark red
+    errMap[np.where((grouthTruth==1) & (preMap==0))] = 2.0 # FN, light red
+    errMap[np.where((grouthTruth==0) & (preMap==1))] = 3.0 # FP, green
+
+    num_color = int(1 + max(np.unique(errMap)))
+    # color_tuple = ([1,1,1], [0.6,0,0], [0,0.8,0], [1, 0.6, 0.6])
+    color_tuple = ([1,1,1], [0.6,0,0], [1, 0.6, 0.6], [0,0.8,0])
+    my_cmap = ListedColormap(color_tuple[:num_color])
+
+    # plt.figure(figsize=(15, 15))
+    # plt.imshow(errMap, cmap=my_cmap)
+
+    if save_url:
+        plt.imsave(save_url, errMap, cmap=my_cmap)
+
+        saveName = os.path.split(save_url)[-1].split('.')[0]
+        errMap_rgb = imread(save_url)
+        wandb.log({f"test_errMap/{saveName}": wandb.Image(errMap_rgb)})
+    return errMap
+
+
+def apply_model_on_event(model, test_id, output_dir, cfg):
+    output_dir = Path(output_dir)
+    output_dir.mkdir(exist_ok=True)
+    data_dir = Path(cfg.data.dir) / "test"
+
+    # orbKeyLen = len(test_id.split("_")[-1]) + 1 
+    # event = test_id[:-orbKeyLen]
+    event = test_id
+    print(event)
+
+    print(f"------------------> {test_id} <-------------------")
+
+    predMask, probMask = inference(model, data_dir, event, cfg)
+
+    print(f"predMask shape: {predMask.shape}, unique: {np.unique(predMask)}")
+    print(f"probMask: [{probMask.min()}, {probMask.max()}]")
+
+    # # mtbs_palette =  ["000000", "006400","7fffd4","ffff00","ff0000","7fff00"]
+    # # [0,100/255,0]
+    # mtbs_palette = [[0,100/255,0], [127/255,1,212/255], [1,1,0], [1,0,0], [127/255,1,0], [1,1,1]]
+
+    plt.imsave(output_dir / f"{test_id}_predLabel.png", predMask, cmap='gray', vmin=0, vmax=1)
+    plt.imsave(output_dir / f"{test_id}_probMap.png", predMask, cmap='gray', vmin=0, vmax=1)
+
+        # read and save true labels
+    if os.path.isfile(data_dir / "mask" / "poly" / f"{event}.tif"):
+        trueLabel = tiff.imread(data_dir / "mask" / "poly" / f"{event}.tif")
+        # _, _, trueLabel = geotiff.read(data_dir / "mask" / "poly" / f"{event}.tif")
+        # geotiff.save(output_dir / f"{test_id}_predLabel.tif", predMask[np.newaxis,]) 
+
+        trueLabel = trueLabel.squeeze()
+        # print(trueLabel.shape, predMask.shape)
+
+        plt.imsave(output_dir / f"{test_id}_trueLabel.png", trueLabel, cmap='gray', vmin=0, vmax=1)
+        gen_errMap(trueLabel, predMask, save_url=output_dir / f"{test_id}.png")
+
+
+def evaluate_model(cfg, model_url, output_dir):
+
+    # import json
+    # json_url = Path(cfg.data.dir) / "train_test.json"
+    # with open(json_url) as json_file:
+    #     split_dict = json.load(json_file)
+    # test_id_list = split_dict['test']['sarname']
+
+    test_id_list = os.listdir(Path(cfg.data.dir) / "test" / "S2" / "post")
+    test_id_list = [test_id[:-4] for test_id in test_id_list]
+    print(test_id_list[0])
+
+    model = torch.load(model_url, map_location=torch.device('cpu'))
+    # output_dir = Path(SegModel.project_dir) / 'outputs'
+    output_dir.mkdir(exist_ok=True)
+
+    band_index_dict = get_band_index_dict(cfg)
+    cfg = edict(cfg)
+    cfg.update({"band_index_dict": band_index_dict})
+    
+    for test_id in test_id_list:
+        apply_model_on_event(model, test_id, output_dir, cfg)
+
+
+
+import hydra
+import wandb
+from omegaconf import DictConfig, OmegaConf
+
+@hydra.main(config_path="./config", config_name="s1s2_unet")
+def run_app(cfg : DictConfig) -> None:
+    print(OmegaConf.to_yaml(cfg))
+
+    # wandb.init(config=cfg, project=cfg.project.name, name=cfg.experiment.name)
+    wandb.init(config=cfg, project=cfg.project.name, entity=cfg.project.entity, name=cfg.experiment.name)
+
+    # project_dir = Path(hydra.utils.get_original_cwd())
+    #########################################################################
+
+    # # load test_id list
+    # import json
+    # json_url = "D:\wildfire-s1s2-dataset-ak-tiles/train_test.json"
+    # with open(json_url) as json_file:
+    #     split_dict = json.load(json_file)
+    # test_id_list = split_dict['test']['sarname']
+
+    # model = torch.load("G:/PyProjects/smp-seg-pytorch/outputs/best_model_mse.pth")
+    # output_dir = Path(f"G:/PyProjects/smp-seg-pytorch/outputs/test_output_mse")
+
+    # for test_id in test_id_list:
+    #     apply_model_on_event(model, test_id, output_dir, satellites=['S1', 'S2'])
+
+    run_dir = Path("/home/p/u/puzhao/smp-seg-pytorch/outputs/run_s1s2_Paddle_unet_resnet18_['S1']_V0_20211225T005247")
+    model_url = run_dir / "model.pth"
+    output_dir = run_dir / "errMap_train"
+    evaluate_model(cfg, model_url, output_dir)
+    
+    #########################################################################
+    wandb.finish()
+
+if __name__ == "__main__":
+    
+    run_app()
+
+
+    # model = torch.load("G:/PyProjects/smp-seg-pytorch/outputs/best_model_s1s2.pth")
+    # output_dir = Path(f"G:/PyProjects/smp-seg-pytorch/outputs/test_output_s1s2_")
+
+    # for test_id in test_id_list:
+    #     apply_model_on_event(model, test_id, output_dir, satellites=['S1', 'S2'])
\ No newline at end of file
diff --git a/main_camvid_smp.py b/scripts/main_camvid_smp.py
similarity index 100%
rename from main_camvid_smp.py
rename to scripts/main_camvid_smp.py
diff --git a/main_camvid_wandb.py b/scripts/main_camvid_wandb.py
similarity index 100%
rename from main_camvid_wandb.py
rename to scripts/main_camvid_wandb.py
diff --git a/main_mtbs_wandb.py b/scripts/main_mtbs.py
similarity index 100%
rename from main_mtbs_wandb.py
rename to scripts/main_mtbs.py
diff --git a/scripts/main_s1s2_SiamUnet_minDiff.py b/scripts/main_s1s2_SiamUnet_minDiff.py
new file mode 100644
index 0000000..069ee98
--- /dev/null
+++ b/scripts/main_s1s2_SiamUnet_minDiff.py
@@ -0,0 +1,319 @@
+
+import os, json
+import random
+from easydict import EasyDict as edict
+from pathlib import Path
+from prettyprinter import pprint
+from imageio import imread, imsave
+
+import hydra
+import wandb
+from omegaconf import DictConfig, OmegaConf
+
+
+###################################################################################
+import os, sys
+import numpy as np
+from pathlib import Path
+import hydra
+from omegaconf import DictConfig, OmegaConf
+
+import copy
+import time
+import torch
+import torch.optim as optim
+import torch.nn as nn
+import torch.nn.functional as F
+from easydict import EasyDict as edict
+
+from tqdm import tqdm as tqdm
+
+import logging
+logger = logging.getLogger(__name__)
+
+import smp
+from models.net_arch import init_model
+import wandb
+
+f_score = smp.utils.functional.f_score
+
+# Dice/F1 score - https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient
+# IoU/Jaccard score - https://en.wikipedia.org/wiki/Jaccard_index
+diceLoss = smp.utils.losses.DiceLoss(eps=1)
+AverageValueMeter =  smp.utils.train.AverageValueMeter
+
+def get_diff_loss(feat_diff, y): 
+    print(">>>>>>>>>>")
+    print(feat_diff.shape, y.shape)
+
+    mask = F.interpolate(y, feat_diff.shape)
+    fenzi = torch.mean(feat_diff * y)
+    fenmu = torch.mean(feat_diff * (1-y))
+    return fenzi / (fenmu + 1e-4)
+
+# Augmentations
+from dataset.augument import get_training_augmentation, \
+    get_validation_augmentation, get_preprocessing
+
+from torch.utils.data import DataLoader
+from dataset.wildfire import S1S2 as Dataset # ------------------------------------------------------- Dataset
+
+from models.lr_schedule import get_cosine_schedule_with_warmup
+
+
+def format_logs(logs):
+    str_logs = ['{}: {:.4}'.format(k, v) for k, v in logs.items()]
+    s = ', '.join(str_logs)
+    return s
+
+class SegModel(object):
+    def __init__(self, cfg) -> None:
+        super().__init__()
+        self.project_dir = Path(hydra.utils.get_original_cwd())
+
+        self.cfg = cfg
+        self.DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
+
+        self.model = init_model(cfg)
+        # self.model_url = str(self.project_dir / "outputs" / "best_model.pth")
+        self.rundir = self.project_dir / self.cfg.experiment.output
+        self.model_url = str( self.rundir / "model.pth")
+
+        self.preprocessing_fn = \
+            smp.encoders.get_preprocessing_fn(cfg.model.ENCODER, cfg.model.ENCODER_WEIGHTS)
+
+        self.metrics = [smp.utils.metrics.IoU(threshold=0.5),
+                        smp.utils.metrics.Fscore()
+                    ]
+
+        ''' -------------> need to improve <-----------------'''
+        # specify data folder
+        self.train_dir = Path(self.cfg.data.dir) / 'train'
+        self.valid_dir = Path(self.cfg.data.dir) / 'test'
+        '''--------------------------------------------------'''
+
+
+    def get_dataloaders(self) -> dict:
+
+        """ Data Preparation """
+        train_dataset = Dataset(
+            self.train_dir, 
+            self.cfg, 
+            # augmentation=get_training_augmentation(), 
+            # preprocessing=get_preprocessing(self.preprocessing_fn),
+            classes=self.cfg.data.CLASSES,
+        )
+
+        valid_dataset = Dataset(
+            self.valid_dir, 
+            self.cfg, 
+            # augmentation=get_validation_augmentation(), 
+            # preprocessing=get_preprocessing(self.preprocessing_fn),
+            classes=self.cfg.data.CLASSES,
+        )
+
+        train_size = int(len(train_dataset) * self.cfg.model.train_ratio)
+        valid_size = len(train_dataset) - train_size
+        train_set, val_set = torch.utils.data.random_split(train_dataset, [train_size, valid_size])
+
+        train_loader = DataLoader(train_set, batch_size=self.cfg.model.batch_size, shuffle=True, num_workers=4)
+        valid_loader = DataLoader(val_set, batch_size=self.cfg.model.batch_size, shuffle=True, num_workers=4)
+        test_loader = DataLoader(valid_dataset, batch_size=self.cfg.model.batch_size, shuffle=False, num_workers=4)
+
+        dataloaders = { 
+                        'train': train_loader, \
+                        'valid': valid_loader, \
+                        'test': test_loader, \
+
+                        'train_size': train_size, \
+                        'valid_size': valid_size, \
+                        'test_size': len(valid_dataset)
+                    }
+
+        return dataloaders
+
+
+    def run(self) -> None:
+        self.dataloaders = self.get_dataloaders()
+        self.optimizer = torch.optim.Adam([dict(
+                params=self.model.parameters(), 
+                lr=self.cfg.model.learning_rate, 
+                weight_decay=self.cfg.model.weight_decay)])
+
+        # lr scheduler
+        per_epoch_steps = self.dataloaders['train_size'] // self.cfg.model.batch_size
+        total_training_steps = self.cfg.model.max_epoch * per_epoch_steps
+        warmup_steps = self.cfg.model.warmup_coef * per_epoch_steps
+        if self.cfg.model.use_lr_scheduler:
+            self.lr_scheduler = get_cosine_schedule_with_warmup(self.optimizer, warmup_steps, total_training_steps)
+
+        self.history_logs = edict()
+        self.history_logs['train'] = []
+        self.history_logs['valid'] = []
+        self.history_logs['test'] = []
+
+        # --------------------------------- Train -------------------------------------------
+        max_score = self.cfg.model.max_score
+        for epoch in range(0, self.cfg.model.max_epoch):
+            epoch = epoch + 1
+            print(f"\n==> train epoch: {epoch}/{self.cfg.model.max_epoch}")
+            self.train_one_epoch(epoch)
+            valid_logs = self.valid_logs
+            
+            # do something (save model, change lr, etc.)
+            if valid_logs['iou_score'] > max_score:
+                max_score = valid_logs['iou_score']
+
+                if (1 == epoch) or (0 == epoch % self.cfg.model.save_interval):
+                    torch.save(self.model, self.model_url)
+                    # torch.save(self.model.state_dict(), self.model_url)
+                    print('Model saved!')
+
+            # if epoch % 50 == 0:
+            #     self.optimizer.param_groups[0]['lr'] = 0.1 * self.optimizer.param_groups[0]['lr']
+                        
+        
+    def train_one_epoch(self, epoch):
+        self.model.to(self.DEVICE)
+        
+        # wandb.
+        for phase in ['train', 'valid', 'test']:
+            if phase == 'train':
+                self.model.train()
+            else:
+                self.model.eval()
+
+            logs = self.step(phase) 
+            # print(phase, logs)
+
+            currlr = self.lr_scheduler.get_last_lr()[0] if self.cfg.model.use_lr_scheduler else self.optimizer.param_groups[0]['lr']          
+            wandb.log({phase: logs, 'epoch': epoch, 'lr': currlr})
+
+            temp = [logs["total_loss"]] + [logs[self.metrics[i].__name__] for i in range(0, len(self.metrics))]
+            self.history_logs[phase].append(temp)
+
+            if phase == 'valid': self.valid_logs = logs
+
+
+
+    def step(self, phase) -> dict:
+        logs = {}
+        loss_meter = AverageValueMeter()
+        diff_meter = AverageValueMeter()
+
+        metrics_meters = {metric.__name__: AverageValueMeter() for metric in self.metrics}
+
+        # if ('Train' in phase) and (self.cfg.useDataWoCAug):
+        #     dataLoader_woCAug = iter(self.dataloaders['Train_woCAug'])
+
+        with tqdm(iter(self.dataloaders[phase]), desc=phase, file=sys.stdout, disable=not self.cfg.model.verbose) as iterator:
+            for (x1, x2, y) in iterator:
+
+                # if ('Train' in phase) and (self.cfg.useDataWoCAug):
+                #     x0, y0 = next(dataLoader_woCAug)  
+                #     x = torch.cat((x0, x), dim=0)
+                #     y = torch.cat((y0, y), dim=0)
+
+                x1, x2, y = x1.to(self.DEVICE), x2.to(self.DEVICE), y.to(self.DEVICE)
+                self.optimizer.zero_grad()
+                self.optimizer.zero_grad()
+
+                returned = self.model.forward((x1, x2))
+                # print(returned)
+                y_pred, diff_tuple = returned[0], returned[1]
+                print("diff_tuple", diff_tuple)
+
+                dice_loss_ =  diceLoss(y_pred, y)
+                # focal_loss_ = self.cfg.alpha * focal_loss(y_pred, y)
+                # tv_loss_ = 1e-5 * self.cfg.beta * torch.mean(tv_loss(y_pred))
+
+                diff_loss = 0
+                if 'train' == phase:
+                    # compute the differnce between bi-temporal features
+                    for feat_diff in diff_tuple:
+                        diff_loss += get_diff_loss(feat_diff, y)
+
+                    loss_ = dice_loss_ + diff_loss
+
+                else:
+                    loss_ = dice_loss_
+
+                # update loss logs
+                loss_value = loss_.cpu().detach().numpy()
+                loss_meter.add(loss_value)
+
+                diff_value = diff_loss.cpu().detach().numpy()
+                diff_meter.add(diff_value)
+
+                # loss_logs = {criterion.__name__: loss_meter.mean}
+                loss_logs = {'total_loss': loss_meter.mean, "diff_loss": diff_meter.mean}
+                logs.update(loss_logs)
+
+                # update metrics logs
+                for metric_fn in self.metrics:
+                    metric_value = metric_fn(y_pred, y).cpu().detach().numpy()
+                    metrics_meters[metric_fn.__name__].add(metric_value)
+
+                metrics_logs = {k: v.mean for k, v in metrics_meters.items()}
+                logs.update(metrics_logs)
+                # print(logs)
+
+                if self.cfg.model.verbose:
+                    s = format_logs(logs)
+                    iterator.set_postfix_str(s)
+
+                if phase == 'train':
+                    loss_.backward()
+                    self.optimizer.step()
+
+                    if self.cfg.model.use_lr_scheduler:
+                        self.lr_scheduler.step()
+
+            return logs
+##############################################################
+
+
+
+def set_random_seed(seed, deterministic=False):
+    """Set random seed.
+
+    Args:
+        seed (int): Seed to be used.
+        deterministic (bool): Whether to set the deterministic option for
+            CUDNN backend, i.e., set `torch.backends.cudnn.deterministic`
+            to True and `torch.backends.cudnn.benchmark` to False.
+            Default: False.
+    """
+    random.seed(seed)
+    np.random.seed(seed)
+    torch.manual_seed(seed)
+    torch.cuda.manual_seed_all(seed)
+    if deterministic:
+        torch.backends.cudnn.deterministic = True
+        torch.backends.cudnn.benchmark = False
+
+
+@hydra.main(config_path="./config", config_name="s1s2_fcnn4cd")
+def run_app(cfg : DictConfig) -> None:
+    print(OmegaConf.to_yaml(cfg))
+
+    # wandb.init(config=cfg, project=cfg.project.name, name=cfg.experiment.name)
+    wandb.init(config=cfg, project=cfg.project.name, entity=cfg.project.entity, name=cfg.experiment.name)
+    # project_dir = Path(hydra.utils.get_original_cwd())
+    
+    # set randome seed
+    set_random_seed(cfg.data.SEED)
+
+    # from experiments.seg_model import SegModel
+    mySegModel = SegModel(cfg)
+    mySegModel.run()
+
+    # evaluation
+    from s1s2_evaluator import evaluate_model
+    evaluate_model(cfg, mySegModel.model_url, mySegModel.rundir / "errMap")
+    
+    wandb.finish()
+
+
+if __name__ == "__main__":
+    run_app()
diff --git a/scripts/main_s1s2_cdc_unet.py b/scripts/main_s1s2_cdc_unet.py
new file mode 100644
index 0000000..7cb336f
--- /dev/null
+++ b/scripts/main_s1s2_cdc_unet.py
@@ -0,0 +1,318 @@
+
+import os, json
+import random
+from easydict import EasyDict as edict
+from pathlib import Path
+from prettyprinter import pprint
+from imageio import imread, imsave
+
+import hydra
+import wandb
+from omegaconf import DictConfig, OmegaConf
+
+
+###################################################################################
+import os, sys
+import numpy as np
+from pathlib import Path
+import hydra
+from omegaconf import DictConfig, OmegaConf
+
+import copy
+import time
+import torch
+import torch.optim as optim
+import torch.nn as nn
+import torch.nn.functional as F
+from easydict import EasyDict as edict
+
+from tqdm import tqdm as tqdm
+
+import logging
+logger = logging.getLogger(__name__)
+
+import smp
+from models.net_arch import init_model
+import wandb
+
+f_score = smp.utils.functional.f_score
+
+# Dice/F1 score - https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient
+# IoU/Jaccard score - https://en.wikipedia.org/wiki/Jaccard_index
+diceLoss = smp.utils.losses.DiceLoss(eps=1)
+AverageValueMeter =  smp.utils.train.AverageValueMeter
+
+# Augmentations
+from dataset.augument import get_training_augmentation, \
+    get_validation_augmentation, get_preprocessing
+
+from torch.utils.data import DataLoader
+from dataset.wildfire import S1S2 as Dataset # ------------------------------------------------------- Dataset
+
+from models.lr_schedule import get_cosine_schedule_with_warmup
+mse_loss = nn.MSELoss(reduction='mean')
+
+# def mse_loss(input, target):
+#     input_sigmoid = torch.sigmoid(input)
+#     iflat = input_sigmoid.flatten()
+#     tflat = target.flatten()
+
+#     return nn.MSELoss()(iflat, tflat) / len(tflat)
+
+
+def format_logs(logs):
+    str_logs = ['{}: {:.4}'.format(k, v) for k, v in logs.items()]
+    s = ', '.join(str_logs)
+    return s
+
+class SegModel(object):
+    def __init__(self, cfg) -> None:
+        super().__init__()
+        self.project_dir = Path(hydra.utils.get_original_cwd())
+
+        self.cfg = cfg
+        self.DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
+
+        self.model = init_model(cfg)
+        # self.model_url = str(self.project_dir / "outputs" / "best_model.pth")
+        self.rundir = self.project_dir / self.cfg.experiment.output
+        self.model_url = str( self.rundir / "model.pth")
+
+        self.preprocessing_fn = \
+            smp.encoders.get_preprocessing_fn(cfg.model.ENCODER, cfg.model.ENCODER_WEIGHTS)
+
+        self.metrics = [smp.utils.metrics.IoU(threshold=0.5),
+                        smp.utils.metrics.Fscore(),
+                    ]
+
+        ''' -------------> need to improve <-----------------'''
+        # specify data folder
+        self.train_dir = Path(self.cfg.data.dir) / 'train'
+        self.valid_dir = Path(self.cfg.data.dir) / 'test'
+        
+        '''--------------------------------------------------'''
+
+
+    def get_dataloaders(self) -> dict:
+
+        """ Data Preparation """
+        train_dataset = Dataset(
+            self.train_dir, 
+            self.cfg, 
+            # augmentation=get_training_augmentation(), 
+            # preprocessing=get_preprocessing(self.preprocessing_fn),
+            classes=self.cfg.data.CLASSES,
+        )
+
+        valid_dataset = Dataset(
+            self.valid_dir, 
+            self.cfg, 
+            # augmentation=get_validation_augmentation(), 
+            # preprocessing=get_preprocessing(self.preprocessing_fn),
+            classes=self.cfg.data.CLASSES,
+        )
+
+        train_size = int(len(train_dataset) * self.cfg.model.train_ratio)
+        valid_size = len(train_dataset) - train_size
+        train_set, val_set = torch.utils.data.random_split(train_dataset, [train_size, valid_size])
+
+        train_loader = DataLoader(train_set, batch_size=self.cfg.model.batch_size, shuffle=True, num_workers=4)
+        valid_loader = DataLoader(val_set, batch_size=self.cfg.model.batch_size, shuffle=True, num_workers=4)
+        test_loader = DataLoader(valid_dataset, batch_size=self.cfg.model.batch_size, shuffle=False, num_workers=4)
+
+        dataloaders = { 
+                        'train': train_loader, \
+                        'valid': valid_loader, \
+                        'test': test_loader, \
+
+                        'train_size': train_size, \
+                        'valid_size': valid_size, \
+                        'test_size': len(valid_dataset)
+                    }
+
+        return dataloaders
+
+
+    def run(self) -> None:
+
+        self.dataloaders = self.get_dataloaders()
+        self.optimizer = torch.optim.Adam([dict(
+                params=self.model.parameters(), 
+                lr=self.cfg.model.learning_rate, 
+                weight_decay=self.cfg.model.weight_decay)])
+
+        # lr scheduler
+        per_epoch_steps = self.dataloaders['train_size'] // self.cfg.model.batch_size
+        total_training_steps = self.cfg.model.max_epoch * per_epoch_steps
+        warmup_steps = self.cfg.model.warmup_coef * per_epoch_steps
+        if self.cfg.model.use_lr_scheduler:
+            self.lr_scheduler = get_cosine_schedule_with_warmup(self.optimizer, warmup_steps, total_training_steps)
+
+        self.history_logs = edict()
+        self.history_logs['train'] = []
+        self.history_logs['valid'] = []
+        self.history_logs['test'] = []
+
+        # --------------------------------- Train -------------------------------------------
+        max_score = self.cfg.model.max_score
+        for epoch in range(0, self.cfg.model.max_epoch):
+            epoch = epoch + 1
+            print(f"\n==> train epoch: {epoch}/{self.cfg.model.max_epoch}")
+            self.train_one_epoch(epoch)
+            valid_logs = self.valid_logs
+            
+            # do something (save model, change lr, etc.)
+            if valid_logs['iou_score'] > max_score:
+                max_score = valid_logs['iou_score']
+
+                if (1 == epoch) or (0 == epoch % self.cfg.model.save_interval):
+                    torch.save(self.model, self.model_url)
+                    # torch.save(self.model.state_dict(), self.model_url)
+                    print('Model saved!')
+
+            # if epoch % 50 == 0:
+            #     self.optimizer.param_groups[0]['lr'] = 0.1 * self.optimizer.param_groups[0]['lr']
+                        
+        
+    def train_one_epoch(self, epoch):
+        self.model.to(self.DEVICE)
+        
+        # wandb.
+        for phase in ['train', 'valid', 'test']:
+            if phase == 'train':
+                self.model.train()
+            else:
+                self.model.eval()
+
+            logs = self.step(phase) 
+
+            currlr = self.lr_scheduler.get_last_lr()[0] if self.cfg.model.use_lr_scheduler else self.optimizer.param_groups[0]['lr']          
+            wandb.log({phase: logs, 'epoch': epoch, 'lr': currlr})
+
+            temp = [logs["total_loss"]] + [logs[self.metrics[i].__name__] for i in range(0, len(self.metrics))]
+            self.history_logs[phase].append(temp)
+
+            if phase == 'valid': self.valid_logs = logs
+
+
+    def step(self, phase) -> dict:
+        logs = {}
+        loss_meter = AverageValueMeter()
+        metrics_meters = {metric.__name__: AverageValueMeter() for metric in self.metrics}
+
+        # if ('Train' in phase) and (self.cfg.useDataWoCAug):
+        #     dataLoader_woCAug = iter(self.dataloaders['Train_woCAug'])
+
+        with tqdm(iter(self.dataloaders[phase]), desc=phase, file=sys.stdout, disable=not self.cfg.model.verbose) as iterator:
+            for (x1, x2, y) in iterator:
+            # for (x1_pre, x1_post, x2_pre, x2_post, y) in iterator:
+                # print(x.shape)
+                # print(y.shape)
+
+                # if ('Train' in phase) and (self.cfg.useDataWoCAug):
+                #     x0, y0 = next(dataLoader_woCAug)  
+                #     x = torch.cat((x0, x), dim=0)
+                #     y = torch.cat((y0, y), dim=0)
+
+                x1, x2, y = x1.to(self.DEVICE), x2.to(self.DEVICE), y.to(self.DEVICE)
+                self.optimizer.zero_grad()
+
+                if 'FuseUNet' in self.cfg.model.ARCH:
+                    # if 'train' == phase: y_pred, decoder_out = self.model.forward((x1, x2))
+                    # else: y_pred, decoder_out = self.model.forward((x1, x2))
+
+                    y_pred, decoder_out = self.model.forward((x1, x2))
+                    cdc_loss = mse_loss(decoder_out[0], decoder_out[1])
+
+                elif 'cdc_unet' in self.cfg.model.ARCH:
+                    y_pred, out2 = self.model.forward((x1, x2))
+                    cdc_loss = mse_loss(y_pred, out2)
+
+                else: 
+                    y_pred = self.model.forward((x1, x2))
+                    cdc_loss = 0
+
+                # y_gt = torch.argmax(y, dim=1)
+                dice_loss_ =  diceLoss(y_pred, y)
+                
+                # focal_loss_ = self.cfg.alpha * focal_loss(y_pred, y)
+                # tv_loss_ = 1e-5 * self.cfg.beta * torch.mean(tv_loss(y_pred))
+
+                loss_ = dice_loss_ + self.cfg.model.cross_domain_coef * cdc_loss
+
+                # update loss logs
+                loss_value = loss_.cpu().detach().numpy()
+                loss_meter.add(loss_value)
+                # loss_logs = {criterion.__name__: loss_meter.mean}
+                loss_logs = {'total_loss': loss_meter.mean}
+                logs.update(loss_logs)
+
+                # update metrics logs
+                for metric_fn in self.metrics:
+                    metric_value = metric_fn(y_pred, y).cpu().detach().numpy()
+                    metrics_meters[metric_fn.__name__].add(metric_value)
+
+                metrics_logs = {k: v.mean for k, v in metrics_meters.items()}
+                metrics_logs['cdc_loss'] = cdc_loss
+                logs.update(metrics_logs)
+                # print(logs)
+
+                if self.cfg.model.verbose:
+                    s = format_logs(logs)
+                    iterator.set_postfix_str(s)
+
+                if phase == 'train':
+                    loss_.backward()
+                    self.optimizer.step()
+
+                    if self.cfg.model.use_lr_scheduler:
+                        self.lr_scheduler.step()
+
+            return logs
+##############################################################
+
+
+
+def set_random_seed(seed, deterministic=False):
+    """Set random seed.
+
+    Args:
+        seed (int): Seed to be used.
+        deterministic (bool): Whether to set the deterministic option for
+            CUDNN backend, i.e., set `torch.backends.cudnn.deterministic`
+            to True and `torch.backends.cudnn.benchmark` to False.
+            Default: False.
+    """
+    random.seed(seed)
+    np.random.seed(seed)
+    torch.manual_seed(seed)
+    torch.cuda.manual_seed_all(seed)
+    if deterministic:
+        torch.backends.cudnn.deterministic = True
+        torch.backends.cudnn.benchmark = False
+
+
+@hydra.main(config_path="./config", config_name="s1s2_fuse_unet")
+def run_app(cfg : DictConfig) -> None:
+    print(OmegaConf.to_yaml(cfg))
+
+    # wandb.init(config=cfg, project=cfg.project.name, name=cfg.experiment.name)
+    wandb.init(config=cfg, project=cfg.project.name, entity=cfg.project.entity, name=cfg.experiment.name)
+    # project_dir = Path(hydra.utils.get_original_cwd())
+    
+    # set randome seed
+    set_random_seed(cfg.data.SEED)
+
+    # from experiments.seg_model import SegModel
+    mySegModel = SegModel(cfg)
+    mySegModel.run()
+
+    # evaluation
+    from s1s2_evaluator import evaluate_model
+    evaluate_model(cfg, mySegModel.model_url, mySegModel.rundir / "errMap")
+    
+    wandb.finish()
+
+
+if __name__ == "__main__":
+    run_app()
diff --git a/main_s1s2_wandb.py b/scripts/main_s1s2_distill_unet_pretrain.py
similarity index 74%
rename from main_s1s2_wandb.py
rename to scripts/main_s1s2_distill_unet_pretrain.py
index 17256c7..9c3fac2 100644
--- a/main_s1s2_wandb.py
+++ b/scripts/main_s1s2_distill_unet_pretrain.py
@@ -32,7 +32,7 @@ import logging
 logger = logging.getLogger(__name__)
 
 import smp
-from models.net_arch import init_model
+from models.model_selection import get_model
 import wandb
 
 f_score = smp.utils.functional.f_score
@@ -42,6 +42,8 @@ f_score = smp.utils.functional.f_score
 diceLoss = smp.utils.losses.DiceLoss(eps=1)
 AverageValueMeter =  smp.utils.train.AverageValueMeter
 
+from smp.base.modules import Activation
+
 # Augmentations
 from dataset.augument import get_training_augmentation, \
     get_validation_augmentation, get_preprocessing
@@ -65,14 +67,19 @@ class SegModel(object):
         self.cfg = cfg
         self.DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
 
-        self.model = init_model(cfg)
-        self.model_url = str(self.project_dir / "outputs" / "best_model.pth")
+        self.model = get_model(cfg)
+        self.activation = Activation(cfg.model.ACTIVATION)
+
+        # self.model_url = str(self.project_dir / "outputs" / "best_model.pth")
+        self.rundir = self.project_dir / self.cfg.experiment.output
+        self.model_url = str( self.rundir / "model.pth")
 
         self.preprocessing_fn = \
             smp.encoders.get_preprocessing_fn(cfg.model.ENCODER, cfg.model.ENCODER_WEIGHTS)
 
         self.metrics = [smp.utils.metrics.IoU(threshold=0.5),
-                        smp.utils.metrics.Fscore()]
+                        smp.utils.metrics.Fscore()
+                    ]
 
         ''' -------------> need to improve <-----------------'''
         # specify data folder
@@ -100,21 +107,28 @@ class SegModel(object):
             classes=self.cfg.data.CLASSES,
         )
 
-        train_loader = DataLoader(train_dataset, batch_size=self.cfg.model.batch_size, shuffle=True, num_workers=12)
-        valid_loader = DataLoader(valid_dataset, batch_size=self.cfg.model.batch_size, shuffle=False, num_workers=4)
+        train_size = int(len(train_dataset) * self.cfg.data.train_ratio)
+        valid_size = len(train_dataset) - train_size
+        train_set, val_set = torch.utils.data.random_split(train_dataset, [train_size, valid_size])
+
+        train_loader = DataLoader(train_set, batch_size=self.cfg.model.batch_size, shuffle=True, num_workers=4)
+        valid_loader = DataLoader(val_set, batch_size=self.cfg.model.batch_size, shuffle=True, num_workers=4)
+        test_loader = DataLoader(valid_dataset, batch_size=self.cfg.model.batch_size, shuffle=False, num_workers=4)
 
         dataloaders = { 
                         'train': train_loader, \
                         'valid': valid_loader, \
-                        'train_size': len(train_dataset),
-                        'valid_size': len(valid_dataset)
+                        'test': test_loader, \
+
+                        'train_size': train_size, \
+                        'valid_size': valid_size, \
+                        'test_size': len(valid_dataset)
                     }
 
         return dataloaders
 
 
     def run(self) -> None:
-
         self.dataloaders = self.get_dataloaders()
         self.optimizer = torch.optim.Adam([dict(
                 params=self.model.parameters(), 
@@ -131,35 +145,41 @@ class SegModel(object):
         self.history_logs = edict()
         self.history_logs['train'] = []
         self.history_logs['valid'] = []
+        self.history_logs['test'] = []
 
         # --------------------------------- Train -------------------------------------------
+        max_score = self.cfg.model.max_score
         for epoch in range(0, self.cfg.model.max_epoch):
             epoch = epoch + 1
             print(f"\n==> train epoch: {epoch}/{self.cfg.model.max_epoch}")
-            valid_logs = self.train_one_epoch(epoch)
+            self.train_one_epoch(epoch)
+            valid_logs = self.valid_logs
             
             # do something (save model, change lr, etc.)
-            if valid_logs['iou_score'] > self.cfg.model.max_score:
-                self.cfg.model.max_score = valid_logs['iou_score']
-                torch.save(self.model, self.model_url)
-                # torch.save(self.model.state_dict(), self.model_url)
-                print('Model saved!')
-
-            if epoch % 50 == 0:
-                self.optimizer.param_groups[0]['lr'] = 0.1 * self.optimizer.param_groups[0]['lr']
+            if valid_logs['iou_score'] > max_score:
+                max_score = valid_logs['iou_score']
+
+                if (1 == epoch) or (0 == epoch % self.cfg.model.save_interval):
+                    torch.save(self.model, self.model_url)
+                    # torch.save(self.model.state_dict(), self.model_url)
+                    print('Model saved!')
+
+            # if epoch % 50 == 0:
+            #     self.optimizer.param_groups[0]['lr'] = 0.1 * self.optimizer.param_groups[0]['lr']
                         
         
     def train_one_epoch(self, epoch):
         self.model.to(self.DEVICE)
         
         # wandb.
-        for phase in ['train', 'valid']:
+        for phase in ['train', 'valid', 'test']:
             if phase == 'train':
                 self.model.train()
             else:
                 self.model.eval()
 
             logs = self.step(phase) 
+            # print(phase, logs)
 
             currlr = self.lr_scheduler.get_last_lr()[0] if self.cfg.model.use_lr_scheduler else self.optimizer.param_groups[0]['lr']          
             wandb.log({phase: logs, 'epoch': epoch, 'lr': currlr})
@@ -167,9 +187,8 @@ class SegModel(object):
             temp = [logs["total_loss"]] + [logs[self.metrics[i].__name__] for i in range(0, len(self.metrics))]
             self.history_logs[phase].append(temp)
 
-            if phase == 'valid':
-                valid_logs = logs
-                return valid_logs
+            if phase == 'valid': self.valid_logs = logs
+
 
 
     def step(self, phase) -> dict:
@@ -181,18 +200,20 @@ class SegModel(object):
         #     dataLoader_woCAug = iter(self.dataloaders['Train_woCAug'])
 
         with tqdm(iter(self.dataloaders[phase]), desc=phase, file=sys.stdout, disable=not self.cfg.model.verbose) as iterator:
-            for (x, y) in iterator:
+            for (x1, x2, y) in iterator:
 
                 # if ('Train' in phase) and (self.cfg.useDataWoCAug):
                 #     x0, y0 = next(dataLoader_woCAug)  
                 #     x = torch.cat((x0, x), dim=0)
                 #     y = torch.cat((y0, y), dim=0)
 
-                x, y = x.to(self.DEVICE), y.to(self.DEVICE)
+                x1, x2, y = x1.to(self.DEVICE), x2.to(self.DEVICE), y.to(self.DEVICE)
                 self.optimizer.zero_grad()
 
-                y_pred = self.model.forward(x)
+                _, out = self.model.forward((x1, x2))
+                y_pred = self.activation(out)
 
+                ''' compute loss '''
                 dice_loss_ =  diceLoss(y_pred, y)
                 # focal_loss_ = self.cfg.alpha * focal_loss(y_pred, y)
                 # tv_loss_ = 1e-5 * self.cfg.beta * torch.mean(tv_loss(y_pred))
@@ -250,23 +271,24 @@ def set_random_seed(seed, deterministic=False):
         torch.backends.cudnn.benchmark = False
 
 
-@hydra.main(config_path="./config", config_name="s1s2_cfg")
+@hydra.main(config_path="./config", config_name="distill_unet_pretrain")
 def run_app(cfg : DictConfig) -> None:
     print(OmegaConf.to_yaml(cfg))
 
-    wandb.init(config=cfg, project=cfg.project.name, name=cfg.experiment.name)
+    # wandb.init(config=cfg, project=cfg.project.name, name=cfg.experiment.name)
+    wandb.init(config=cfg, project=cfg.project.name, entity=cfg.project.entity, name=cfg.experiment.name)
     # project_dir = Path(hydra.utils.get_original_cwd())
     
     # set randome seed
     set_random_seed(cfg.data.SEED)
 
     # from experiments.seg_model import SegModel
-    model = SegModel(cfg)
-    model.run()
+    mySegModel = SegModel(cfg)
+    mySegModel.run()
 
     # evaluation
     from s1s2_evaluator import evaluate_model
-    evaluate_model(cfg, SegModel=model)
+    evaluate_model(cfg, mySegModel.model_url, mySegModel.rundir / "errMap")
     
     wandb.finish()
 
diff --git a/main_s1s2_fusion.py b/scripts/main_s1s2_fuse_unet.py
similarity index 78%
rename from main_s1s2_fusion.py
rename to scripts/main_s1s2_fuse_unet.py
index 334de7a..b0e5d74 100644
--- a/main_s1s2_fusion.py
+++ b/scripts/main_s1s2_fuse_unet.py
@@ -74,14 +74,16 @@ class SegModel(object):
         self.DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
 
         self.model = init_model(cfg)
-        self.model_url = str(self.project_dir / "outputs" / "best_model.pth")
+        # self.model_url = str(self.project_dir / "outputs" / "best_model.pth")
+        self.rundir = self.project_dir / self.cfg.experiment.output
+        self.model_url = str( self.rundir / "model.pth")
 
         self.preprocessing_fn = \
             smp.encoders.get_preprocessing_fn(cfg.model.ENCODER, cfg.model.ENCODER_WEIGHTS)
 
         self.metrics = [smp.utils.metrics.IoU(threshold=0.5),
                         smp.utils.metrics.Fscore(),
-                        ]
+                    ]
 
         ''' -------------> need to improve <-----------------'''
         # specify data folder
@@ -104,20 +106,28 @@ class SegModel(object):
 
         valid_dataset = Dataset(
             self.valid_dir, 
-            self.cfg,
+            self.cfg, 
             # augmentation=get_validation_augmentation(), 
             # preprocessing=get_preprocessing(self.preprocessing_fn),
             classes=self.cfg.data.CLASSES,
         )
 
-        train_loader = DataLoader(train_dataset, batch_size=self.cfg.model.batch_size, shuffle=True, num_workers=12)
-        valid_loader = DataLoader(valid_dataset, batch_size=self.cfg.model.batch_size, shuffle=False, num_workers=4)
+        train_size = int(len(train_dataset) * self.cfg.model.train_ratio)
+        valid_size = len(train_dataset) - train_size
+        train_set, val_set = torch.utils.data.random_split(train_dataset, [train_size, valid_size])
+
+        train_loader = DataLoader(train_set, batch_size=self.cfg.model.batch_size, shuffle=True, num_workers=4)
+        valid_loader = DataLoader(val_set, batch_size=self.cfg.model.batch_size, shuffle=True, num_workers=4)
+        test_loader = DataLoader(valid_dataset, batch_size=self.cfg.model.batch_size, shuffle=False, num_workers=4)
 
         dataloaders = { 
                         'train': train_loader, \
                         'valid': valid_loader, \
-                        'train_size': len(train_dataset),
-                        'valid_size': len(valid_dataset)
+                        'test': test_loader, \
+
+                        'train_size': train_size, \
+                        'valid_size': valid_size, \
+                        'test_size': len(valid_dataset)
                     }
 
         return dataloaders
@@ -141,29 +151,34 @@ class SegModel(object):
         self.history_logs = edict()
         self.history_logs['train'] = []
         self.history_logs['valid'] = []
+        self.history_logs['test'] = []
 
         # --------------------------------- Train -------------------------------------------
+        max_score = self.cfg.model.max_score
         for epoch in range(0, self.cfg.model.max_epoch):
             epoch = epoch + 1
             print(f"\n==> train epoch: {epoch}/{self.cfg.model.max_epoch}")
-            valid_logs = self.train_one_epoch(epoch)
+            self.train_one_epoch(epoch)
+            valid_logs = self.valid_logs
             
             # do something (save model, change lr, etc.)
-            if valid_logs['iou_score'] > self.cfg.model.max_score:
+            if valid_logs['iou_score'] > max_score:
                 max_score = valid_logs['iou_score']
-                torch.save(self.model, self.model_url)
-                # torch.save(self.model.state_dict(), self.model_url)
-                print('Model saved!')
 
-            if epoch % 50 == 0:
-                self.optimizer.param_groups[0]['lr'] = 0.1 * self.optimizer.param_groups[0]['lr']
+                if (1== epoch) or (0 == epoch % self.cfg.model.save_interval):
+                    torch.save(self.model, self.model_url)
+                    # torch.save(self.model.state_dict(), self.model_url)
+                    print('Model saved!')
+
+            # if epoch % 50 == 0:
+            #     self.optimizer.param_groups[0]['lr'] = 0.1 * self.optimizer.param_groups[0]['lr']
                         
         
     def train_one_epoch(self, epoch):
         self.model.to(self.DEVICE)
         
         # wandb.
-        for phase in ['train', 'valid']:
+        for phase in ['train', 'valid', 'test']:
             if phase == 'train':
                 self.model.train()
             else:
@@ -177,9 +192,7 @@ class SegModel(object):
             temp = [logs["total_loss"]] + [logs[self.metrics[i].__name__] for i in range(0, len(self.metrics))]
             self.history_logs[phase].append(temp)
 
-            if phase == 'valid':
-                valid_logs = logs
-                return valid_logs
+            if phase == 'valid': self.valid_logs = logs
 
 
     def step(self, phase) -> dict:
@@ -192,6 +205,7 @@ class SegModel(object):
 
         with tqdm(iter(self.dataloaders[phase]), desc=phase, file=sys.stdout, disable=not self.cfg.model.verbose) as iterator:
             for (x1, x2, y) in iterator:
+            # for (x1_pre, x1_post, x2_pre, x2_post, y) in iterator:
                 # print(x.shape)
                 # print(y.shape)
 
@@ -203,8 +217,12 @@ class SegModel(object):
                 x1, x2, y = x1.to(self.DEVICE), x2.to(self.DEVICE), y.to(self.DEVICE)
                 self.optimizer.zero_grad()
 
-                if 'Fuse' in self.cfg.model.ARCH:
+                if 'FuseUNet' in self.cfg.model.ARCH:
+                    # if 'train' == phase: y_pred, decoder_out = self.model.forward((x1, x2))
+                    # else: y_pred, decoder_out = self.model.forward((x1, x2))
+
                     y_pred, decoder_out = self.model.forward((x1, x2))
+                    
                     cross_domain_loss = mse_loss(decoder_out[0], decoder_out[1])
 
                 else: 
@@ -271,23 +289,24 @@ def set_random_seed(seed, deterministic=False):
         torch.backends.cudnn.benchmark = False
 
 
-@hydra.main(config_path="./config", config_name="s1s2_cfg")
+@hydra.main(config_path="./config", config_name="s1s2_fuse_unet")
 def run_app(cfg : DictConfig) -> None:
     print(OmegaConf.to_yaml(cfg))
 
-    wandb.init(config=cfg, project=cfg.project.name, name=cfg.experiment.name)
+    # wandb.init(config=cfg, project=cfg.project.name, name=cfg.experiment.name)
+    wandb.init(config=cfg, project=cfg.project.name, entity=cfg.project.entity, name=cfg.experiment.name)
     # project_dir = Path(hydra.utils.get_original_cwd())
     
     # set randome seed
     set_random_seed(cfg.data.SEED)
 
     # from experiments.seg_model import SegModel
-    model = SegModel(cfg)
-    model.run()
+    mySegModel = SegModel(cfg)
+    mySegModel.run()
 
     # evaluation
     from s1s2_evaluator import evaluate_model
-    evaluate_model(cfg, SegModel=model)
+    evaluate_model(cfg, mySegModel.model_url, mySegModel.rundir / "errMap")
     
     wandb.finish()
 
diff --git a/scripts/main_s1s2_siamunet_bitemporal.py b/scripts/main_s1s2_siamunet_bitemporal.py
new file mode 100644
index 0000000..2262627
--- /dev/null
+++ b/scripts/main_s1s2_siamunet_bitemporal.py
@@ -0,0 +1,296 @@
+
+import os, json
+import random
+from easydict import EasyDict as edict
+from pathlib import Path
+from prettyprinter import pprint
+from imageio import imread, imsave
+
+import hydra
+import wandb
+from omegaconf import DictConfig, OmegaConf
+
+
+###################################################################################
+import os, sys
+import numpy as np
+from pathlib import Path
+import hydra
+from omegaconf import DictConfig, OmegaConf
+
+import copy
+import time
+import torch
+import torch.optim as optim
+import torch.nn as nn
+import torch.nn.functional as F
+from easydict import EasyDict as edict
+
+from tqdm import tqdm as tqdm
+
+import logging
+logger = logging.getLogger(__name__)
+
+import smp
+from models.model_selection import get_model
+import wandb
+
+f_score = smp.utils.functional.f_score
+
+# Dice/F1 score - https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient
+# IoU/Jaccard score - https://en.wikipedia.org/wiki/Jaccard_index
+diceLoss = smp.utils.losses.DiceLoss(eps=1)
+AverageValueMeter =  smp.utils.train.AverageValueMeter
+
+from smp.base.modules import Activation
+
+# Augmentations
+from dataset.augument import get_training_augmentation, \
+    get_validation_augmentation, get_preprocessing
+
+from torch.utils.data import DataLoader
+from dataset.wildfire import S1S2 as Dataset # ------------------------------------------------------- Dataset
+
+from models.lr_schedule import get_cosine_schedule_with_warmup
+
+
+def format_logs(logs):
+    str_logs = ['{}: {:.4}'.format(k, v) for k, v in logs.items()]
+    s = ', '.join(str_logs)
+    return s
+
+class SegModel(object):
+    def __init__(self, cfg) -> None:
+        super().__init__()
+        self.project_dir = Path(hydra.utils.get_original_cwd())
+
+        self.cfg = cfg
+        self.DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
+
+        self.model = get_model(cfg)
+        self.activation = Activation(cfg.model.ACTIVATION)
+
+        # self.model_url = str(self.project_dir / "outputs" / "best_model.pth")
+        self.rundir = self.project_dir / self.cfg.experiment.output
+        self.model_url = str( self.rundir / "model.pth")
+
+        self.preprocessing_fn = \
+            smp.encoders.get_preprocessing_fn(cfg.model.ENCODER, cfg.model.ENCODER_WEIGHTS)
+
+        self.metrics = [smp.utils.metrics.IoU(threshold=0.5),
+                        smp.utils.metrics.Fscore()
+                    ]
+
+        ''' -------------> need to improve <-----------------'''
+        # specify data folder
+        self.train_dir = Path(self.cfg.data.dir) / 'train'
+        self.valid_dir = Path(self.cfg.data.dir) / 'test'
+        '''--------------------------------------------------'''
+
+
+    def get_dataloaders(self) -> dict:
+
+        """ Data Preparation """
+        train_dataset = Dataset(
+            self.train_dir, 
+            self.cfg, 
+            # augmentation=get_training_augmentation(), 
+            # preprocessing=get_preprocessing(self.preprocessing_fn),
+            classes=self.cfg.data.CLASSES,
+        )
+
+        valid_dataset = Dataset(
+            self.valid_dir, 
+            self.cfg, 
+            # augmentation=get_validation_augmentation(), 
+            # preprocessing=get_preprocessing(self.preprocessing_fn),
+            classes=self.cfg.data.CLASSES,
+        )
+
+        train_size = int(len(train_dataset) * self.cfg.data.train_ratio)
+        valid_size = len(train_dataset) - train_size
+        train_set, val_set = torch.utils.data.random_split(train_dataset, [train_size, valid_size])
+
+        train_loader = DataLoader(train_set, batch_size=self.cfg.model.batch_size, shuffle=True, num_workers=4)
+        valid_loader = DataLoader(val_set, batch_size=self.cfg.model.batch_size, shuffle=True, num_workers=4)
+        test_loader = DataLoader(valid_dataset, batch_size=self.cfg.model.batch_size, shuffle=False, num_workers=4)
+
+        dataloaders = { 
+                        'train': train_loader, \
+                        'valid': valid_loader, \
+                        'test': test_loader, \
+
+                        'train_size': train_size, \
+                        'valid_size': valid_size, \
+                        'test_size': len(valid_dataset)
+                    }
+
+        return dataloaders
+
+
+    def run(self) -> None:
+        self.dataloaders = self.get_dataloaders()
+        self.optimizer = torch.optim.Adam([dict(
+                params=self.model.parameters(), 
+                lr=self.cfg.model.learning_rate, 
+                weight_decay=self.cfg.model.weight_decay)])
+
+        # lr scheduler
+        per_epoch_steps = self.dataloaders['train_size'] // self.cfg.model.batch_size
+        total_training_steps = self.cfg.model.max_epoch * per_epoch_steps
+        warmup_steps = self.cfg.model.warmup_coef * per_epoch_steps
+        if self.cfg.model.use_lr_scheduler:
+            self.lr_scheduler = get_cosine_schedule_with_warmup(self.optimizer, warmup_steps, total_training_steps)
+
+        self.history_logs = edict()
+        self.history_logs['train'] = []
+        self.history_logs['valid'] = []
+        self.history_logs['test'] = []
+
+        # --------------------------------- Train -------------------------------------------
+        max_score = self.cfg.model.max_score
+        for epoch in range(0, self.cfg.model.max_epoch):
+            epoch = epoch + 1
+            print(f"\n==> train epoch: {epoch}/{self.cfg.model.max_epoch}")
+            self.train_one_epoch(epoch)
+            valid_logs = self.valid_logs
+            
+            # do something (save model, change lr, etc.)
+            if valid_logs['iou_score'] > max_score:
+                max_score = valid_logs['iou_score']
+
+                if (1 == epoch) or (0 == epoch % self.cfg.model.save_interval):
+                    torch.save(self.model, self.model_url)
+                    # torch.save(self.model.state_dict(), self.model_url)
+                    print('Model saved!')
+
+            # if epoch % 50 == 0:
+            #     self.optimizer.param_groups[0]['lr'] = 0.1 * self.optimizer.param_groups[0]['lr']
+                        
+        
+    def train_one_epoch(self, epoch):
+        self.model.to(self.DEVICE)
+        
+        # wandb.
+        for phase in ['train', 'valid', 'test']:
+            if phase == 'train':
+                self.model.train()
+            else:
+                self.model.eval()
+
+            logs = self.step(phase) 
+            # print(phase, logs)
+
+            currlr = self.lr_scheduler.get_last_lr()[0] if self.cfg.model.use_lr_scheduler else self.optimizer.param_groups[0]['lr']          
+            wandb.log({phase: logs, 'epoch': epoch, 'lr': currlr})
+
+            temp = [logs["total_loss"]] + [logs[self.metrics[i].__name__] for i in range(0, len(self.metrics))]
+            self.history_logs[phase].append(temp)
+
+            if phase == 'valid': self.valid_logs = logs
+
+
+
+    def step(self, phase) -> dict:
+        logs = {}
+        loss_meter = AverageValueMeter()
+        metrics_meters = {metric.__name__: AverageValueMeter() for metric in self.metrics}
+
+        # if ('Train' in phase) and (self.cfg.useDataWoCAug):
+        #     dataLoader_woCAug = iter(self.dataloaders['Train_woCAug'])
+
+        with tqdm(iter(self.dataloaders[phase]), desc=phase, file=sys.stdout, disable=not self.cfg.model.verbose) as iterator:
+            for (x, y) in iterator:
+                self.optimizer.zero_grad()
+
+                ''' move data to GPU '''
+                input = []
+                for x_i in x: input.append(x_i.to(self.DEVICE))
+                y = y.to(self.DEVICE)
+                # print(len(input))
+
+                ''' do prediction '''
+                out = self.model.forward(input)[-1]
+                y_pred = self.activation(out)
+
+                dice_loss_ =  diceLoss(y_pred, y)
+                # focal_loss_ = self.cfg.alpha * focal_loss(y_pred, y)
+                # tv_loss_ = 1e-5 * self.cfg.beta * torch.mean(tv_loss(y_pred))
+
+                loss_ = dice_loss_
+
+                # update loss logs
+                loss_value = loss_.cpu().detach().numpy()
+                loss_meter.add(loss_value)
+                # loss_logs = {criterion.__name__: loss_meter.mean}
+                loss_logs = {'total_loss': loss_meter.mean}
+                logs.update(loss_logs)
+
+                # update metrics logs
+                for metric_fn in self.metrics:
+                    metric_value = metric_fn(y_pred, y).cpu().detach().numpy()
+                    metrics_meters[metric_fn.__name__].add(metric_value)
+
+                metrics_logs = {k: v.mean for k, v in metrics_meters.items()}
+                logs.update(metrics_logs)
+                # print(logs)
+
+                if self.cfg.model.verbose:
+                    s = format_logs(logs)
+                    iterator.set_postfix_str(s)
+
+                if phase == 'train':
+                    loss_.backward()
+                    self.optimizer.step()
+
+                    if self.cfg.model.use_lr_scheduler:
+                        self.lr_scheduler.step()
+
+            return logs
+##############################################################
+
+
+
+def set_random_seed(seed, deterministic=False):
+    """Set random seed.
+
+    Args:
+        seed (int): Seed to be used.
+        deterministic (bool): Whether to set the deterministic option for
+            CUDNN backend, i.e., set `torch.backends.cudnn.deterministic`
+            to True and `torch.backends.cudnn.benchmark` to False.
+            Default: False.
+    """
+    random.seed(seed)
+    np.random.seed(seed)
+    torch.manual_seed(seed)
+    torch.cuda.manual_seed_all(seed)
+    if deterministic:
+        torch.backends.cudnn.deterministic = True
+        torch.backends.cudnn.benchmark = False
+
+
+@hydra.main(config_path="./config", config_name="siam_unet_bitemoral")
+def run_app(cfg : DictConfig) -> None:
+    print(OmegaConf.to_yaml(cfg))
+
+    # wandb.init(config=cfg, project=cfg.project.name, name=cfg.experiment.name)
+    wandb.init(config=cfg, project=cfg.project.name, entity=cfg.project.entity, name=cfg.experiment.name)
+    # project_dir = Path(hydra.utils.get_original_cwd())
+    
+    # set randome seed
+    set_random_seed(cfg.data.SEED)
+
+    # from experiments.seg_model import SegModel
+    mySegModel = SegModel(cfg)
+    mySegModel.run()
+
+    # evaluation
+    from s1s2_evaluator import evaluate_model
+    evaluate_model(cfg, mySegModel.model_url, mySegModel.rundir / "errMap")
+    
+    wandb.finish()
+
+
+if __name__ == "__main__":
+    run_app()
diff --git a/scripts/main_s1s2_siamunet_multisensor.py b/scripts/main_s1s2_siamunet_multisensor.py
new file mode 100644
index 0000000..99c0e9e
--- /dev/null
+++ b/scripts/main_s1s2_siamunet_multisensor.py
@@ -0,0 +1,296 @@
+
+import os, json
+import random
+from easydict import EasyDict as edict
+from pathlib import Path
+from prettyprinter import pprint
+from imageio import imread, imsave
+
+import hydra
+import wandb
+from omegaconf import DictConfig, OmegaConf
+
+
+###################################################################################
+import os, sys
+import numpy as np
+from pathlib import Path
+import hydra
+from omegaconf import DictConfig, OmegaConf
+
+import copy
+import time
+import torch
+import torch.optim as optim
+import torch.nn as nn
+import torch.nn.functional as F
+from easydict import EasyDict as edict
+
+from tqdm import tqdm as tqdm
+
+import logging
+logger = logging.getLogger(__name__)
+
+import smp
+from models.model_selection import get_model
+import wandb
+
+f_score = smp.utils.functional.f_score
+
+# Dice/F1 score - https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient
+# IoU/Jaccard score - https://en.wikipedia.org/wiki/Jaccard_index
+diceLoss = smp.utils.losses.DiceLoss(eps=1)
+AverageValueMeter =  smp.utils.train.AverageValueMeter
+
+from smp.base.modules import Activation
+
+# Augmentations
+from dataset.augument import get_training_augmentation, \
+    get_validation_augmentation, get_preprocessing
+
+from torch.utils.data import DataLoader
+from dataset.wildfire import S1S2 as Dataset # ------------------------------------------------------- Dataset
+
+from models.lr_schedule import get_cosine_schedule_with_warmup
+
+
+def format_logs(logs):
+    str_logs = ['{}: {:.4}'.format(k, v) for k, v in logs.items()]
+    s = ', '.join(str_logs)
+    return s
+
+class SegModel(object):
+    def __init__(self, cfg) -> None:
+        super().__init__()
+        self.project_dir = Path(hydra.utils.get_original_cwd())
+
+        self.cfg = cfg
+        self.DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
+
+        self.model = get_model(cfg)
+        self.activation = Activation(cfg.model.ACTIVATION)
+
+        # self.model_url = str(self.project_dir / "outputs" / "best_model.pth")
+        self.rundir = self.project_dir / self.cfg.experiment.output
+        self.model_url = str( self.rundir / "model.pth")
+
+        self.preprocessing_fn = \
+            smp.encoders.get_preprocessing_fn(cfg.model.ENCODER, cfg.model.ENCODER_WEIGHTS)
+
+        self.metrics = [smp.utils.metrics.IoU(threshold=0.5),
+                        smp.utils.metrics.Fscore()
+                    ]
+
+        ''' -------------> need to improve <-----------------'''
+        # specify data folder
+        self.train_dir = Path(self.cfg.data.dir) / 'train'
+        self.valid_dir = Path(self.cfg.data.dir) / 'test'
+        '''--------------------------------------------------'''
+
+
+    def get_dataloaders(self) -> dict:
+
+        """ Data Preparation """
+        train_dataset = Dataset(
+            self.train_dir, 
+            self.cfg, 
+            # augmentation=get_training_augmentation(), 
+            # preprocessing=get_preprocessing(self.preprocessing_fn),
+            classes=self.cfg.data.CLASSES,
+        )
+
+        valid_dataset = Dataset(
+            self.valid_dir, 
+            self.cfg, 
+            # augmentation=get_validation_augmentation(), 
+            # preprocessing=get_preprocessing(self.preprocessing_fn),
+            classes=self.cfg.data.CLASSES,
+        )
+
+        train_size = int(len(train_dataset) * self.cfg.data.train_ratio)
+        valid_size = len(train_dataset) - train_size
+        train_set, val_set = torch.utils.data.random_split(train_dataset, [train_size, valid_size])
+
+        train_loader = DataLoader(train_set, batch_size=self.cfg.model.batch_size, shuffle=True, num_workers=4)
+        valid_loader = DataLoader(val_set, batch_size=self.cfg.model.batch_size, shuffle=True, num_workers=4)
+        test_loader = DataLoader(valid_dataset, batch_size=self.cfg.model.batch_size, shuffle=False, num_workers=4)
+
+        dataloaders = { 
+                        'train': train_loader, \
+                        'valid': valid_loader, \
+                        'test': test_loader, \
+
+                        'train_size': train_size, \
+                        'valid_size': valid_size, \
+                        'test_size': len(valid_dataset)
+                    }
+
+        return dataloaders
+
+
+    def run(self) -> None:
+        self.dataloaders = self.get_dataloaders()
+        self.optimizer = torch.optim.Adam([dict(
+                params=self.model.parameters(), 
+                lr=self.cfg.model.learning_rate, 
+                weight_decay=self.cfg.model.weight_decay)])
+
+        # lr scheduler
+        per_epoch_steps = self.dataloaders['train_size'] // self.cfg.model.batch_size
+        total_training_steps = self.cfg.model.max_epoch * per_epoch_steps
+        warmup_steps = self.cfg.model.warmup_coef * per_epoch_steps
+        if self.cfg.model.use_lr_scheduler:
+            self.lr_scheduler = get_cosine_schedule_with_warmup(self.optimizer, warmup_steps, total_training_steps)
+
+        self.history_logs = edict()
+        self.history_logs['train'] = []
+        self.history_logs['valid'] = []
+        self.history_logs['test'] = []
+
+        # --------------------------------- Train -------------------------------------------
+        max_score = self.cfg.model.max_score
+        for epoch in range(0, self.cfg.model.max_epoch):
+            epoch = epoch + 1
+            print(f"\n==> train epoch: {epoch}/{self.cfg.model.max_epoch}")
+            self.train_one_epoch(epoch)
+            valid_logs = self.valid_logs
+            
+            # do something (save model, change lr, etc.)
+            if valid_logs['iou_score'] > max_score:
+                max_score = valid_logs['iou_score']
+
+                if (1 == epoch) or (0 == epoch % self.cfg.model.save_interval):
+                    torch.save(self.model, self.model_url)
+                    # torch.save(self.model.state_dict(), self.model_url)
+                    print('Model saved!')
+
+            # if epoch % 50 == 0:
+            #     self.optimizer.param_groups[0]['lr'] = 0.1 * self.optimizer.param_groups[0]['lr']
+                        
+        
+    def train_one_epoch(self, epoch):
+        self.model.to(self.DEVICE)
+        
+        # wandb.
+        for phase in ['train', 'valid', 'test']:
+            if phase == 'train':
+                self.model.train()
+            else:
+                self.model.eval()
+
+            logs = self.step(phase) 
+            # print(phase, logs)
+
+            currlr = self.lr_scheduler.get_last_lr()[0] if self.cfg.model.use_lr_scheduler else self.optimizer.param_groups[0]['lr']          
+            wandb.log({phase: logs, 'epoch': epoch, 'lr': currlr})
+
+            temp = [logs["total_loss"]] + [logs[self.metrics[i].__name__] for i in range(0, len(self.metrics))]
+            self.history_logs[phase].append(temp)
+
+            if phase == 'valid': self.valid_logs = logs
+
+
+
+    def step(self, phase) -> dict:
+        logs = {}
+        loss_meter = AverageValueMeter()
+        metrics_meters = {metric.__name__: AverageValueMeter() for metric in self.metrics}
+
+        # if ('Train' in phase) and (self.cfg.useDataWoCAug):
+        #     dataLoader_woCAug = iter(self.dataloaders['Train_woCAug'])
+
+        with tqdm(iter(self.dataloaders[phase]), desc=phase, file=sys.stdout, disable=not self.cfg.model.verbose) as iterator:
+            for (x, y) in iterator:
+                self.optimizer.zero_grad()
+
+                ''' move data to GPU '''
+                input = []
+                for x_i in x: input.append(x_i.to(self.DEVICE))
+                y = y.to(self.DEVICE)
+                # print(len(input))
+
+                ''' do prediction '''
+                out = self.model.forward(input)[-1]
+                y_pred = self.activation(out)
+
+                dice_loss_ =  diceLoss(y_pred, y)
+                # focal_loss_ = self.cfg.alpha * focal_loss(y_pred, y)
+                # tv_loss_ = 1e-5 * self.cfg.beta * torch.mean(tv_loss(y_pred))
+
+                loss_ = dice_loss_
+
+                # update loss logs
+                loss_value = loss_.cpu().detach().numpy()
+                loss_meter.add(loss_value)
+                # loss_logs = {criterion.__name__: loss_meter.mean}
+                loss_logs = {'total_loss': loss_meter.mean}
+                logs.update(loss_logs)
+
+                # update metrics logs
+                for metric_fn in self.metrics:
+                    metric_value = metric_fn(y_pred, y).cpu().detach().numpy()
+                    metrics_meters[metric_fn.__name__].add(metric_value)
+
+                metrics_logs = {k: v.mean for k, v in metrics_meters.items()}
+                logs.update(metrics_logs)
+                # print(logs)
+
+                if self.cfg.model.verbose:
+                    s = format_logs(logs)
+                    iterator.set_postfix_str(s)
+
+                if phase == 'train':
+                    loss_.backward()
+                    self.optimizer.step()
+
+                    if self.cfg.model.use_lr_scheduler:
+                        self.lr_scheduler.step()
+
+            return logs
+##############################################################
+
+
+
+def set_random_seed(seed, deterministic=False):
+    """Set random seed.
+
+    Args:
+        seed (int): Seed to be used.
+        deterministic (bool): Whether to set the deterministic option for
+            CUDNN backend, i.e., set `torch.backends.cudnn.deterministic`
+            to True and `torch.backends.cudnn.benchmark` to False.
+            Default: False.
+    """
+    random.seed(seed)
+    np.random.seed(seed)
+    torch.manual_seed(seed)
+    torch.cuda.manual_seed_all(seed)
+    if deterministic:
+        torch.backends.cudnn.deterministic = True
+        torch.backends.cudnn.benchmark = False
+
+
+@hydra.main(config_path="./config", config_name="siam_unet_multisensor")
+def run_app(cfg : DictConfig) -> None:
+    print(OmegaConf.to_yaml(cfg))
+
+    # wandb.init(config=cfg, project=cfg.project.name, name=cfg.experiment.name)
+    wandb.init(config=cfg, project=cfg.project.name, entity=cfg.project.entity, name=cfg.experiment.name)
+    # project_dir = Path(hydra.utils.get_original_cwd())
+    
+    # set randome seed
+    set_random_seed(cfg.data.SEED)
+
+    # from experiments.seg_model import SegModel
+    mySegModel = SegModel(cfg)
+    mySegModel.run()
+
+    # evaluation
+    from s1s2_evaluator import evaluate_model
+    evaluate_model(cfg, mySegModel.model_url, mySegModel.rundir / "errMap")
+    
+    wandb.finish()
+
+
+if __name__ == "__main__":
+    run_app()
diff --git a/scripts/s1s2_evaluator_V0.py b/scripts/s1s2_evaluator_V0.py
new file mode 100644
index 0000000..707929c
--- /dev/null
+++ b/scripts/s1s2_evaluator_V0.py
@@ -0,0 +1,296 @@
+from ntpath import join
+import os
+from cv2 import _InputArray_OPENGL_BUFFER
+import matplotlib.pyplot as plt
+from imageio import imread, imsave
+import torch
+import numpy as np
+from tqdm import tqdm
+
+from pathlib import Path
+import tifffile as tiff
+from easydict import EasyDict as edict
+
+import smp
+from smp.base.modules import Activation
+
+
+
+import matplotlib.pyplot as plt
+from matplotlib.colors import ListedColormap, LinearSegmentedColormap
+# from utils.GeoTIFF import GeoTIFF
+# geotiff = GeoTIFF()
+
+import wandb
+
+def image_padding(img, patchsize):
+    def zero_padding(arr, patchsize):
+        # print("zero_padding patchsize: {}".format(patchsize))
+        (c, h, w) = arr.shape
+        pad_h = (1 + np.floor(h/patchsize)) * patchsize - h
+        pad_w = (1 + np.floor(w/patchsize)) * patchsize - w
+
+        arr_pad = np.pad(arr, ((0, 0), (0, int(pad_h)), (0, int(pad_w))), mode='symmetric')
+        return arr_pad
+
+    padSize = int(patchsize/2)
+
+    img_pad0 = zero_padding(img, patchsize) # pad img into a shape: (m*PATCHSIZE, n*PATCHSIZE)
+    img_pad = np.pad(img_pad0, ((0, 0), (padSize, padSize), (padSize, padSize)), mode='symmetric')
+    return img_pad
+
+def get_band_index_dict(cfg):
+    ALL_BANDS = cfg.data.ALL_BANDS
+    INPUT_BANDS = cfg.data.INPUT_BANDS
+
+    def get_band_index(sat):
+        all_bands = list(ALL_BANDS[sat])
+        input_bands = list(INPUT_BANDS[sat])
+
+        band_index = []
+        for band in input_bands:
+            band_index.append(all_bands.index(band))
+        return band_index
+
+    band_index_dict = {}
+    for sat in ['S1', 'ALOS', 'S2']:
+        band_index_dict[sat] = get_band_index(sat)
+    
+    return band_index_dict
+
+def inference(model, test_dir, test_id, cfg):
+
+    patchsize = cfg.eval.patchsize
+    NUM_CLASS = len(list(cfg.data.CLASSES))
+    # model.cpu()
+
+    if torch.cuda.is_available():
+        model.to("cuda")
+
+    input_tensors = []
+    for sat in cfg.data.satellites:
+        
+        post_url = test_dir / sat / "post" / f"{test_id}.tif"
+        post_image = tiff.imread(post_url) # C*H*W
+        post_image = post_image[cfg.band_index_dict[sat],] # select bands
+
+        if sat in ['S1', 'ALOS']: post_image = (np.clip(post_image, -30, 0) + 30) / 30
+        post_image_pad = image_padding(post_image, patchsize)
+
+        # img_preprocessed = self.preprocessing_fn(img_pad)
+        post_image_tensor = torch.from_numpy(post_image_pad).unsqueeze(0) # n * C * H * W
+        
+        if 'pre' in cfg.data.prepost:
+            pre_url = test_dir / sat / "pre" / f"{test_id}.tif"
+            pre_image = tiff.imread(pre_url)
+            pre_image = pre_image[cfg.band_index_dict[sat],] # select bands 
+
+            if sat in ['S1', 'ALOS']: pre_image = (np.clip(pre_image, -30, 0) + 30) / 30
+
+            pre_image_pad = image_padding(pre_image, patchsize)
+            pre_image_tensor = torch.from_numpy(pre_image_pad).unsqueeze(0) # n * C * H * W
+        
+            input_tensors.append((pre_image_tensor, post_image_tensor))
+
+        else:
+            input_tensors.append(post_image_tensor)
+
+    C, H, W = post_image.shape
+    _, _, Height, Width = input_tensors[0][0].shape
+    pred_mask_pad = np.zeros((Height, Width))
+    prob_mask_pad = np.zeros((NUM_CLASS, Height, Width))
+
+    input_patchsize = 2 * patchsize
+    padSize = int(patchsize/2) 
+    for i in tqdm(range(0, Height - input_patchsize + 1, patchsize)):
+        for j in range(0, Width - input_patchsize + 1, patchsize):
+            # print(i, i+input_patchsize, j, j+input_patchsize)
+
+            ''' ------------> tile input data <---------- '''
+            input_patchs = []
+            for sat_tensor in input_tensors:
+                post_patch = (sat_tensor[1][..., i:i+input_patchsize, j:j+input_patchsize]).type(torch.cuda.FloatTensor)
+                if 'pre' in cfg.data.prepost: 
+                    pre_patch = (sat_tensor[0][..., i:i+input_patchsize, j:j+input_patchsize]).type(torch.cuda.FloatTensor)
+                    
+                    if cfg.data.stacking: 
+                        inputPatch = torch.cat([pre_patch, post_patch], dim=1) # stacked inputs
+                        input_patchs.append(inputPatch)
+                    else:
+                        input_patchs += [pre_patch, post_patch]
+                else:
+                    input_patchs.append(post_patch)
+
+            ''' ------------> apply model <--------------- '''
+            if 'UNet' == cfg.model.ARCH:
+                if len(cfg.data.satellites) == 1: input = input_patchs[0] # single sensor
+                else: input = torch.cat(input_patchs) # stack multi-sensor data
+                out = model.forward(input) # Old model: input should NOT be a list.
+
+            elif 'distill_unet' == cfg.model.ARCH:
+                _, out = model.forward(input_patchs[0]) # only input S1
+
+            elif 'SiamResUNet' in cfg.model.ARCH:
+                out, decoder_out = model.forward(input_patchs, False)
+
+            elif 'cdc_unet' in cfg.model.ARCH:
+                out, decoder_out = model.forward(input_patchs, False)
+            
+            else: # SiamUnet
+                out = model.forward(input_patchs)
+            ''' ------------------------------------------ '''
+
+            # predPatch = decoder_out[1].squeeze().cpu().detach().numpy()#.round()
+            # predLabel = 1 - np.argmax(predPatch, axis=0).squeeze()
+            activation = Activation(name=cfg.model.ACTIVATION)
+            predPatch = activation(out)
+
+            predPatch = predPatch.cpu().detach().numpy()#.round()
+            
+            if predPatch.shape[0] > 1:
+                predLabel = np.argmax(predPatch, axis=0).squeeze()
+                prob_mask_pad[:, i+padSize:i+padSize+patchsize, j+padSize:j+padSize+patchsize] = predPatch[:, padSize:padSize+patchsize, padSize:padSize+patchsize]  # need to modify
+            else:
+                predPatch = predPatch.squeeze()
+                predLabel = np.round(predPatch)
+                # print(predLabel.shape)
+                prob_mask_pad[:, i+padSize:i+padSize+patchsize, j+padSize:j+padSize+patchsize] = predPatch[padSize:padSize+patchsize, padSize:padSize+patchsize]  # need to modify
+
+            pred_mask_pad[i+padSize:i+padSize+patchsize, j+padSize:j+padSize+patchsize] = predLabel[padSize:padSize+patchsize, padSize:padSize+patchsize]  # need to modify
+            
+    pred_mask = pred_mask_pad[padSize:padSize+H, padSize:padSize+W] # clip back to original shape
+    prod_mask = prob_mask_pad[:, padSize:padSize+H, padSize:padSize+W] # clip back to original shape
+
+    return pred_mask, prod_mask
+
+def gen_errMap(grouthTruth, preMap, save_url=False):
+    errMap = np.zeros(preMap.shape)
+    # errMap[np.where((OptREF==0) & (SARREF==0))] = 0
+    errMap[np.where((grouthTruth==1) & (preMap==1))] = 1.0 # TP, dark red
+    errMap[np.where((grouthTruth==1) & (preMap==0))] = 2.0 # FN, light red
+    errMap[np.where((grouthTruth==0) & (preMap==1))] = 3.0 # FP, green
+
+    num_color = int(1 + max(np.unique(errMap)))
+    # color_tuple = ([1,1,1], [0.6,0,0], [0,0.8,0], [1, 0.6, 0.6])
+    color_tuple = ([1,1,1], [0.6,0,0], [1, 0.6, 0.6], [0,0.8,0])
+    my_cmap = ListedColormap(color_tuple[:num_color])
+
+    # plt.figure(figsize=(15, 15))
+    # plt.imshow(errMap, cmap=my_cmap)
+
+    if save_url:
+        plt.imsave(save_url, errMap, cmap=my_cmap)
+
+        saveName = os.path.split(save_url)[-1].split('.')[0]
+        errMap_rgb = imread(save_url)
+        wandb.log({f"test_errMap/{saveName}": wandb.Image(errMap_rgb)})
+    return errMap
+
+
+def apply_model_on_event(model, test_id, output_dir, cfg):
+    output_dir = Path(output_dir)
+    output_dir.mkdir(exist_ok=True)
+    data_dir = Path(cfg.data.dir) / "test_images"
+
+    # orbKeyLen = len(test_id.split("_")[-1]) + 1 
+    # event = test_id[:-orbKeyLen]
+    event = test_id
+    print(event)
+
+    print(f"------------------> {test_id} <-------------------")
+
+    predMask, probMask = inference(model, data_dir, event, cfg)
+
+    print(f"predMask shape: {predMask.shape}, unique: {np.unique(predMask)}")
+    print(f"probMask: [{probMask.min()}, {probMask.max()}]")
+
+    # # mtbs_palette =  ["000000", "006400","7fffd4","ffff00","ff0000","7fff00"]
+    # # [0,100/255,0]
+    # mtbs_palette = [[0,100/255,0], [127/255,1,212/255], [1,1,0], [1,0,0], [127/255,1,0], [1,1,1]]
+
+    plt.imsave(output_dir / f"{test_id}_pred.png", predMask, cmap='gray', vmin=0, vmax=1)
+    # plt.imsave(output_dir / f"{test_id}_probMap.png", predMask, cmap='gray', vmin=0, vmax=1)
+
+        # read and save true labels
+    if os.path.isfile(data_dir / "mask" / "poly" / f"{event}.tif"):
+        trueLabel = tiff.imread(data_dir / "mask" / "poly" / f"{event}.tif")
+        # _, _, trueLabel = geotiff.read(data_dir / "mask" / "poly" / f"{event}.tif")
+        # geotiff.save(output_dir / f"{test_id}_predLabel.tif", predMask[np.newaxis,]) 
+
+        trueLabel = trueLabel.squeeze()
+        # print(trueLabel.shape, predMask.shape)
+
+        plt.imsave(output_dir / f"{test_id}_gts.png", trueLabel, cmap='gray', vmin=0, vmax=1)
+        gen_errMap(trueLabel, predMask, save_url=output_dir / f"{test_id}.png")
+
+
+def evaluate_model(cfg, model_url, output_dir):
+
+    # import json
+    # json_url = Path(cfg.data.dir) / "train_test.json"
+    # with open(json_url) as json_file:
+    #     split_dict = json.load(json_file)
+    # test_id_list = split_dict['test']['sarname']
+
+    test_id_list = os.listdir(Path(cfg.data.dir) / "test_images" / "S2" / "post")
+    test_id_list = [test_id[:-4] for test_id in test_id_list]
+    print(test_id_list[0])
+
+    model = torch.load(model_url, map_location=torch.device('cpu'))
+    # output_dir = Path(SegModel.project_dir) / 'outputs'
+    output_dir.mkdir(exist_ok=True)
+
+    band_index_dict = get_band_index_dict(cfg)
+    cfg = edict(cfg)
+    cfg.update({"band_index_dict": band_index_dict})
+    
+    for test_id in test_id_list:
+        apply_model_on_event(model, test_id, output_dir, cfg)
+
+
+
+import hydra
+import wandb
+from omegaconf import DictConfig, OmegaConf
+
+@hydra.main(config_path="./config", config_name="siam_unet_bitemoral")
+def run_app(cfg : DictConfig) -> None:
+    print(OmegaConf.to_yaml(cfg))
+
+    # wandb.init(config=cfg, project=cfg.project.name, name=cfg.experiment.name)
+    wandb.init(config=cfg, project=cfg.project.name, entity=cfg.project.entity, name=cfg.experiment.name)
+
+    # project_dir = Path(hydra.utils.get_original_cwd())
+    #########################################################################
+
+    # # load test_id list
+    # import json
+    # json_url = "D:\wildfire-s1s2-dataset-ak-tiles/train_test.json"
+    # with open(json_url) as json_file:
+    #     split_dict = json.load(json_file)
+    # test_id_list = split_dict['test']['sarname']
+
+    # model = torch.load("G:/PyProjects/smp-seg-pytorch/outputs/best_model_mse.pth")
+    # output_dir = Path(f"G:/PyProjects/smp-seg-pytorch/outputs/test_output_mse")
+
+    # for test_id in test_id_list:
+    #     apply_model_on_event(model, test_id, output_dir, satellites=['S1', 'S2'])
+
+    run_dir = Path("/home/p/u/puzhao/smp-seg-pytorch/outputs/run_s1s2_SiamUnet_conc_['S2']_None_20220108T004641")
+    model_url = run_dir / "model.pth"
+    output_dir = run_dir / "errMap"
+    evaluate_model(cfg, model_url, output_dir)
+    
+    #########################################################################
+    wandb.finish()
+
+if __name__ == "__main__":
+    
+    run_app()
+
+
+    # model = torch.load("G:/PyProjects/smp-seg-pytorch/outputs/best_model_s1s2.pth")
+    # output_dir = Path(f"G:/PyProjects/smp-seg-pytorch/outputs/test_output_s1s2_")
+
+    # for test_id in test_id_list:
+    #     apply_model_on_event(model, test_id, output_dir, satellites=['S1', 'S2'])
\ No newline at end of file
diff --git a/smp/base/modules.py b/smp/base/modules.py
index bc178a8..426f9f7 100644
--- a/smp/base/modules.py
+++ b/smp/base/modules.py
@@ -34,6 +34,8 @@ class Conv2dReLU(nn.Sequential):
         )
         relu = nn.ReLU(inplace=True)
 
+        dropout = nn.Dropout2d(p=0.2) # added by puzhao
+
         if use_batchnorm == "inplace":
             bn = InPlaceABN(out_channels, activation="leaky_relu", activation_param=0.0)
             relu = nn.Identity()
@@ -44,7 +46,8 @@ class Conv2dReLU(nn.Sequential):
         else:
             bn = nn.Identity()
 
-        super(Conv2dReLU, self).__init__(conv, bn, relu)
+        # super(Conv2dReLU, self).__init__(conv, bn, relu)
+        super(Conv2dReLU, self).__init__(conv, bn, relu, dropout) # added by puzhao
 
 
 class SCSEModule(nn.Module):
diff --git a/smp/utils/losses.py b/smp/utils/losses.py
index e1e1790..ee6818c 100644
--- a/smp/utils/losses.py
+++ b/smp/utils/losses.py
@@ -5,7 +5,7 @@ from . import base
 from . import functional as F
 from ..base.modules import Activation
 
-
+# IoU Loss
 class JaccardLoss(base.Loss):
 
     def __init__(self, eps=1., activation=None, ignore_channels=None, **kwargs):
diff --git a/submitit_test.py b/submitit_test.py
new file mode 100644
index 0000000..3066260
--- /dev/null
+++ b/submitit_test.py
@@ -0,0 +1,18 @@
+import submitit
+from main_s1s2_unet import run_app
+
+def add(a, b):
+    for i in range(int(1e4)):
+        c = a + b
+        print(c)
+    return c
+
+# executor is the submission interface (logs are dumped in the folder)
+executor = submitit.AutoExecutor(folder="log_test")
+# set timeout in min, and partition for running the job
+executor.update_parameters(timeout_min=1, slurm_partition="geoinfo")
+job = executor.submit(run_app)  # will compute add(5, 7)
+print(job.job_id)  # ID of your job
+
+output = job.result()  # waits for completion and returns output
+# assert output == 12  # 5 + 7 = 12...  your addition was computed in the cluster
\ No newline at end of file
diff --git a/test.py b/test.py
index 72c53a5..76e121b 100644
--- a/test.py
+++ b/test.py
@@ -1,22 +1,243 @@
-import numpy as np
-import tifffile as tiff
-import matplotlib.pyplot as plt
-from prettyprinter import pprint
+# # import numpy as np
+# # import tifffile as tiff
+# # import matplotlib.pyplot as plt
+# # from prettyprinter import pprint
+
+# # img = tiff.imread(r"D:\wildfire-s1s2-dataset-ak-tiles\train\mask\poly/ak6069116074620190612_2_2.tif")
+
+# # print(img[np.newaxis,].shape)
+# # print(np.unique(img))
+
+# # plt.imshow(img, 'gray')
+# # plt.show()
+
+
+# # import json
+
+# # json_url = "D:\wildfire-s1s2-dataset-ak-tiles/train_test.json"
+# # with open(json_url) as json_file:
+# #     split_dict = json.load(json_file)
+# # test_list = split_dict['test']['sarname']
+# # pprint(test_list)
+
+
+# # cd /cephyr/NOBACKUP/groups/snic2021-7-104/puzhao-snic-500G/smp-seg-pytorch ; 
+# # singularity exec --nv /cephyr/users/puzhao/Alvis/PyTorch_v1.7.0-py3.sif python3 main_s1s2_unet.py
+
+
+
+# # def get_band_index(ALL_BANDS, INPUT_BANDS):
+# #     BAND_INDEX = []
+# #     for band in INPUT_BANDS:
+# #         BAND_INDEX.append(ALL_BANDS.index(band))
+# #     return BAND_INDEX
+
+# # ALL_BANDS = ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']
+# # INPUT_BANDS = ['B4', 'B8', 'B11']
+# # print(get_band_index(ALL_BANDS, INPUT_BANDS))
+
+
+# # post_url = "/cephyr/NOBACKUP/groups/snic2021-7-104/puzhao-snic-500G/US_2021_Dixie/S1/post/20210622T01_ASC137.tif"
+# # post_image = tiff.imread(post_url).transpose(2,0,1) # C*H*W
+# # post_image = (np.clip(post_image, -30, 0) + 30) / 30
+# # post_image = np.nan_to_num(post_image, 0)
+# # print(f"post: {post_image.shape}")
+# # print(post_image.min(), post_image.max())
+
+# import os
+# from cv2 import imread
+# # print(len(os.listdir("/cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ca-tiles/test/S2/pre")))
+# # print(len(os.listdir("/cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ca-tiles/test/S2/post")))
+# # print(len(os.listdir("/cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ca-tiles/test/S1/pre")))
+# # print(len(os.listdir("/home/p/u/puzhao/wildfire-s1s2-dataset-ca-tiles/test/S1/post")))
+
+# # print(set(os.listdir("/cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ca-tiles/train/S2/post"))-\
+# # set(set(os.listdir("/cephyr/NOBACKUP/groups/snic2021-7-104/wildfire-s1s2-dataset-ca-tiles/train/S1/post"))))
+
+
+
+
+
+# # dict = {'S1': [], 'S2': []}
+# # for k in sorted(dict.keys()):
+# #     print(k)
+
+
+
+
+# # import pandas as pd
+# # from prettyprinter import pprint
+
+# # d = {'a': 1,
+# #      'c': {'a': 2, 'b': {'x': 5, 'y' : 10}},
+# #      'd': [1, 2, 3]}
+
+# # c = {
+# #     'project': {'name': 'IGARSS-2022', 'entity': 'wildfire'},
+# #     'data': {'dir': '/home/p/u/puzhao/wildfire-s1s2-dataset-ca-tiles', 'name': 's1s2', 'satellites': ['S1', 'S2'], 'prepost': ['pre', 'post'], 'stacking': True, 'ALL_BANDS': {'S1': ['ND', 'VH', 'VV'], 'ALOS': ['ND', 'VH', 'VV'], 'S2': ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']}, 'INPUT_BANDS': {'S1': ['ND', 'VH', 'VV'], 'ALOS': ['ND', 'VH', 'VV'], 'S2': ['B4', 'B8', 'B12']}, 'useDataWoCAug': False, 'SEED': 42, 'random_state': 42, 'CLASSES': ['burned'], 'REF_MASK': 'poly', 'train_ratio': 0.9},
+# #     'model': {'ARCH': 'distill_unet', 'TOPO': [16, 32, 64, 128], 'S2_PRETRAIN': '/home/p/u/puzhao/smp-seg-pytorch/outputs/run_s1s2_distill_unet_S2_pretrain_B4812_20220108T164608/model.pth', 'DISTILL': True, 'LOSS_COEF': [0, 0], 'ACTIVATION': 'sigmoid', 'ENCODER': 'resnet18', 'ENCODER_WEIGHTS': 'imagenet', 'max_epoch': 100, 'batch_size': 16, 'learning_rate': 0.0001, 'weight_decay': 0.0001, 'cross_domain_coef': 0, 'use_lr_scheduler': True, 'warmup_coef': 2, 'max_score': 0.1, 'save_interval': 5, 'verbose': True},
+# #     'eval': {'patchsize': 512},
+# #     'experiment': {'note': None, 'name': '${data.name}_${model.ARCH}_${data.satellites}_${experiment.note}', 'output': './outputs/run_${experiment.name}_${now:%Y%m%dT%H%M%S}'}
+# # }
+
+# # # df = pd.json_normalize(d, sep='.').to_dict(orient='records')[0]
+
+# # df = pd.json_normalize(c, sep='.').to_dict(orient='records')[0]
+# # # df.to_dict(orient='records')[0]
+
+# # print(df)
+
+
+
+
+# # import torch
+# # from torchvision import datasets, transforms as T
+
+# # transform = T.Compose([T.Resize(256), T.CenterCrop(224), T.ToTensor()])
+# # dataset = datasets.ImageNet("/home/p/u/puzhao/tiny-imagenet-200", split="train", transform=transform)
 
-# img = tiff.imread(r"D:\wildfire-s1s2-dataset-ak-tiles\train\mask\poly/ak6069116074620190612_2_2.tif")
+# # means = []
+# # stds = []
+# # for img in (dataset):
+# #     means.append(torch.mean(img))
+# #     stds.append(torch.std(img))
 
-# print(img[np.newaxis,].shape)
-# print(np.unique(img))
+# # mean = torch.mean(torch.tensor(means))
+# # std = torch.mean(torch.tensor(stds))
+
+# # import torchvision.transforms as transforms
+# # import torchvision.datasets as datasets
+# # import torchvision.models as models
+
+# # traindir = ''
+# # train_loader = torch.utils.data.DataLoader(
+# #         datasets.ImageFolder(traindir, transforms.Compose([
+# #             transforms.RandomSizedCrop(224),
+# #             transforms.RandomHorizontalFlip(),
+# #             transforms.ToTensor(),
+# #             normalize,
+# #         ])),
+
+
+
+# from imageio import imsave
+# import tifffile as tiff
+# import matplotlib.pyplot as plt
+
+# img = tiff.imread("/home/p/u/puzhao/wildfire-s1s2-dataset-ca-tiles/test_images/S2/post/CA_2019_BC_691.tif")
+# img = img[[2,3,5],:,:].transpose(1,2,0)
+# mean = img.mean()
+# std = img.std()
+
+# img_norm = (img - mean) / std
+# vmin = img_norm.min()
+# vmax = img_norm.max()
+# img_norm_vis = (img_norm - vmin) / (vmax - vmin)
+# print(img_norm.min(), img_norm.max())
+
+# # plt.imshow(img)
+# # plt.imshow(img_norm_vis)
+# # plt.show()
+
+# imsave("~/tmp/input_image.png", img)
+# imsave("~/tmp/img_norm.png", img_norm)
+
+# import torch
+# import torch.nn as nn
+# m = nn.Dropout3d(p=0.2)
+# input = torch.rand(1,10,3,3)
+
+# out = m(input)
+# print(input)
+# print(out)
+
+
+# import tifffile as tiff
+# import numpy as np
+# import random
+# from pathlib import Path
+
+# from torch import random
+# from dataset.augument import get_training_augmentation
+# root_dir = Path("/home/p/u/puzhao/wildfire-s1s2-dataset-ca-tiles/test")
+
+
+# augment = get_training_augmentation()
+
+# img1 = tiff.imread(root_dir / 'S2' / 'post' / "CA_2019_AB_133_8_5.tif")[[2,3,5],]
+# img2 = tiff.imread(root_dir / 'S1' / 'post' / "CA_2019_AB_133_8_5.tif")
+# mask = tiff.imread(root_dir / 'mask' / 'poly' / "CA_2019_AB_133_8_5.tif")[np.newaxis,]
+
+
+# # print(img1.shape)
+# print(img1.shape, img2.shape, mask.shape)
+# res = augment(image=img1.transpose(1,2,0), mask=mask.transpose(1,2,0))
+
+# img_list = [img1, img2, img1]
+# mask_list = [mask, mask, mask]
+
+# # random.seed(0)
+# np.random.seed(0)
+
+# x = list(map(lambda image, mask: augment(image=image, mask=mask), img_list, mask_list))
+
+# for key in ['image', 'mask']:
+#     print(key)
+#     print(np.mean(x[0][key]-x[-1][key]))
+#     print(np.mean(mask-x[-1]['mask']))
+#     print("-----")
+
+
+# res1 = augment(image=img1.transpose(1,2,0), mask=mask.transpose(1,2,0)) 
+
+# print(mask[0,].shape, res1["mask"].shape, res1["image"].shape)
+# print(res['image']-res["mask"])
+# # print(np.mean(res['image']-res["mask"]))
+# print(np.mean(res['image']-res1["image"]))
+
+
+# import tifffile as tiff
+# from imageio import imread, imsave
+# import numpy as np
+# import random
+# from pathlib import Path
+# root_dir = Path("/home/p/u/puzhao/wildfire-s1s2-dataset-ca-tiles/test_images")
+# event = "CA_2019_NT_8"
+
+# pre = tiff.imread(root_dir / 'S2' / 'pre' / f"{event}.tif")
+# post = tiff.imread(root_dir / 'S2' / 'post' / f"{event}.tif")
+
+# def nbr(image):
+#     nir = image[3,]
+#     swir = image[5,]
+#     return (nir - swir)/(nir + swir)
+
+# dnbr = nbr(pre) - nbr(post)
+
+# imsave(f'{event}_dNBR.png', dnbr)
+# imsave(f'{event}_NBR_pre.png', nbr(pre))
+# imsave(f'{event}_NBR_post.png', nbr(post))
+# # imsave(f'{event}_dNBR_bin_0.1.png', (dnbr>0.1).astype(float))
+
+
+import tifffile as tiff
+import numpy as np
+from pathlib import Path
+import os
 
-# plt.imshow(img, 'gray')
-# plt.show()
+# data = tiff.imread("/home/p/u/puzhao/wildfire-s1s2-dataset-us-tiles/train/mask/mtbs/az3178211084220170423_3_3_.tif")
+# print(np.unique(data))
 
+# dataPath = Path("/home/p/u/puzhao/wildfire-s1s2-dataset-us-tiles/train")
+# s1_dir = Path("/home/p/u/puzhao/wildfire-s1s2-dataset-us-tiles/train/S1/post")
+# s2_dir = Path("/home/p/u/puzhao/wildfire-s1s2-dataset-us-tiles/train/S2/post")
+# print(len(os.listdir(s1_dir)))
+# print(len(os.listdir(s2_dir)))
 
-import json
 
-json_url = "D:\wildfire-s1s2-dataset-ak-tiles/train_test.json"
-with open(json_url) as json_file:
-    split_dict = json.load(json_file)
-test_list = split_dict['test']['sarname']
-pprint(test_list)
+# import torch
+# x = torch.from_numpy(np.random.rand(10,1,3,3)).cuda()
+# print(x)
 
+img = tiff.imread("/home/p/u/puzhao/wildfire-progression-dataset/CA_2021_Kamloops/S2/post/20210907T19_S2.tif")
+print(img.shape)
diff --git a/test_geoinfo_gpu.py b/test_geoinfo_gpu.py
new file mode 100644
index 0000000..1968f51
--- /dev/null
+++ b/test_geoinfo_gpu.py
@@ -0,0 +1,14 @@
+#import torch
+import wandb
+import os
+
+print(os.getcwd())
+wandb.init(name="test", dir=os.getcwd())
+
+gpu = torch.cuda.get_device_name(0)
+wandb.log({"gpu": gpu})
+print(gpu)
+for i in range(1, 10):
+    wandb.log({f"print_{i}":f"This is {i}"})
+
+wandb.finish()
diff --git a/utils/GeoTIFF.py b/utils/GeoTIFF.py
deleted file mode 100644
index 0b7b03d..0000000
--- a/utils/GeoTIFF.py
+++ /dev/null
@@ -1,199 +0,0 @@
-import gdal
-import os, glob
-from imageio import imread
-from pathlib import Path 
-import numpy as np
-
-class GeoTIFF:
-    """ GeoTIFF is written to read and save GeoTiff data"""
-
-    # read image files
-    def read(self, url):
-        if not isinstance(url, str): url = str(url)
-        raster = gdal.Open(url)  # open file
-
-        im_width = raster.RasterXSize  # get width
-        im_height = raster.RasterYSize  # get height
-
-        im_geotrans = raster.GetGeoTransform()  # get geoTransform
-        im_proj = raster.GetProjection()  # get Projection
-        im_data = raster.ReadAsArray(0, 0, im_width, im_height)  # read data as array
-
-        if len(im_data.shape)==2: im_data = im_data[np.newaxis,]
-
-        del raster
-        self.im_proj, self.im_geotrans = im_proj, im_geotrans
-        return im_proj, im_geotrans, im_data
-
-    # write tiff file
-    def save(self, url, im_data, bandNameList=None, im_proj=None, im_geotrans=None, tiling=False):
-        # gdal data types include:
-        # gdal.GDT_Byte,
-        # gdal .GDT_UInt16, gdal.GDT_Int16, gdal.GDT_UInt32, gdal.GDT_Int32,
-        # gdal.GDT_Float32, gdal.GDT_Float64
-
-        if (im_proj is None) or (im_geotrans is None): im_proj, im_geotrans = self.im_proj, self.im_geotrans
-
-        # check the datatype of raster data
-        if 'int8' in im_data.dtype.name:
-            datatype = gdal.GDT_Byte
-        elif 'int16' in im_data.dtype.name:
-            datatype = gdal.GDT_UInt16
-        else:
-            datatype = gdal.GDT_Float32
-
-        # get the dimension
-        if len(im_data.shape) == 3:
-            im_bands, im_height, im_width = im_data.shape
-        
-        if len(im_data.shape) == 2:
-            im_data = im_data[np.newaxis,]
-            im_bands, im_height, im_width = im_data.shape
-
-        # create output folder
-        outputFolder = os.path.split(url)[0]
-        if not os.path.exists(outputFolder): os.makedirs(outputFolder)
-
-        # create the output file
-        driver = gdal.GetDriverByName("GTiff")  # specify the format
-        if not isinstance(url, str): url = str(url)
-        raster = driver.Create(url, im_width, im_height, im_bands, datatype, options=["TILED=YES",
-                                                                                    "COMPRESS=LZW",
-                                                                                    "INTERLEAVE=BAND"])
-
-        if (raster != None):
-            if tiling: 
-                pass
-                # print("tiling ...")
-            else:
-                raster.SetGeoTransform(im_geotrans)  # write affine transformation parameter
-                raster.SetProjection(im_proj)  # write Projection
-        else:
-            print("Fails to create output file !!!")
-        
-        # write bands one by one
-        for idx in range(0, im_bands):
-            rasterBand = raster.GetRasterBand(idx+1)
-            rasterBand.SetNoDataValue(0)
-
-            if bandNameList is not None: rasterBand.SetDescription(bandNameList[idx])
-            rasterBand.WriteArray(im_data[idx, ...])            
-
-        del raster
-
-    def convert_png_to_geotiff(self, src_url, dst_url, geo_url, bandNameList=None, norm=True):
-        im_proj, im_geotrans, _ = self.read(geo_url)
-        im_data = imread(src_url) 
-
-        im_data = imread(src_url) # H*W*C
-        if len(im_data.shape)==2: im_data = im_data[np.newaxis,]
-        im_data = im_data.transpose(2,0,1) # C*H*W
-
-        if norm: im_data = im_data / 255.0
-        self.save(dst_url, im_data, bandNameList, im_proj, im_geotrans)
-
-
-
-class GRID:
-    """ GeoTIFF is written to read and save GeoTiff data"""
-
-    # read image files
-    def read_data(self, url):
-        if not isinstance(url, str): url = str(url)
-        raster = gdal.Open(url)  # open file
-
-        im_width = raster.RasterXSize  # get width
-        im_height = raster.RasterYSize  # get height
-
-        im_geotrans = raster.GetGeoTransform()  # get geoTransform
-        im_proj = raster.GetProjection()  # get Projection
-        im_data = raster.ReadAsArray(0, 0, im_width, im_height)  # read data as array
-
-        if len(im_data.shape)==2: im_data = im_data[np.newaxis,]
-
-        del raster
-        self.im_proj, self.im_geotrans = im_proj, im_geotrans
-        return im_proj, im_geotrans, im_data
-
-    # write tiff file
-    def write_data(self, url, im_data, im_proj=None, im_geotrans=None, bandNameList=None):
-        # gdal data types include:
-        # gdal.GDT_Byte,
-        # gdal .GDT_UInt16, gdal.GDT_Int16, gdal.GDT_UInt32, gdal.GDT_Int32,
-        # gdal.GDT_Float32, gdal.GDT_Float64
-
-        if (im_proj is None) or (im_geotrans is None): im_proj, im_geotrans = self.im_proj, self.im_geotrans
-
-        # check the datatype of raster data
-        if 'int8' in im_data.dtype.name:
-            datatype = gdal.GDT_Byte
-        elif 'int16' in im_data.dtype.name:
-            datatype = gdal.GDT_UInt16
-        else:
-            datatype = gdal.GDT_Float32
-
-        # get the dimension
-        if len(im_data.shape) == 3:
-            im_bands, im_height, im_width = im_data.shape
-        
-        if len(im_data.shape) == 2:
-            im_data = im_data[np.newaxis,]
-            im_bands, im_height, im_width = im_data.shape
-
-        # create output folder
-        outputFolder = os.path.split(url)[0]
-        if not os.path.exists(outputFolder): os.makedirs(outputFolder)
-
-        # create the output file
-        driver = gdal.GetDriverByName("GTiff")  # specify the format
-        if not isinstance(url, str): url = str(url)
-        raster = driver.Create(url, im_width, im_height, im_bands, datatype, options=["TILED=YES",
-                                                                                    "COMPRESS=LZW",
-                                                                                    "INTERLEAVE=BAND"])
-
-        if (raster != None):
-            raster.SetGeoTransform(im_geotrans)  # write affine transformation parameter
-            raster.SetProjection(im_proj)  # write Projection
-        else:
-            print("Fails to create output file !!!")
-        
-        # write bands one by one
-        for idx in range(0, im_bands):
-            rasterBand = raster.GetRasterBand(idx+1)
-            rasterBand.SetNoDataValue(0)
-
-            if bandNameList is not None: rasterBand.SetDescription(bandNameList[idx])
-            rasterBand.WriteArray(im_data[idx, ...])            
-
-        del raster
-
-    def convert_png_to_geotiff(self, src_url, dst_url, geo_url, bandNameList=None, norm=True):
-        im_proj, im_geotrans, _ = self.read(geo_url)
-        im_data = imread(src_url) 
-
-        im_data = imread(src_url) # H*W*C
-        if len(im_data.shape)==2: im_data = im_data[np.newaxis,]
-        im_data = im_data.transpose(2,0,1) # C*H*W
-
-        if norm: im_data = im_data / 255.0
-        self.save(dst_url, im_data, bandNameList, im_proj, im_geotrans)
-
-if __name__ == "__main__":
-    
-    workspace = Path("E:\Wildfire_SE_Events_2018_V2\SWE_Data_for_Users_Drive\SE2018Ljusdals")
-
-    geotiff = GeoTIFF()
-    
-    src_url = workspace / "S2_MSI_RGB" / "20180717T10_S2.png"
-    geo_url = workspace / "S2_MSI_Tif" / "20180831T10_S2.tif"
-    dst_url = workspace / "Saved_Tif_V1" / "polyMap.tif"
-
-    # geotiff.convert_png_to_geotiff(src_url, dst_url, geo_url, bandNameList=['B12', 'B8', 'B11'], norm=True)
-
-    im_proj, im_geotrans, im_data = geotiff.read(workspace / "S1_SAR_Tif" / "20180728T16_ASC175.tif")
-    geotiff.save(workspace / "Saved_Tif_V1" / "polyMap_V2.tif", im_data, ['sarPolyMap'], im_proj, im_geotrans)
-
-    
-
-
-    
\ No newline at end of file
diff --git a/utils/__init__.py b/utils/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/utils/draft.py b/utils/draft.py
deleted file mode 100644
index a526700..0000000
--- a/utils/draft.py
+++ /dev/null
@@ -1,45 +0,0 @@
-import cv2
-import numpy as np
-from pathlib import Path
-import matplotlib.pyplot as plt
-import tifffile as tiff
-
-mask_dir = Path("G:\SAR4Wildfire_Dataset\Fire_Perimeters_US\Monitoring_Trend_in_Burn_Severity\MTBS_L8_Dataset\MTBS_US_toTiles\mask")
-
-img = cv2.imread(str(mask_dir / "az3455711116020180427.tif"), 0)
-img_ = tiff.imread(str(mask_dir / "az3455711116020180427.tif"))
-
-print(img.shape, np.unique(img))
-print(img_.shape, np.unique(img_))
-
-plt.imshow(img_, cmap='gray')
-plt.show()
-
-
-
-# y_gt_cpu = predPatch.cpu().detach().numpy()
-
-# import os
-# from imageio import imread, imsave
-# for b in range(y_gt_cpu.shape[0]): 
-#     mask = y_gt_cpu[b,]
-
-#     for c in range(mask.shape[0]):
-#         print(b, c)
-
-#         if not os.path.exists(f"batch_{b}"): os.makedirs(f"batch_{b}")
-#         imsave(f"batch_{b}/class_{c}.png", mask[c,])
-
-
-# mask = predPatch.squeeze().cpu().detach().numpy()
-
-# mask = predLabel
-# import os
-# from imageio import imread, imsave
-# for c in range(mask.shape[0]):
-#     # print(b, c)
-#     channel = mask[c,]
-#     print(c, channel.min(), channel.max())
-
-#     # if not os.path.exists(f"batch_{b}"): os.makedirs(f"batch_{b}")
-#     imsave(f"class_{c}.png", mask[c,])
\ No newline at end of file
diff --git a/utils/iou4all.py b/utils/iou4all.py
new file mode 100644
index 0000000..b836a4a
--- /dev/null
+++ b/utils/iou4all.py
@@ -0,0 +1,212 @@
+
+
+import io
+import os, glob
+import numpy as np
+import torch
+from pathlib import Path
+from imageio import imread, imsave
+import tifffile as tiff
+import wandb
+
+# from smp.utils import train
+
+def IoU_score(gt, pr, eps=1e-3):
+    intersection = np.sum(gt * pr) # TP
+    union = np.sum(gt) + np.sum(pr) - intersection + eps
+
+    iou = intersection / union
+    f1 = 2 * intersection / (intersection + union)
+    return [iou, f1, intersection, union]
+
+
+################################################
+###### MTBS Two-Class IoU and F1-Score #######
+################################################
+
+def compute_IoU_F1(phase, result_dir, dataset_dir):
+    event_dir = Path(f"{dataset_dir}/{phase}/S2/post")
+
+    eventList = [filename[:-4] for filename in os.listdir(event_dir)]
+    eventList = sorted(eventList)
+    # print(len(eventList))
+
+    if False:
+        ss = np.random.randint(0, len(eventList),len(eventList))
+        eventList = [eventList[i] for i in ss]
+
+    step = 16 #len(eventList) #, valid batch size
+
+    results = []
+    for i, event in enumerate(eventList):
+        print()
+
+        gt = imread(result_dir / f"{event}_gts.png")[:,:,0] / 255
+        pr = imread(result_dir / f"{event}_pred.png")[:,:,0] / 255
+
+        measure = IoU_score(gt, pr)
+        results.append(measure)
+
+        arr = np.array(results)
+        # print(arr.shape)
+        
+        # method 1
+        # IoU, F1, TP, FP, FN = np.sum(arr, axis=0)
+
+        # total_IoU = TP / (TP + FP + FN + eps)
+        # total_F1 = 2 * TP / (2 * TP + FP + FN + eps)
+
+        # method 2
+        total_iou, total_f1, total_intersection, total_union = np.sum(arr, axis=0)
+        IoU = total_intersection / total_union
+        F1 = 2 * total_intersection / (total_intersection + total_union)
+
+        print(event)
+        print(f"IoU: {measure[0]:.4f}, F1: {measure[1]:.4f}")
+        # print("total: ", TP, FP, FN)
+
+    N = arr.shape[0]
+    print(f"================== total metrics on {phase} ====================")
+    print(f"(dataset as a whole) IoU: {IoU:.4f}, F1: {F1:.4f}")
+    print(f"(average across events) IoU: {total_iou/N:.4f}, avg_F1: {total_f1/N:.4f}")
+    print()
+
+    wandb.log({'final': {f'{phase}.IoU': IoU, f'{phase}.F1': F1}})
+
+    if phase in ['train', 'test']:
+        print("----> batch IoU <-----")
+        batch_measure = []
+        num_batches = len(eventList) // step
+        print(num_batches)
+        for i in range(0, 1 + num_batches):
+            start = i * step
+            end = (i + 1) * step
+
+            if end > len(eventList):
+                end = len(eventList)
+            # print(i, end)
+
+            total_iou, total_f1, total_intersection, total_union = np.sum(np.array(results)[start:end,], axis=0)
+            IoU = total_intersection / total_union
+            F1 = 2 * total_intersection / (total_intersection + total_union)
+
+            batch_measure.append([IoU, F1])
+            avg_batch_IoU, avg_batch_F1 = np.mean(np.array(batch_measure), axis=0)
+
+            print(f"batch {i} [{start}:{end}], batchsize: {end-start}, avg_batch_IoU: {avg_batch_IoU:.4f}, avg avg_batch_F1: {avg_batch_F1:.4f}")
+
+        wandb.log({'batch': {f'{phase}.avg_IoU': avg_batch_IoU, f'{phase}.avg_F1': avg_batch_F1}})
+
+
+################################################
+###### MTBS Multi-Class IoU and F1-Score #######
+################################################
+
+def mtbs_label_preprocess(label):
+    label[label==0] = 1 # both 0 and 1 are unburned
+    label[label==5] = 1 # treat 'greener' (5) as unburned
+    label[label==6] = 0 # ignore 'cloud' (6)
+    label = label - 1 # [4 classes in total: 0, 1, 2, 3], cloud: -1
+    return label
+
+def multiclass_IoU_F1(pred_dir, gts_dir, NUM_CLASS=4, phase='test_images'):
+
+    # gts_dir = Path(f"{dataset_dir}/{phase}/mask/mtbs")
+    TEST_MASK =  os.path.split(gts_dir)[-1]
+
+    eventList = [filename[:-4] for filename in os.listdir(gts_dir)]
+    eventList = sorted(eventList)
+
+    # initialize a dict to store results
+    results = {}
+    for cls in range(0, NUM_CLASS):
+        results[f'class{cls}'] = []
+    
+    # loop over all test events
+    for event in eventList:
+        print(event)
+
+        gt = tiff.imread(gts_dir / f"{event}.tif")
+        if 'mtbs' == TEST_MASK: gt = mtbs_label_preprocess(gt)
+        pr = tiff.imread(pred_dir / f"{event}_pred.tif")
+        
+        # compute IoU and F1 for each class per event
+        for cls in range(0, NUM_CLASS):
+            cls_gt = (gt==cls).astype(float)
+            cls_pr = (pr==cls).astype(float)
+
+            measure = IoU_score(cls_gt, cls_pr)
+            measure = measure + [cls_gt.sum()]
+            results[f'class{cls}'].append(measure)
+
+            print(f"class{cls} IoU: {measure[0]:.4f}, class{cls} F1: {measure[1]:.4f}")
+        
+        print()
+
+    # compute IoU and F1 for each class over all test images
+    class_IoUs = []
+    class_F1s = []
+    class_pixels = []
+    for key in results.keys():
+        arr = np.array(results[key]) # iou, f1, intersection, union, pixel number
+
+        # method 2
+        total_iou, total_f1, total_intersection, total_union, total_pixels = np.sum(arr, axis=0)
+        IoU = total_intersection / total_union
+        F1 = 2 * total_intersection / (total_intersection + total_union)
+
+        N = arr.shape[0]
+        print(f"-------------------- total metrics on {phase} -----------------------")
+        print(f"(dataset as a whole) {key} IoU: {IoU:.4f}, {key} F1: {F1:.4f}")
+        print(f"(average across events) {key} avg_IoU: {total_iou/N:.4f}, {key} avg_F1: {total_f1/N:.4f}")
+        print()
+
+        class_IoUs.append(IoU.round(4))
+        class_F1s.append(F1.round(4))
+        class_pixels.append(total_pixels)
+
+        # log results into wandb for each class
+        wandb.log({'final': {
+                f'{phase}.IoU_{key}': IoU, 
+                f'{phase}.F1_{key}': F1}
+            })
+
+    if NUM_CLASS == 2:
+        wandb.log({'final': {
+                f'{phase}.IoU': class_IoUs[-1], 
+                f'{phase}.F1': class_F1s[-1]}
+            })
+
+    mIoU = np.array(class_IoUs).mean()
+
+    # # Frequency-weighted IoU (FwIoU)
+    class_frequency = np.array(class_pixels) / np.array(class_pixels).sum()
+    FwIoU = (np.array(class_IoUs) * class_frequency).sum()
+
+    print(f"class IoU: {np.array(class_IoUs).round(4)}")
+    print(f"class frequency: {np.array(class_frequency).round(4)}")
+    print(f"mIoU: {mIoU:.4f}, FwIoU: {FwIoU:.4f}")
+
+    wandb.log({'final': {
+            f'{phase}.mIoU': mIoU,
+            f'{phase}.FwIoU': FwIoU
+        }
+    })
+
+
+if __name__ == "__main__":
+
+    # Fresh
+    phase = 'test_images'
+    wandb.init(project='wildfire', name='test-multi-class-IoU')
+
+    # dataset_dir = "/home/p/u/puzhao/wildfire-s1s2-dataset-ca-tiles"
+    # result_dir = Path("/home/p/u/puzhao/smp-seg-pytorch/outputs_igarss/run_s1s2_UNet_['S1']_modis_20220204T181119/errMap")
+    # compute_IoU_F1(phase, result_dir, dataset_dir)
+
+
+    gts_dir = Path("/home/p/u/puzhao/wildfire-s1s2-dataset-us-tiles") / "test_images/mask/mtbs"
+    pred_dir = Path("/home/p/u/puzhao/smp-seg-pytorch/outputs/run_s1s2_UNet_['S2']_mtbs_20220227T233525_work/errMap")
+    multiclass_IoU_F1(pred_dir, gts_dir, NUM_CLASS=3)
+
+    wandb.finish()
\ No newline at end of file
diff --git a/utils/tiff2tiles.py b/utils/tiff2tiles.py
deleted file mode 100644
index 4551bb0..0000000
--- a/utils/tiff2tiles.py
+++ /dev/null
@@ -1,71 +0,0 @@
-import os
-import numpy as np
-from utils.GeoTIFF import GeoTIFF
-from pathlib import Path
-
-
-geotiff = GeoTIFF()
-def geotiff_tiling(phase, src_url, dst_dir, BANDS, BANDS_INDEX, tile_size=256, tiling=False):
-    
-    event_id = os.path.split(src_url)[-1][:-4].split("_")[0]
-    print(f"{event_id}: tiling")
-    print(f"Bands: {BANDS}")
-
-    _, _, im_data = geotiff.read(src_url)
-    im_data = np.nan_to_num(im_data, 0)
-    im_data = im_data[BANDS_INDEX, :, :]
-
-    if 'test' == phase: 
-        test_img_dir = Path(str(dst_dir).replace('test', 'test_images'))
-        geotiff.save(
-                url= test_img_dir / f"{event_id}.tif", 
-                im_data=im_data, 
-                bandNameList=BANDS
-            )
-
-    # print(im_data.dtype.name)
-    # print(im_data.shape)
-    # print(im_data.min(), im_data.max())
-
-    if tiling:
-        C, H, W = im_data.shape
-        # print(C, H, W)
-
-        H_ = (H // tile_size + 1) * tile_size - H
-        W_ = (W // tile_size + 1) * tile_size - W
-
-        # print(H_, W_)
-        # select bands
-        bottom_pad = np.flip(im_data[:, H-H_:H, :], axis=1)
-        # print(bottom_pad.shape)
-
-        # dim2
-        im_data_expanded = np.hstack((im_data, bottom_pad))
-        right_pad = np.flip(im_data_expanded[:, :, W-W_:W], axis=2)
-
-        # dim3
-        im_data_expanded = np.dstack((im_data_expanded, right_pad))
-        # print(im_data_expanded.shape)
-
-        _, H1, W1 = im_data_expanded.shape
-        for i in range(0, H1 // tile_size):
-            for j in range(0, W1 // tile_size):
-                tile = im_data_expanded[:, i*256:(i+1)*256, j*256:(j+1)*256]
-                geotiff.save(
-                    url=dst_dir / f"{event_id}_{i}_{j}.tif", 
-                    im_data=tile, 
-                    bandNameList=BANDS,
-                    tiling=tiling
-                )
-
-
-
-if __name__ == "__main__":
-
-    BANDS = ['B2', 'B3', 'B4', 'B8', 'B11', 'B12']
-    BANDS_INDEX = [0, 1, 2, 6, 8, 9]
-
-    src_url = "D:\wildfire-s1s2-dataset-ak\S2\post/ak6186714639320190717.tif"
-    dst_dir = Path("G:\PyProjects\smp-seg-pytorch\outputs")
-
-    geotiff_tiling(src_url, dst_dir, BANDS, BANDS_INDEX, tile_size=256)
\ No newline at end of file
diff --git a/utils/visualize.py b/utils/visualize.py
deleted file mode 100644
index 3d5b021..0000000
--- a/utils/visualize.py
+++ /dev/null
@@ -1,14 +0,0 @@
-
-import matplotlib.pyplot as plt
-# helper function for data visualization
-def visualize(**images):
-    """PLot images in one row."""
-    n = len(images)
-    plt.figure(figsize=(16, 5))
-    for i, (name, image) in enumerate(images.items()):
-        plt.subplot(1, n, i + 1)
-        plt.xticks([])
-        plt.yticks([])
-        plt.title(' '.join(name.split('_')).title())
-        plt.imshow(image)
-    plt.show()
\ No newline at end of file
diff --git a/wandb/settings b/wandb/settings
deleted file mode 100644
index a5408f2..0000000
--- a/wandb/settings
+++ /dev/null
@@ -1,4 +0,0 @@
-[default]
-disabled = true
-mode = offline
-
